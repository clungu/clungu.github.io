
<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Getting-the-data" data-toc-modified-id="Getting-the-data-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Getting the data</a></span></li><li><span><a href="#Loading-the-zip-file-object" data-toc-modified-id="Loading-the-zip-file-object-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Loading the zip file object</a></span></li><li><span><a href="#Basic-transforms" data-toc-modified-id="Basic-transforms-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Basic transforms</a></span><ul class="toc-item"><li><span><a href="#Pandas-df" data-toc-modified-id="Pandas-df-3.1"><span class="toc-item-num">3.1&nbsp;&nbsp;</span>Pandas df</a></span></li><li><span><a href="#NLTK-Word-tokenisation" data-toc-modified-id="NLTK-Word-tokenisation-3.2"><span class="toc-item-num">3.2&nbsp;&nbsp;</span>NLTK Word tokenisation</a></span></li><li><span><a href="#Saving-the-pandas-dataframe" data-toc-modified-id="Saving-the-pandas-dataframe-3.3"><span class="toc-item-num">3.3&nbsp;&nbsp;</span>Saving the pandas dataframe</a></span></li><li><span><a href="#Loading-the-pandas-dataframe" data-toc-modified-id="Loading-the-pandas-dataframe-3.4"><span class="toc-item-num">3.4&nbsp;&nbsp;</span>Loading the pandas dataframe</a></span></li></ul></li><li><span><a href="#Data-processing" data-toc-modified-id="Data-processing-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Data processing</a></span></li><li><span><a href="#Model" data-toc-modified-id="Model-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Model</a></span><ul class="toc-item"><li><span><a href="#Data" data-toc-modified-id="Data-5.1"><span class="toc-item-num">5.1&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href="#Vocabulary" data-toc-modified-id="Vocabulary-5.2"><span class="toc-item-num">5.2&nbsp;&nbsp;</span>Vocabulary</a></span><ul class="toc-item"><li><span><a href="#Id2Word" data-toc-modified-id="Id2Word-5.2.1"><span class="toc-item-num">5.2.1&nbsp;&nbsp;</span>Id2Word</a></span></li><li><span><a href="#Special-words" data-toc-modified-id="Special-words-5.2.2"><span class="toc-item-num">5.2.2&nbsp;&nbsp;</span>Special words</a></span></li></ul></li><li><span><a href="#Reversed-lookup-dictionary" data-toc-modified-id="Reversed-lookup-dictionary-5.3"><span class="toc-item-num">5.3&nbsp;&nbsp;</span>Reversed lookup dictionary</a></span></li><li><span><a href="#Charset" data-toc-modified-id="Charset-5.4"><span class="toc-item-num">5.4&nbsp;&nbsp;</span>Charset</a></span></li><li><span><a href="#Windowed-data" data-toc-modified-id="Windowed-data-5.5"><span class="toc-item-num">5.5&nbsp;&nbsp;</span>Windowed data</a></span><ul class="toc-item"><li><span><a href="#Using-deques-experiments" data-toc-modified-id="Using-deques-experiments-5.5.1"><span class="toc-item-num">5.5.1&nbsp;&nbsp;</span>Using deques experiments</a></span></li><li><span><a href="#Rolling-window-functions" data-toc-modified-id="Rolling-window-functions-5.5.2"><span class="toc-item-num">5.5.2&nbsp;&nbsp;</span>Rolling window functions</a></span></li></ul></li><li><span><a href="#Data-formatting" data-toc-modified-id="Data-formatting-5.6"><span class="toc-item-num">5.6&nbsp;&nbsp;</span>Data formatting</a></span></li><li><span><a href="#Debug-functions" data-toc-modified-id="Debug-functions-5.7"><span class="toc-item-num">5.7&nbsp;&nbsp;</span>Debug functions</a></span></li><li><span><a href="#Training-set" data-toc-modified-id="Training-set-5.8"><span class="toc-item-num">5.8&nbsp;&nbsp;</span>Training set</a></span></li><li><span><a href="#Model-implementation" data-toc-modified-id="Model-implementation-5.9"><span class="toc-item-num">5.9&nbsp;&nbsp;</span>Model implementation</a></span><ul class="toc-item"><li><span><a href="#Sparse-categorical-digression" data-toc-modified-id="Sparse-categorical-digression-5.9.1"><span class="toc-item-num">5.9.1&nbsp;&nbsp;</span>Sparse categorical digression</a></span></li><li><span><a href="#Memory-optimsations" data-toc-modified-id="Memory-optimsations-5.9.2"><span class="toc-item-num">5.9.2&nbsp;&nbsp;</span>Memory optimsations</a></span></li><li><span><a href="#Training-(with-generators)" data-toc-modified-id="Training-(with-generators)-5.9.3"><span class="toc-item-num">5.9.3&nbsp;&nbsp;</span>Training (with generators)</a></span></li><li><span><a href="#Plot-the-model" data-toc-modified-id="Plot-the-model-5.9.4"><span class="toc-item-num">5.9.4&nbsp;&nbsp;</span>Plot the model</a></span></li><li><span><a href="#Inference-model" data-toc-modified-id="Inference-model-5.9.5"><span class="toc-item-num">5.9.5&nbsp;&nbsp;</span>Inference model</a></span></li><li><span><a href="#Line-by-line-beam-search" data-toc-modified-id="Line-by-line-beam-search-5.9.6"><span class="toc-item-num">5.9.6&nbsp;&nbsp;</span>Line by line beam search</a></span><ul class="toc-item"><li><span><a href="#Loop" data-toc-modified-id="Loop-5.9.6.1"><span class="toc-item-num">5.9.6.1&nbsp;&nbsp;</span>Loop</a></span></li></ul></li></ul></li><li><span><a href="#Complete-beam_search" data-toc-modified-id="Complete-beam_search-5.10"><span class="toc-item-num">5.10&nbsp;&nbsp;</span>Complete beam_search</a></span></li><li><span><a href="#Model-with-sparse_categorical_crossentropy-training" data-toc-modified-id="Model-with-sparse_categorical_crossentropy-training-5.11"><span class="toc-item-num">5.11&nbsp;&nbsp;</span>Model with sparse_categorical_crossentropy training</a></span></li><li><span><a href="#Model-with-PADD_WORD" data-toc-modified-id="Model-with-PADD_WORD-5.12"><span class="toc-item-num">5.12&nbsp;&nbsp;</span>Model with PADD_WORD</a></span><ul class="toc-item"><li><span><a href="#Id2Word" data-toc-modified-id="Id2Word-5.12.1"><span class="toc-item-num">5.12.1&nbsp;&nbsp;</span>Id2Word</a></span></li><li><span><a href="#Special-words" data-toc-modified-id="Special-words-5.12.2"><span class="toc-item-num">5.12.2&nbsp;&nbsp;</span>Special words</a></span></li><li><span><a href="#Charset" data-toc-modified-id="Charset-5.12.3"><span class="toc-item-num">5.12.3&nbsp;&nbsp;</span>Charset</a></span></li><li><span><a href="#Reverse-lookup" data-toc-modified-id="Reverse-lookup-5.12.4"><span class="toc-item-num">5.12.4&nbsp;&nbsp;</span>Reverse lookup</a></span></li><li><span><a href="#Training-set" data-toc-modified-id="Training-set-5.12.5"><span class="toc-item-num">5.12.5&nbsp;&nbsp;</span>Training set</a></span></li><li><span><a href="#Encoding-functions" data-toc-modified-id="Encoding-functions-5.12.6"><span class="toc-item-num">5.12.6&nbsp;&nbsp;</span>Encoding functions</a></span></li><li><span><a href="#Model-definition" data-toc-modified-id="Model-definition-5.12.7"><span class="toc-item-num">5.12.7&nbsp;&nbsp;</span>Model definition</a></span></li><li><span><a href="#Model-training" data-toc-modified-id="Model-training-5.12.8"><span class="toc-item-num">5.12.8&nbsp;&nbsp;</span>Model training</a></span></li><li><span><a href="#Inference-model" data-toc-modified-id="Inference-model-5.12.9"><span class="toc-item-num">5.12.9&nbsp;&nbsp;</span>Inference model</a></span></li></ul></li></ul></li><li><span><a href="#Trash-and-experiments" data-toc-modified-id="Trash-and-experiments-6"><span class="toc-item-num">6&nbsp;&nbsp;</span>Trash and experiments</a></span></li></ul></div>

# Getting the data

Data is downloaded from "http://www.manythings.org/anki/"


```python
%pwd
```




    '/Volumes/Untitled/keras/course_fast_ai/part 2'




```python
!mkdir data
```


```python
!wget "http://www.manythings.org/anki/fra-eng.zip" -O ./data/fra-eng.zip
```

    --2019-02-08 17:07:36--  http://www.manythings.org/anki/fra-eng.zip
    Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196
    Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 3305307 (3.2M) [application/zip]
    Saving to: './data/fra-eng.zip'
    
    ./data/fra-eng.zip  100%[===================>]   3.15M  2.45MB/s    in 1.3s    
    
    2019-02-08 17:07:38 (2.45 MB/s) - './data/fra-eng.zip' saved [3305307/3305307]
    


# Loading the zip file object


```python
import zipfile
z = zipfile.ZipFile("./data/fra-eng.zip")
```

Printing the filenames inside the archive


```python
z.filelist
```




    [<ZipInfo filename='_about.txt' compress_type=deflate filemode='-rw-r--r--' file_size=1441 compress_size=701>,
     <ZipInfo filename='fra.txt' compress_type=deflate filemode='-rw-r--r--' file_size=11245636 compress_size=3304398>]



Reading from the fra.txt file


```python
z.read("fra.txt")[:50]
```




    b'Go.\tVa !\nHi.\tSalut !\nRun!\tCours\xe2\x80\xaf!\nRun!\tCourez\xe2\x80\xaf'




```python
import pandas as pd
```


```python
z.extract("fra.txt", "./data/")
```




    'data/fra.txt'




```python
with open("./data/fra.txt", "r", encoding="utf-8") as f:
    buffer = f.read() 
    lines = buffer.split("\n")
lines[:10]
```




    ['Go.\tVa !',
     'Hi.\tSalut !',
     'Run!\tCours\u202f!',
     'Run!\tCourez\u202f!',
     'Wow!\tÇa alors\u202f!',
     'Fire!\tAu feu !',
     "Help!\tÀ l'aide\u202f!",
     'Jump.\tSaute.',
     'Stop!\tÇa suffit\u202f!',
     'Stop!\tStop\u202f!']




```python
len(lines)
```




    160873



# Basic transforms

## Pandas df

Reading the file intor a pandas dataframe


```python
raw = pd.read_csv("./data/fra.txt", sep="\t", header=None, names=["English", "French"])
raw.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>English</th>
      <th>French</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Go.</td>
      <td>Va !</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Hi.</td>
      <td>Salut !</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Run!</td>
      <td>Cours !</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Run!</td>
      <td>Courez !</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Wow!</td>
      <td>Ça alors !</td>
    </tr>
  </tbody>
</table>
</div>



## NLTK Word tokenisation


```python
from nltk.tokenize import word_tokenize
```


```python
word_tokenize("Va!", language="french")
```




    ['Va', '!']




```python
raw['French_tokens'] = raw.French.transform(lambda x: word_tokenize(x, language="french"))
raw.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>English</th>
      <th>French</th>
      <th>French_tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Go.</td>
      <td>Va !</td>
      <td>[Va, !]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Hi.</td>
      <td>Salut !</td>
      <td>[Salut, !]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Run!</td>
      <td>Cours !</td>
      <td>[Cours, !]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Run!</td>
      <td>Courez !</td>
      <td>[Courez, !]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Wow!</td>
      <td>Ça alors !</td>
      <td>[Ça, alors, !]</td>
    </tr>
  </tbody>
</table>
</div>




```python
raw['English_tokens'] = raw.English.transform(lambda x: word_tokenize(x, language="english"))
raw.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>English</th>
      <th>French</th>
      <th>French_tokens</th>
      <th>English_tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Go.</td>
      <td>Va !</td>
      <td>[Va, !]</td>
      <td>[Go, .]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Hi.</td>
      <td>Salut !</td>
      <td>[Salut, !]</td>
      <td>[Hi, .]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Run!</td>
      <td>Cours !</td>
      <td>[Cours, !]</td>
      <td>[Run, !]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Run!</td>
      <td>Courez !</td>
      <td>[Courez, !]</td>
      <td>[Run, !]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Wow!</td>
      <td>Ça alors !</td>
      <td>[Ça, alors, !]</td>
      <td>[Wow, !]</td>
    </tr>
  </tbody>
</table>
</div>




```python
raw['English_length'] = raw.English_tokens.transform(lambda x: len(x))
raw['French_length'] = raw.French_tokens.transform(lambda x: len(x))
raw.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>English</th>
      <th>French</th>
      <th>French_tokens</th>
      <th>English_tokens</th>
      <th>English_length</th>
      <th>French_length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Go.</td>
      <td>Va !</td>
      <td>[Va, !]</td>
      <td>[Go, .]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Hi.</td>
      <td>Salut !</td>
      <td>[Salut, !]</td>
      <td>[Hi, .]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Run!</td>
      <td>Cours !</td>
      <td>[Cours, !]</td>
      <td>[Run, !]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Run!</td>
      <td>Courez !</td>
      <td>[Courez, !]</td>
      <td>[Run, !]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Wow!</td>
      <td>Ça alors !</td>
      <td>[Ça, alors, !]</td>
      <td>[Wow, !]</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>



## Saving the pandas dataframe


```python
raw.to_parquet("./data/fra_df.hdf5")
```

## Loading the pandas dataframe


```python
raw = pd.read_parquet("./data/fra_df.hdf5")
raw.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>English</th>
      <th>French</th>
      <th>French_tokens</th>
      <th>English_tokens</th>
      <th>English_length</th>
      <th>French_length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Go.</td>
      <td>Va !</td>
      <td>[Va, !]</td>
      <td>[Go, .]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Hi.</td>
      <td>Salut !</td>
      <td>[Salut, !]</td>
      <td>[Hi, .]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Run!</td>
      <td>Cours !</td>
      <td>[Cours, !]</td>
      <td>[Run, !]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Run!</td>
      <td>Courez !</td>
      <td>[Courez, !]</td>
      <td>[Run, !]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Wow!</td>
      <td>Ça alors !</td>
      <td>[Ça, alors, !]</td>
      <td>[Wow, !]</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>



# Data processing

Maximum sentences lengths


```python
raw.English_length.max(), raw.French_length.max()
```




    (51, 60)



Example of long sentences.


```python
raw[raw.English_length == 51][['English', 'French']].values
```




    array([["If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.",
            "Si quelqu'un qui ne connaît pas vos antécédents dit que vous parlez comme un locuteur natif, cela veut dire qu'il a probablement remarqué quelque chose à propos de votre élocution qui l'a fait prendre conscience que vous n'êtes pas un locuteur natif. En d'autres termes, vous ne parlez pas vraiment comme un locuteur natif."],
           ['It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.',
            "Il est peut-être impossible d'obtenir un Corpus complètement dénué de fautes, étant donnée la nature de ce type d'entreprise collaborative. Cependant, si nous encourageons les membres à produire des phrases dans leurs propres langues plutôt que d'expérimenter dans les langues qu'ils apprennent, nous pourrions être en mesure de réduire les erreurs."]],
          dtype=object)



Number of distinct words in each language


```python
raw.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>English</th>
      <th>French</th>
      <th>French_tokens</th>
      <th>English_tokens</th>
      <th>English_length</th>
      <th>French_length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Go.</td>
      <td>Va !</td>
      <td>[Va, !]</td>
      <td>[Go, .]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Hi.</td>
      <td>Salut !</td>
      <td>[Salut, !]</td>
      <td>[Hi, .]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Run!</td>
      <td>Cours !</td>
      <td>[Cours, !]</td>
      <td>[Run, !]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Run!</td>
      <td>Courez !</td>
      <td>[Courez, !]</td>
      <td>[Run, !]</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Wow!</td>
      <td>Ça alors !</td>
      <td>[Ça, alors, !]</td>
      <td>[Wow, !]</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>




```python
raw.French_tokens.values[:10].tolist()
```




    [['Va', '!'],
     ['Salut', '!'],
     ['Cours', '!'],
     ['Courez', '!'],
     ['Ça', 'alors', '!'],
     ['Au', 'feu', '!'],
     ['À', "l'aide", '!'],
     ['Saute', '.'],
     ['Ça', 'suffit', '!'],
     ['Stop', '!']]



# Model

## Data


```python
max_eng_size = raw.English_length.max()
max_fre_size = raw.French_length.max()

max_eng_size, max_fre_size
```




    (51, 60)



## Vocabulary

### Id2Word


```python
subsample = 10000
subsample = -1 # all words
```


```python
from functools import reduce
from tqdm import tqdm_notebook as tqdm
# from tqdm import trange

def flatten(series: list) -> list:
    """
    Flattens the given list and returns a list of (unique) tokens
    """
    return list(reduce(lambda collect, value: collect | set(value), series, set()))

def unique(vocabulary: list) -> list:
    return list(set(vocabulary))

def lower(vocabulary: list) -> list:
    return list(map(str.lower, vocabulary))

id2en = sorted(unique(lower(flatten(tqdm(raw.English_tokens[:subsample])))))
id2fr = sorted(unique(lower(flatten(tqdm(raw.French_tokens[:subsample])))))

max_eng_size = raw.English_length[:subsample].max()
max_fre_size = raw.French_length[:subsample].max()

len(id2en), len(id2fr)
```


    HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))


    



    HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))


    





    (14200, 28614)



### Special words


```python
START_WORD = ">"
STOP_WORD = "_"

SPECIAL_WORDS = [START_WORD, STOP_WORD]

assert set(SPECIAL_WORDS) & set(id2en) == set()
assert set(SPECIAL_WORDS) & set(id2fr) == set()
```

We add the special words as the first elements of each vocabulary list


```python
id2en = SPECIAL_WORDS + id2en
id2fr = SPECIAL_WORDS + id2fr
```


```python
np.savez("./data/fr_en_vocabulary.npz", 
         id2en=id2en, 
         id2fr=id2fr, 
         max_eng_size=max_eng_size, 
         max_fre_size=max_fre_size, 
         start_word=START_WORD,
         stop_word=STOP_WORD
        )
```

## Reversed lookup dictionary


```python
from typing import Dict

def word2id(elements: list) -> Dict[str, int]:
    return {word: _id for _id, word in enumerate(elements)}
    
en2id = word2id(tqdm(id2en))
fr2id = word2id(tqdm(id2fr))

# Test that for the first 100 elements the two datastructures are conssitent
for i in range(100):
    assert fr2id[id2fr[i]] == i
    assert en2id[id2en[i]] == i
    
# Test that we have the special words in front
assert fr2id[START_WORD] == 0
assert en2id[STOP_WORD] == 1
```


    HBox(children=(IntProgress(value=0, max=14202), HTML(value='')))


    



    HBox(children=(IntProgress(value=0, max=28616), HTML(value='')))


    



```python
id2fr[:10]
```




    ['>', '_', '!', '$', '%', '&', "'", "''", "'maison", "'oublie"]



## Charset


```python
fr_charset = sorted(unique(flatten(id2fr)))
en_charset = sorted(unique(flatten(id2en)))

from pprint import pprint
print("French chars: ", ", ".join(fr_charset))
print("English chars: ", ", ".join(en_charset))
len(fr_charset), len(en_charset)
```

    French chars:  !, $, %, &, ', (, ), +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, >, ?, _, `, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, «, », à, á, â, ç, è, é, ê, ë, î, ï, ô, ö, ù, û, œ, с, ​, –, ‘, ’, …, ‽, ₂
    English chars:  !, $, %, &, ', +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, >, ?, _, `, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, ­, ç, é, ö, ú, а, –, —, ‘, ’, ₂, €





    (79, 64)



## Windowed data

### Using deques experiments


```python
from collections import deque
```


```python
d = deque([1, 2, 3, 4, 5, 6, 7, 8], maxlen=4)
d
```




    deque([5, 6, 7, 8])




```python
d.append(9)
d
```




    deque([6, 7, 8, 9])




```python
list(d)
```




    [6, 7, 8, 9]



### Rolling window functions


```python
from collections import deque

def window(values: list, size: int=3, padding="_"):
    print(type(size))
    size = int(size)   # np.int64 won't do!
    padded_list = [padding] * (size + 1)
    d = deque(padded_list, maxlen=size)
    for element in values[::-1]:
        d.appendleft(element)
        yield list(d)

list(window([1, 2, 3, 4], 3))
```

    <class 'int'>





    [[4, '_', '_'], [3, 4, '_'], [2, 3, 4], [1, 2, 3]]




```python
def gen_windows(tokens: list, size:int = 3) -> list:
    """
    Generates all the windows with the given size, from the initial token list
    """
    all_windows = list(window(tokens, size=size, padding=STOP_WORD))
    windows_excluding_the_all_stop_list = all_windows[1:]
    windows_in_natural_order = windows_excluding_the_all_stop_list[::-1]
    return windows_in_natural_order

gen_windows([START_WORD, 'Va', '!', STOP_WORD])
```

    <class 'int'>





    [['>', 'Va', '!'], ['Va', '!', '_'], ['!', '_', '_']]



## Data formatting


```python
def phrase(tokens: list) -> list:
    """
    Padds the token list with START and STOP words
    """
    return [START_WORD] + lower(tokens) + [STOP_WORD]

# Each phrase increases with 2 more elements so the maxiums change as well
max_eng_size = max_eng_size + 2
max_fre_size = max_fre_size + 2

phrase(['Va', '!'])
```




    ['>', 'va', '!', '_']




```python
from typing import List, Dict

def indexed(tokens: List[str], vocabulary: Dict[str, int]) -> List[int]:
    """
    Returns a vectorized list for the given token list. 
    The vectorisation is done using the given vocabulary mapping. 
    """
    return [vocabulary[word] for word in tokens]

from functools import partial
fr_indexed = partial(indexed, vocabulary=fr2id)
en_indexed = partial(indexed, vocabulary=en2id)

indexed(phrase(['Va', '!']), vocabulary=fr2id), fr_indexed(phrase(['Va', '!']))
```




    ([0, 27005, 2, 1], [0, 27005, 2, 1])




```python
def padd_right(tokens: list, fixed_size=3, padd="|")-> list:
    """
    Adds enought padding characters to the right of the given list to as to make it a fixed size == fixed_size
    """
    l = len(tokens)
    to_padd = max(0, fixed_size - l)
    return list(tokens) + ([padd] * to_padd)

padded = padd_right([1, 2, 3], fixed_size=10, padd=0)

fixed_length_fr = partial(padd_right, fixed_size=max_fre_size, padd=STOP_WORD)
fixed_length_en = partial(padd_right, fixed_size=max_eng_size, padd=STOP_WORD)

assert len(padded) == 10
assert padded[-1] == 0
padded
```




    [1, 2, 3, 0, 0, 0, 0, 0, 0, 0]



## Debug functions


```python
def to_str(tokens: List[object]):
    """
    Pretty prints a list of tokens
    """
    return " ".join(map(str, tokens))
    
print(to_str(fixed_length_fr([1, 2, 3])))
```

    1 2 3 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _



```python
def decode(indexed_tokens: List[int], vocabulary: List[str]) -> List[str]:
    """
    Decodes a list of token ids to into a list of words, using the given vocabulary
    """
    return [vocabulary[_id] for _id in indexed_tokens]

en_decode = partial(decode, vocabulary=id2en)
fr_decode = partial(decode, vocabulary=id2fr)

to_str(fr_decode(encode_input[0]))
```




    'coincés ! _ _ _ _ _ _ _ _ _ _ _ _ _'



## Training set


```python
fixed_length_fre_generator = map(lambda tokens: fr_indexed(fixed_length_fr(lower(tokens))), tqdm(raw.French_tokens[:subsample]))
fixed_length_eng_generator = map(lambda tokens: en_indexed(fixed_length_en(phrase(tokens))), tqdm(raw.English_tokens[:subsample]))

encode_input = np.array(list(fixed_length_fre_generator), dtype=np.int32)
decode_input = np.array(list(fixed_length_eng_generator), dtype=np.int32)

encode_input.shape, decode_input.shape
```


    HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))



    HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))





    ((160871, 62), (160871, 53))




```python
decode_output = np.full_like(decode_input, fill_value=en2id[STOP_WORD])

decode_output[:,:-1] = decode_input[:, 1:]

# Test that the decode_output is one timestep ahead
assert np.alltrue(decode_output[:, :max_eng_size-1] == decode_input[:, 1:max_eng_size])

decode_output.shape
```




    (160871, 53)




```python
decode_input[0], decode_output[0]
```




    (array([   0, 5597,   20,    1,    1,    1,    1,    1,    1,    1,    1,
               1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
               1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
               1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
               1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32),
     array([5597,   20,    1,    1,    1,    1,    1,    1,    1,    1,    1,
               1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
               1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
               1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
               1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32))



So the model needs to learn that:
```
0 -> 5599
5599 -> 20
20 -> 1
1 -> 1, etc.
```
all in one go!


```python
from sklearn.model_selection import train_test_split
encode_input_train, encode_input_test, decode_input_train, decode_input_test, decode_output_train, decode_output_test = train_test_split(encode_input, decode_input, decode_output, test_size=0.2, shuffle=True)

encode_input_train.shape, decode_input_train.shape, decode_output_train.shape
```




    ((128696, 62), (128696, 53), (128696, 53))



## Model implementation


```python
from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed
from keras.models import Model
from keras.layers import concatenate
from keras import backend as K
```


```python
# Note: turns out that we can pass the shape shape=(None,) and this will make the model a variable length one!
fre = Input(shape=(max_fre_size,), name="FreTokens")
eng = Input(shape=(max_eng_size,), name="EngTokens")

fre_emb = Embedding(input_dim=len(id2fr), output_dim=200, name="FreEmbeds")
eng_emb = Embedding(input_dim=len(id2en), output_dim=200, name="EngEmbeds")

# TODO: this shouldn't have used return_state=True!!
fre_lstm = LSTM(201, name="FreLSTM", return_state=True)

# Although we don't need in the trainign session, the internal state of the decoder part
# (so we could pass return_state=False) we want to reuse the same layer in the decoder model
# used for inference, so we use retrurn_state=True after all 
eng_lstm = LSTM(201, name="EngLSTM", return_state=True, return_sequences=True)


# We recieve three things here because we've set return_state=True.
# We also get the internal states of the RNN (hidden, constants)
# 
# hidden = <tf.Tensor 'FreLSTM_4/while/Exit_3:0' shape=(?, 10) dtype=float32>,
# constants = <tf.Tensor 'FreLSTM_4/while/Exit_4:0' shape=(?, 10) dtype=float32>
single_fre_out, fre_hidden_state, fre_constants = fre_lstm(fre_emb(fre))

# At this stage, the single_fre_out is irrelevant to us. What we want to get 
# is only the internal state of the LSTM
_ = single_fre_out

# Initialize the English LSTM with the hidden state that we got by passing through 
# all the words in the French sentence. We basically start from the point that LSTM stopped and we wish to 
# "go back in time" if we can, but reversing in a different language (English).
all_eng_out, eng_hidden_state, eng_constants = eng_lstm(eng_emb(eng), initial_state=[fre_hidden_state, fre_constants])

# I'm still unsure what's the difference bewteen applying TimeDistributed vs. Dense layer directly.
# Both seem to act the same
#
# We will need to reuse the trained Dense layer so we can requre it to the inference inputs
eng_decode = Dense(len(id2en), activation="softmax")
output = eng_decode(all_eng_out)

model = Model(inputs=[fre, eng], outputs=[output])
model.summary()

model.compile(optimizer="rmsprop", loss="categorical_crossentropy")
```

    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    FreTokens (InputLayer)          (None, 62)           0                                            
    __________________________________________________________________________________________________
    EngTokens (InputLayer)          (None, 53)           0                                            
    __________________________________________________________________________________________________
    FreEmbeds (Embedding)           (None, 62, 200)      5723200     FreTokens[0][0]                  
    __________________________________________________________________________________________________
    EngEmbeds (Embedding)           (None, 53, 200)      2840400     EngTokens[0][0]                  
    __________________________________________________________________________________________________
    FreLSTM (LSTM)                  [(None, 201), (None, 323208      FreEmbeds[0][0]                  
    __________________________________________________________________________________________________
    EngLSTM (LSTM)                  [(None, 53, 201), (N 323208      EngEmbeds[0][0]                  
                                                                     FreLSTM[0][1]                    
                                                                     FreLSTM[0][2]                    
    __________________________________________________________________________________________________
    dense_2 (Dense)                 (None, 53, 14202)    2868804     EngLSTM[0][0]                    
    ==================================================================================================
    Total params: 12,078,820
    Trainable params: 12,078,820
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
eng_constants
```




    <tf.Tensor 'EngLSTM_2/while/Exit_4:0' shape=(?, 201) dtype=float32>



### Sparse categorical digression

Depending on what we end up getting in the training data, the one-hot-encoding will miss some rare words (from the end of the vocabulary) that will be found among the decoder_output values, so (probably) the "sparse_categorical_crossentropy" won't work (because most probably it inferes the right dimensions of the inlined one-hot-encoding from the input data).

This is why (I believe) that it was suggested ([here](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)) to one-hot-encode the outputs by hand. 

It seems we can't really do this on this computer because we get OutOfMemory error.

If this is the case, the only other option would be to prune from the decoder_output train set, all the rows that contain ids which are nowhere to be found in the decode_inputs.


```python
from keras.utils import to_categorical
decode_output.shape
to_categorical(decode_output[:10]).shape, to_categorical(decode_output[:200]).shape
```




    ((10, 53, 14091), (200, 53, 14163))



We want to test the above hypotesis and see how many ids only exist in either decode_input or decoder_output.


```python
ids_in_output = set(np.unique(decode_output.flatten()))
ids_in_input = set(np.unique(decode_input.flatten()))

only_in_input = ids_in_input - ids_in_output
only_in_output = ids_in_output - ids_in_input

only_in_input, only_in_output
```




    ({0}, set())



### Memory optimsations

We can fit 100 samples, but going above this, will lead for memory errors.


```python
from keras.utils import to_categorical
model.fit(x=[encode_input_train[:100], decode_input_train[:100]], y=to_categorical(decode_output_train[:100], num_classes=len(id2en)), validation_split=0.2)
```

    Train on 80 samples, validate on 20 samples
    Epoch 1/1
    80/80 [==============================] - 7s 83ms/step - loss: 9.4169 - val_loss: 7.5384





    <keras.callbacks.History at 0x150f52ef0>



The full training data (lables), when one-hot-encoded take arround 11GB of RAM space, which is larger than we might want. We use a generator to split batches and one-hot-encode a batch at a time. This is more time consuming but way more space efficient.


```python
english_language_vocabulary_size = len(id2en)
batch_size = 64

def data_generator(encode_input, decode_input, decode_output, batch_size=32, subsample=10000):
    assert encode_input.shape[0] == decode_input.shape[0] == decode_output.shape[0]
    length = encode_input.shape[0] if not subsample else subsample
    while True:
        for i in range(0, length, batch_size):
            yield [encode_input[i:i+batch_size, :], decode_input[i:i+batch_size, :]], to_categorical(decode_output[i:i+batch_size, :], num_classes=english_language_vocabulary_size)
            
gen = data_generator(encode_input_train, decode_input_train, decode_output_train, batch_size=batch_size)
[inp, tch], out = next(gen)
inp.shape, tch.shape, out.shape
```




    ((64, 62), (64, 53), (64, 53, 14202))




```python
sample = 11

def _fr(data):
    return to_str(fr_decode(data))[:100]

def _en(data):
    return to_str(en_decode(data))[:100]

_fr(inp[sample]), _en(tch[sample]), _en(np.argmax(out[sample], axis=-1))
```




    ('tom nous doit une faveur . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _',
     '> tom owes us a favor . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ',
     'tom owes us a favor . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ')




```python
for t in range(decode_input_test.shape[1] - 1):
    assert np.alltrue(decode_input_test[:, t+1] == decode_output_test[:, t])
print("Assertion CORRECT!")
```

    Assertion CORRECT!


### Training (with generators)


```python
subsample_train = None # encode_input_train.shape[0]
batch_size = 64
subsample_test = None  # encode_input_test.shape[0]
epochs = 100

steps_per_epoch_train = subsample_train if subsample_train else encode_input_train.shape[0] // batch_size
steps_per_epoch_test = subsample_test if subsample_test else encode_input_test.shape[0] // batch_size

train_gen = data_generator(encode_input_train, decode_input_train, decode_output_train, batch_size=batch_size, subsample=subsample_train)
test_gen = data_generator(encode_input_test, decode_input_test, decode_output_test, batch_size=batch_size, subsample=subsample_test)

history2 = model.fit_generator(train_gen, steps_per_epoch=steps_per_epoch_train, epochs=epochs, validation_data=test_gen, validation_steps=steps_per_epoch_test)
```

    Epoch 1/100
     279/2010 [===>..........................] - ETA: 1:29:08 - loss: 0.1640


    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    <ipython-input-513-a57de0249da2> in <module>
         10 test_gen = data_generator(encode_input_test, decode_input_test, decode_output_test, batch_size=batch_size, subsample=subsample_test)
         11 
    ---> 12 history2 = model.fit_generator(train_gen, steps_per_epoch=steps_per_epoch_train, epochs=epochs, validation_data=test_gen, validation_steps=steps_per_epoch_test)
    

    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs)
         89                 warnings.warn('Update your `' + object_name + '` call to the ' +
         90                               'Keras 2 API: ' + signature, stacklevel=2)
    ---> 91             return func(*args, **kwargs)
         92         wrapper._original_function = func
         93         return wrapper


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
       1416             use_multiprocessing=use_multiprocessing,
       1417             shuffle=shuffle,
    -> 1418             initial_epoch=initial_epoch)
       1419 
       1420     @interfaces.legacy_generator_methods_support


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
        215                 outs = model.train_on_batch(x, y,
        216                                             sample_weight=sample_weight,
    --> 217                                             class_weight=class_weight)
        218 
        219                 outs = to_list(outs)


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight)
       1215             ins = x + y + sample_weights
       1216         self._make_train_function()
    -> 1217         outputs = self.train_function(ins)
       1218         return unpack_singleton(outputs)
       1219 


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)
       2713                 return self._legacy_call(inputs)
       2714 
    -> 2715             return self._call(inputs)
       2716         else:
       2717             if py_any(is_tensor(x) for x in inputs):


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)
       2673             fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)
       2674         else:
    -> 2675             fetched = self._callable_fn(*array_vals)
       2676         return fetched[:len(self.outputs)]
       2677 


    ~/Envs/deep3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)
       1437           ret = tf_session.TF_SessionRunCallable(
       1438               self._session._session, self._handle, args, status,
    -> 1439               run_metadata_ptr)
       1440         if run_metadata:
       1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)


    KeyboardInterrupt: 



```python
model.save_weights("./data/fre_eng_lstm_fixed.ep100.subsampled.hdf5",overwrite=True)
```


```python
model.load_weights("./data/fre_eng_lstm_fixed.ep100.subsampled.hdf5")
```


```python
model.save_weights("./data/fre_eng_lstm_fixed.ep19.hdf5",overwrite=True)
```


```python
model.load_weights("./data/fre_eng_lstm_fixed.ep8.hdf5")
```

### Plot the model


```python
from keras.utils import plot_model
plot_model(model)
```


    ---------------------------------------------------------------------------

    ImportError                               Traceback (most recent call last)

    <ipython-input-521-18c84a4b0e8e> in <module>
          1 from keras.utils import plot_model
    ----> 2 plot_model(model)
    

    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/utils/vis_utils.py in plot_model(model, to_file, show_shapes, show_layer_names, rankdir)
        130             'LR' creates a horizontal plot.
        131     """
    --> 132     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir)
        133     _, extension = os.path.splitext(to_file)
        134     if not extension:


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/utils/vis_utils.py in model_to_dot(model, show_shapes, show_layer_names, rankdir)
         53     from ..models import Sequential
         54 
    ---> 55     _check_pydot()
         56     dot = pydot.Dot()
         57     dot.set('rankdir', rankdir)


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/utils/vis_utils.py in _check_pydot()
         18     if pydot is None:
         19         raise ImportError(
    ---> 20             'Failed to import `pydot`. '
         21             'Please install `pydot`. '
         22             'For example with `pip install pydot`.')


    ImportError: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.



```python
from matplotlib import pyplot as plt

history = history2

# Plot training & validation accuracy values
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-509-87dc1cd5c61b> in <module>
          1 from matplotlib import pyplot as plt
          2 
    ----> 3 history = history2
          4 
          5 # Plot training & validation accuracy values


    NameError: name 'history2' is not defined


### Inference model


```python
tokens = ["Allez", "!"]

def fr_input(tokens):
    return np.expand_dims(np.array(fr_indexed(fixed_length_fr(lower(tokens)))), 0)

def en_input(tokens: list):
    return np.expand_dims(np.array(en_indexed(fixed_length_en(tokens))), 0)

fr_input(tokens), en_input([START_WORD])
```




    (array([[992,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,
               1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,
               1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,
               1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,
               1,   1,   1,   1,   1,   1,   1,   1,   1,   1]]),
     array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
             1, 1, 1, 1, 1, 1, 1, 1, 1]]))



Trying to manually add one word at a time


```python
tokens
```




    ['vous', 'ne', 'semblez', 'pas', 'si', 'maligne', '.']




```python
# tokens = ["Allez", "a", "la", "voiture", "avec", "nous", "."]
tokens = encode_input_train[12]
tokens = strip_plaintext(fr_decode(tokens))

pred = [START_WORD]
while pred[-1] != STOP_WORD:
    p = np.argmax(model.predict(x=[fr_input(tokens), en_input(pred)]), axis=-1)[0][len(pred)-1]
    pred.append(id2en[p])
" ".join(tokens), " ".join(pred)
```




    ('nous sommes tellement contentes de vous avoir ici !',
     "> we 're so glad to have you here . _")




```python
tokens = ["C'est", "une", "jeune"]
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "she"])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "she", "is"])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "she", "is", "a"])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "she", "is", "a", "little"])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "she", "is", "a", "little", "young"])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "she", "is", "a", "little", "young", "."])]), axis=-1)[0])[:10])
```

    ['get', 'what', '_', '_', '_', '_', '_', '_', '_', '_']
    ['get', "'s", '_', '_', '_', '_', '_', '_', '_', '_']
    ['get', "'s", 'car', 'to', '_', '_', '_', '_', '_', '_']
    ['get', "'s", 'car', 'car', 'let', '_', '_', '_', '_', '_']
    ['get', "'s", 'car', 'car', 'car', 'felt', '_', '_', '_', '_']
    ['get', "'s", 'car', 'car', 'car', 'children', 'arrived', '_', '_', '_']
    ['get', "'s", 'car', 'car', 'car', 'children', '_', '_', '_', '_']



```python
tokens = ["pourquoi", "l'as-tu", "gardé", "secret", "?"] # taken from training set
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "why"])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "why", "did"])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "why", "did", "you"])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "why", "did", "you", "keep"])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "why", "did", "you", "keep", "it"])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "why", "did", "you", "keep", "it", "a"])]), axis=-1)[0])[:10])
print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, "why", "did", "you", "keep", "it", "a", "secret"])]), axis=-1)[0])[:10])
```

    ['why', 'why', '_', '_', '_', '_', '_', '_', '_', '_']
    ['why', 'did', '_', '_', '_', '_', '_', '_', '_', '_']
    ['why', 'did', 'you', '_', '_', '_', '_', '_', '_', '_']
    ['why', 'did', 'you', 'keep', '_', '_', '_', '_', '_', '_']
    ['why', 'did', 'you', 'keep', 'it', '_', '_', '_', '_', '_']
    ['why', 'did', 'you', 'keep', 'it', 'a', '_', '_', '_', '_']
    ['why', 'did', 'you', 'keep', 'it', 'a', 'secret', '_', '_', '_']
    ['why', 'did', 'you', 'keep', 'it', 'a', 'secret', '?', '_', '_']


That's not bad at all!

If you get the dimensions wrong and get an error, even if you solve it, because you probabbly was able to execute the eng_lstm call before the error was raised, the lstm bound the erroneus inputs in it's dependency graph and this will make subsequent calls fail (requiring more parameters than needed).

The only solution is to reinitialize the lstm (reinstantiate it from the training code), load the model weights (which you hopefully have saved already) and redo the correct bindings.


```python
# get encoder states
fre_var = Input(shape=(None,), name="FreVariableToken")
_, fre_hidden_var, fre_const_var = fre_lstm(fre_emb(fre_var))
encoder = Model(inputs=fre_var, outputs=[fre_hidden_var, fre_const_var])
encoder.summary()

# Make the initial states of the decoder, input symbols
eng_one = Input(shape=(1,), name="EngSingleToken")
hidden_state = Input(shape=(201,), name="EngHiddLSTM")
const_state = Input(shape=(201,), name="EngConsLSTM")

# wire the input states to the trained LSTM
# we are reusing the eng_emb trained layer to convert token ids into embeddings
# (?) If we pass the initial_state to the LSTM do we still need to reuse the already trained one?
# (?) i.e. does the LSTM have any other state only persisted in the trained LSTM instance?
# (?) EDIT: There are some Wx , Wh matrices what are indeed trained and only stored internally
all_eng_out, eng_hidden_state, eng_constants = eng_lstm(eng_emb(eng_one), initial_state=[hidden_state, const_state])

# wire the all steps output to the trained eng_decode Dense layer
one_hot_words = eng_decode(all_eng_out)

decoder = Model(inputs=[eng_one, hidden_state, const_state], outputs=[one_hot_words, eng_hidden_state, eng_constants])
decoder.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    FreVariableToken (InputLayer (None, None)              0         
    _________________________________________________________________
    FreEmbeds (Embedding)        multiple                  5723200   
    _________________________________________________________________
    FreLSTM (LSTM)               [(None, 201), (None, 201) 323208    
    =================================================================
    Total params: 6,046,408
    Trainable params: 6,046,408
    Non-trainable params: 0
    _________________________________________________________________
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    EngSingleToken (InputLayer)     (None, 1)            0                                            
    __________________________________________________________________________________________________
    EngEmbeds (Embedding)           multiple             2840400     EngSingleToken[0][0]             
    __________________________________________________________________________________________________
    EngHiddLSTM (InputLayer)        (None, 201)          0                                            
    __________________________________________________________________________________________________
    EngConsLSTM (InputLayer)        (None, 201)          0                                            
    __________________________________________________________________________________________________
    EngLSTM (LSTM)                  multiple             323208      EngEmbeds[5][0]                  
                                                                     EngHiddLSTM[0][0]                
                                                                     EngConsLSTM[0][0]                
    __________________________________________________________________________________________________
    dense_2 (Dense)                 multiple             2868804     EngLSTM[5][0]                    
    ==================================================================================================
    Total params: 6,032,412
    Trainable params: 6,032,412
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
[hidden], [const] = encoder.predict(fr_input(tokens))
hidden.shape, const.shape
```




    ((201,), (201,))




```python
max_eng_size
```




    53




```python
def is_not_finished(generated_english_word_ids):
    """
    Checks if we have a stopping event encountered.
    Resons to stop generating words:
    * STOP_WORD encountered
    * max_lenght words generated
    """
    id_last_word_predicted = generated_english_word_ids[-1]
    
    is_stop_word_hit = id_last_word_predicted == en2id[STOP_WORD]
    is_max_length_hit = len(generated_english_word_ids) == max_eng_size
    
    return not is_stop_word_hit and not is_max_length_hit 


def translate(fre_tokens: List[str], debug=True) -> List[str]:
    """
    Translates the given sentence from French to English.
    
    The input of this function is a list of french word tokens.
    """
    # Scan the full french phrase and return the hidden state of the LSTM
    # This will be used to initialize the first step of the LSTM english decoder
    french_phrase = fr_input(fre_tokens)[0]
    
    # Somehow it matters to have the full padded string as input
    # Sending only the actuall tokens without the padding considerably degrades the performance
#     french_phrase = fr_indexed(lower(fre_tokens + [STOP_WORD] * 45))
    
    if debug:
        print(f"---------------------------")
        print(f"[0] French: \t{to_str(fr_decode(french_phrase))[:80]}")
        print(f"[0] Indexed: \t{french_phrase[:10]}\n")
    
    eng_tokens = [START_WORD]
    eng_inp = en_indexed(lower(eng_tokens))

    [hidden], [const] = encoder.predict([[french_phrase]])
#     print(hidden.shape, const.shape)
#     print(f"[0] Hidden:\t{np.round(hidden, 2)[:10]}")
#     print(f"[0] Const:\t{np.round(const, 2)[:10]}")

#     print(hidden[:, 0], const[:, 0])
        
    id_last_word_predicted = eng_inp[-1]
#     id_stop_word = en2id[STOP_WORD]
    
    # Continously make predictions until we have a stopping event
    while is_not_finished(eng_inp):
#         fixed_eng_inp = fixed_length_en([eng_inp[-1]], padd=id_stop_word)
        fixed_eng_inp = [eng_inp[-1]]
    
        if debug:
            print(f"[{len(eng_inp)-1}] Hidden:\t{np.round(hidden, 2)[:10]}")
            print(f"[{len(eng_inp)-1}] Const:\t{np.round(const, 2)[:10]}")
            print(f"[{len(eng_inp)-1}] Input: \t{to_str(en_decode(fixed_eng_inp))}")

        # pass the decoder LSTM hidden staten and the last word we've just predicted
        [[[one_hot_words]], [hidden], [const]] = decoder.predict([[fixed_eng_inp], [hidden], [const]])
#         [[[one_hot_words], [o2]], [hidden, h1], [const, c1]] = decoder.predict([[fixed_eng_inp, fixed_eng_inp], [hidden, hidden], [const, const]])
        print(one_hot_words.shape, hidden.shape, const.shape)
        
        # one_hot_words.shape = (53, 14203)
        # predicted_words = (53,) 
        predicted_words = np.argmax(one_hot_words, axis=-1)
        print(predicted_words.shape)
        
        # the last word is the one corresponding to the last seen french word (if we discard the START_WORD) 
        # id_last_word_predicted = predicted_words[len(eng_inp) - 1]
        id_last_word_predicted = predicted_words
        
        if debug:
            # print the top words predicted, for each timestep
            top_words = np.argsort(one_hot_words)[::-1][:10]
            print(f"[{len(eng_inp)-1}] Top: \t{[id2en[_id] for _id in top_words]}")
            print(f"[{len(eng_inp)-1}] Score: \t{[np.round(one_hot_words[_id], 2) for _id in top_words]}")
            print(f"[{len(eng_inp)-1}] Pred: \t`{to_str(en_decode(eng_inp))}` ~~~: {id2en[id_last_word_predicted]}\n")
            
        eng_inp.append(id_last_word_predicted)
        

    return en_decode(eng_inp)

to_str(translate(["Allez", "a", "la", "maison", "avec", "nous", "."]))
to_str(translate(["Va", "!"]))
```

    ---------------------------
    [0] French: 	allez a la maison avec nous . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
    [0] Indexed: 	[  992   236 14552 15685  2176 17655    19     1     1     1]
    
    [0] Hidden:	[ 0.07  0.27  0.   -0.    0.27 -0.4   0.21 -0.79  0.   -0.  ]
    [0] Const:	[ 0.16  0.72  0.32 -0.53  9.13 -0.74  0.47 -3.32  1.8  -0.  ]
    [0] Input: 	>
    (14202,) (201,) (201,)
    ()
    [0] Top: 	['go', 'turn', 'leave', 'get', 'come', 'keep', 'look', 'stay', 'their', 'take']
    [0] Score: 	[0.41, 0.2, 0.1, 0.1, 0.03, 0.02, 0.01, 0.01, 0.01, 0.01]
    [0] Pred: 	`>` ~~~: go
    
    [1] Hidden:	[ 0.09  0.42 -0.55  0.55  0.16 -0.54  0.34 -0.73  0.47  0.11]
    [1] Const:	[ 0.1   0.44 -0.62  0.62  8.19 -0.6   0.36 -2.84  0.52  0.11]
    [1] Input: 	go
    (14202,) (201,) (201,)
    ()
    [1] Top: 	['home', 'to', 'with', 'away', 'into', 'and', 'out', 'back', 'at', 'around']
    [1] Score: 	[0.92, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0]
    [1] Pred: 	`> go` ~~~: home
    
    [2] Hidden:	[ 0.44 -0.52 -0.27 -0.    0.21 -0.72 -0.34 -0.86  0.85  0.  ]
    [2] Const:	[ 0.48 -0.58 -0.91 -0.35  9.18 -0.91 -1.   -3.26  1.41  1.  ]
    [2] Input: 	home
    (14202,) (201,) (201,)
    ()
    [2] Top: 	['.', 'with', ',', 'and', '!', 'to', 'by', 'on', 'he', 'without']
    [2] Score: 	[0.71, 0.14, 0.08, 0.04, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0]
    [2] Pred: 	`> go home` ~~~: .
    
    [3] Hidden:	[ 0.76 -0.68 -0.22 -0.26  0.25 -0.4  -0.41 -0.96  0.71  0.39]
    [3] Const:	[ 1.   -0.83 -0.22 -1.18  1.92 -0.42 -0.91 -2.    1.77  1.  ]
    [3] Input: 	.
    (14202,) (201,) (201,)
    ()
    [3] Top: 	['_', 'let', 'everyone', '``', 'he', "''", 'get', 'please', 'they', 'these']
    [3] Score: 	[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    [3] Pred: 	`> go home .` ~~~: _
    
    ---------------------------
    [0] French: 	va ! _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
    [0] Indexed: 	[27005     2     1     1     1     1     1     1     1     1]
    
    [0] Hidden:	[ 0.37 -0.07  0.   -0.01  0.46  0.17  0.45 -0.09  0.36 -0.  ]
    [0] Const:	[ 0.67 -0.65  1.04 -0.49 22.77  0.18  1.87 -0.09  2.74 -0.  ]
    [0] Input: 	>
    (14202,) (201,) (201,)
    ()
    [0] Top: 	['go', 'get', 'take', 'make', 'run', 'stand', 'have', 'see', 'keep', 'turn']
    [0] Score: 	[0.83, 0.11, 0.02, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    [0] Pred: 	`>` ~~~: go
    
    [1] Hidden:	[ 0.5   0.26  0.23  0.51  0.17  0.39  0.96 -0.33  0.82  0.04]
    [1] Const:	[ 0.55  0.27  0.24  0.62 21.    0.42  1.92 -0.75  1.15  0.04]
    [1] Input: 	go
    (14202,) (201,) (201,)
    ()
    [1] Top: 	['on', 'down', 'away', 'off', 'showed', 'together', 'ahead', '!', '.', 'out']
    [1] Score: 	[0.31, 0.11, 0.07, 0.07, 0.04, 0.03, 0.03, 0.03, 0.02, 0.02]
    [1] Pred: 	`> go` ~~~: on
    
    [2] Hidden:	[ 0.78  0.69 -0.21 -0.    0.24 -0.04 -0.21 -0.13  0.91  0.  ]
    [2] Const:	[ 1.04  1.04 -0.82 -1.   22.   -0.04 -0.65 -0.67  1.54  0.99]
    [2] Input: 	on
    (14202,) (201,) (201,)
    ()
    [2] Top: 	['.', '!', 'your', ',', 'it', '?', 'to', 'her', 'his', 'a']
    [2] Score: 	[0.93, 0.04, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    [2] Pred: 	`> go on` ~~~: .
    
    [3] Hidden:	[ 0.58  0.68  0.25 -0.    0.59 -0.32 -0.64 -0.48  0.96  0.93]
    [3] Const:	[ 0.69  1.58  0.26 -1.   23.   -0.34 -0.75 -0.77  1.95  1.64]
    [3] Input: 	.
    (14202,) (201,) (201,)
    ()
    [3] Top: 	['_', "''", 'get', 'have', 'go', 'tell', 'take', 'let', 'make', 'come']
    [3] Score: 	[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    [3] Pred: 	`> go on .` ~~~: _
    





    '> go on . _'




```python
class Candidate():
    def __init__(self):
        self.words = list()
        self.prob = list()
        self.p = 0
        self.hidden = None
        self.const = None
        
    def __add_word(word_id):
        self.words.append(word_id)
    
    def __update_probability(self, prob: float):
        p_log = -np.log(prob)
        self.p += p_log
        self.prob.append(prob)
    
    def __update_current_state(self, hidden, const):
        self.hidden = hidden
        self.const = const

    def add_state(self, word_id, prob, hidden, const):
        self.words.append(word_id)
        self.__update_probability(prob)
        self.__update_current_state(hidden, const)
        return self
    
    def score(self):
        return np.exp(np.sum(np.log(self.prob)) / (len(strip_indexed(self.words))**alpha))

    def __repr__(self):
        score = self.score()
        return f"\n{to_str(en_decode(self.words))}\n[{np.round(score, 3)}] {' '.join([str(val) for val in np.round(self.prob, 3)])}"
    
Candidate().add_state(en2id[START_WORD], 1, [], []).add_state(en2id[STOP_WORD], 1, [], [])
```




    
    > _
    [1.0] 1 1




```python
phrase = "monte dans la camionnette !".split(" ")
french_phrase = fr_indexed(lower(["Allez", "a", "la", "maison", "avec", "nous", "."]))
french_phrase
```




    [992, 236, 14552, 15685, 2176, 17655, 19]




```python
[fr_hidden], [fr_const] = encoder.predict([[french_phrase]])
fr_hidden.shape, fr_const.shape
```




    ((201,), (201,))




```python
width = 5
candidates = [Candidate().add_state(word_id=en2id[START_WORD], prob=1, hidden=fr_hidden, const=fr_const)] * width
candidates
```




    [
     >
     [1.0] 1, 
     >
     [1.0] 1, 
     >
     [1.0] 1, 
     >
     [1.0] 1, 
     >
     [1.0] 1]




```python
word = [[candidate.words[-1]] for candidate in candidates]
hidden = [candidate.hidden for candidate in candidates]
const = [candidate.const for candidate in candidates]
np.array(word).shape, np.array(hidden).shape, np.array(const).shape
```




    ((5, 1), (5, 201), (5, 201))




```python
[one_hot_words, hiddens, consts] = decoder.predict([word, hidden, const])
one_hot_words.shape, hiddens.shape, consts.shape
```




    ((5, 1, 14202), (5, 201), (5, 201))




```python
np.argmax([[1, 2, 1, 1, 1], [2, 1, 1, 0, 0], [0, 0, 2, 0, 0]], axis=0)
```




    array([1, 0, 2, 0, 0])




```python
one_hot_words.shape, np.squeeze(one_hot_words, axis=1).shape
```




    ((5, 1, 14202), (5, 14202))




```python
np.array([[candidate.p] for candidate in candidates])
```




    array([[0.],
           [0.],
           [0.],
           [0.],
           [0.]])




```python
c = np.array([[1, 2, 3, 4, 5, 6], [2, 3, 4, 5, 6, 7], [3, 4, 5, 6, 7, 8]]) + np.array([[1], [2], [3]])
c
```




    array([[ 2,  3,  4,  5,  6,  7],
           [ 4,  5,  6,  7,  8,  9],
           [ 6,  7,  8,  9, 10, 11]])




```python
c[[0, 2, 1, 0], [0, 2, 4, 5]]
```




    array([2, 8, 8, 7])



First step of the beam search.. we need unique words


```python
prior_prob = np.array([[candidate.p] for candidate in candidates])
```


```python
word_prob = np.squeeze(one_hot_words, axis=1)   # reduce middle 1 dimension axis
word_log_prob = -np.log(word_prob)  # reason on the negative logs since are more numericaly stable
joint_prob = word_log_prob + prior_prob  # update the priors with the current word predictions
```


```python
candidate_with_max_joint_prob = np.argmax(joint_prob, axis=0)  # get the dimensions on which each word scores best
best_join_prob = np.max(joint_prob, axis=0)     # actually compute the best scores for each word

best_k_words = np.argsort(best_join_prob)[::-1][:5]
prior_prob = best_join_prob[best_k_words]
candidates_that_generated_best_words = candidate_with_max_joint_prob[best_k_words]
best_word_probab = word_prob[candidates_that_generated_best_words, best_k_words]

prior_prob, best_k_words, candidates_that_generated_best_words, best_word_probab
```




    (array([21.92240715, 21.88848305, 21.49939156, 21.30414391, 21.00562477]),
     array([13953,  7127, 12435, 14157,  2290]),
     array([0, 0, 0, 0, 0]),
     array([3.0145272e-10, 3.1185465e-10, 4.6018508e-10, 5.5940652e-10,
            7.5400242e-10], dtype=float32))



### Line by line beam search

Second step. From this step on, we don't need unicity since we're guaranteed to have started from unque words, ..unless, we choose two continuations from the same root..


```python
width = 20
```


```python
phrase = "monte dans la camionnette !".split(" ")
phrase = ["Allez", "a", "la", "maison", "avec", "nous", "."]
french_phrase = fr_indexed(lower(phrase))
french_phrase = fr_input(phrase)[0]
french_phrase
```




    array([  992,   236, 14552, 15685,  2176, 17655,    19,     1,     1,
               1,     1,     1,     1,     1,     1,     1,     1,     1,
               1,     1,     1,     1,     1,     1,     1,     1,     1,
               1,     1,     1,     1,     1,     1,     1,     1,     1,
               1,     1,     1,     1,     1,     1,     1,     1,     1,
               1,     1,     1,     1,     1,     1,     1,     1,     1,
               1,     1,     1,     1,     1,     1,     1,     1])




```python
[fr_hidden], [fr_const] = encoder.predict([[french_phrase]])
fr_hidden.shape, fr_const.shape
```




    ((201,), (201,))




```python
candidates = [Candidate().add_state(word_id=en2id[START_WORD], prob=1, hidden=fr_hidden, const=fr_const)]
candidates
```




    [
     >
     [1.0] 1]



#### Loop


```python
word = [[candidate.words[-1]] for candidate in candidates]
hidden = [candidate.hidden for candidate in candidates]
const = [candidate.const for candidate in candidates]
np.array(word).shape, np.array(hidden).shape, np.array(const).shape
```




    ((20, 1), (20, 201), (20, 201))




```python
[one_hot_words, hiddens, consts] = decoder.predict([word, hidden, const])
one_hot_length = one_hot_words.shape[-1]


one_hot_words.shape, hiddens.shape, consts.shape, one_hot_length
```




    ((20, 1, 14202), (20, 201), (20, 201), 14202)




```python
prior_prob = np.array([[candidate.p] for candidate in candidates])
prior_prob
```




    array([[1.32064765],
           [3.74089378],
           [3.98072754],
           [4.1721368 ],
           [4.4435384 ],
           [4.62313444],
           [4.71460426],
           [4.80957479],
           [4.89995852],
           [5.12437141],
           [5.12614423],
           [5.15888573],
           [4.49333486],
           [5.44525023],
           [5.48002739],
           [5.63113855],
           [4.87495226],
           [5.82714534],
           [5.98053499],
           [6.06416011]])




```python
candidate_lengths = np.array([[len(strip_indexed(candidate.words))] for candidate in candidates])
candidate_lengths.shape
```




    (20, 1)




```python
word_prob = np.squeeze(one_hot_words, axis=1)   # reduce middle 1 dimension axis
word_log_prob = -np.log(word_prob)  # reason on the negative logs since are more numericaly stable
joint_prob = word_log_prob + prior_prob  # update the priors with the current word predictions
joint_prob
```




    array([[42.55307334,  1.32064765, 34.61363929, ..., 42.82775443,
            40.76403373, 42.54854148],
           [36.43469185,  3.74090046, 25.03494018, ..., 36.63613075,
            29.9799723 , 36.24504036],
           [35.19212089,  3.98072921, 26.74879967, ..., 35.10809074,
            30.93763481, 35.04446922],
           ...,
           [39.13041377,  5.82715285, 27.97051692, ..., 39.09188533,
            29.54118037, 38.81853938],
           [24.64738412, 13.0365438 , 19.89150186, ..., 24.64166971,
            20.35563226, 24.41324563],
           [24.8362205 , 35.2090385 , 12.82446361, ..., 24.84001994,
            28.52832103, 24.65184093]])




```python
alpha = 0.7
lenght_normalized = 1 / np.power(candidate_lengths, alpha) * joint_prob
lenght_normalized
```




    array([[16.12459955,  0.50043188, 13.11611662, ..., 16.22868422,
            15.44668031, 16.1228823 ],
           [10.39463456,  1.06726011,  7.14234267, ..., 10.45210406,
             8.5531355 , 10.34052794],
           [10.04013531,  1.13568205,  7.63129817, ..., 10.01616193,
             8.8263518 ,  9.99801103],
           ...,
           [11.16371048,  1.66245744,  7.97984797, ..., 11.15271851,
             8.42795036, 11.07473428],
           [ 7.03177488,  3.71926046,  5.67494556, ...,  7.03014458,
             5.80735963,  6.96497634],
           [ 7.08564895, 10.04496181,  3.65875504, ...,  7.08673291,
             8.13898668,  7.03304638]])




```python
descending_sorted_matrix = np.argsort(lenght_normalized, axis=None)
selected_indexes = descending_sorted_matrix[:width] 
restored_coordinates = np.unravel_index(selected_indexes, (width, one_hot_length))
candidates_that_generated_best_words, best_k_words = restored_coordinates
prior_prob = best_join_prob[best_k_words]
best_word_probab = word_prob[candidates_that_generated_best_words, best_k_words]

prior_prob, best_k_words, candidates_that_generated_best_words, best_word_probab
```




    (array([12.8642416 , 12.8642416 , 12.8642416 , 12.8642416 , 12.8642416 ,
            10.17941952, 12.8642416 , 10.17941952, 10.17941952, 12.8642416 ,
             7.86781597, 10.17941952, 12.8642416 , 11.17679501,  3.8466444 ,
            12.8642416 , 10.17941952, 12.8642416 ,  6.02731228,  9.36805248]),
     array([    1,     1,     1,     1,     1,    20,     1,    20,    20,
                1,  6006,    20,     1, 12680,  7344,     1,    20,     1,
             4583,  1992]),
     array([ 0,  1,  2,  3,  5,  6,  7, 10,  9,  8, 11, 15, 17, 13,  4, 12, 19,
            16,  4,  4]),
     array([1.        , 0.9999933 , 0.99999833, 0.9999968 , 0.9999831 ,
            0.9907782 , 0.9999981 , 0.86585575, 0.803639  , 1.        ,
            0.5720617 , 0.8412858 , 0.9999925 , 0.613074  , 0.22260535,
            1.        , 0.98199445, 1.        , 0.12272349, 0.088715  ],
           dtype=float32))




```python
from copy import deepcopy

new_candidates = []
for i in range(width):
    word_id = best_k_words[i]
    word_probability = best_word_probab[i]
    candidate_parent = candidates_that_generated_best_words[i]
    hidden = hiddens[candidate_parent]
    const = consts[candidate_parent]
    parent = candidates[candidate_parent]
    new_candidates.append(deepcopy(parent).add_state(word_id=word_id, prob=word_probability, hidden=hidden, const=const))
new_candidates    
# candidates = [deepcopy(candidates[candidate_id]).add_state(word_id=best_k_words[i], prob=best_word_probab[i], hidden=hidden[i], const=fr_const) for i, candidate_id in enumerate(candidates_that_generated_best_words)]
```




    [
     > go home . _ _ _
     [0.606] 1.0 0.407 0.921 0.713 1.0 1.0 1.0, 
     > go home with us . _
     [0.344] 1.0 0.407 0.921 0.145 0.439 0.999 1.0, 
     > leave house with them . _
     [0.321] 1.0 0.102 0.343 0.856 0.646 0.962 1.0, 
     > leave home with them . _
     [0.304] 1.0 0.102 0.342 0.931 0.476 0.996 1.0, 
     > go home with them . _
     [0.267] 1.0 0.407 0.921 0.145 0.182 0.999 1.0, 
     > get home with the time .
     [0.298] 1.0 0.102 0.613 0.778 0.794 0.232 0.991, 
     > leave home with us . _
     [0.254] 1.0 0.102 0.342 0.931 0.25 0.999 1.0, 
     > turn up with the house .
     [0.259] 1.0 0.197 0.051 0.992 0.866 0.688 0.866, 
     > get home with the one .
     [0.255] 1.0 0.102 0.613 0.778 0.794 0.154 0.804, 
     > turn at home . _ _
     [0.204] 1.0 0.197 0.075 0.652 0.775 1.0 1.0, 
     > go home , and what he
     [0.231] 1.0 0.407 0.921 0.083 0.381 0.487 0.572, 
     > turn out with the day .
     [0.226] 1.0 0.197 0.09 0.883 0.566 0.404 0.841, 
     > get home with the . _
     [0.19] 1.0 0.102 0.613 0.778 0.794 0.076 1.0, 
     > go home , he gave them
     [0.219] 1.0 0.407 0.921 0.083 0.374 0.373 0.613, 
     > turn in the house , let
     [0.218] 1.0 0.197 0.14 0.687 0.861 0.72 0.223, 
     > get home . _ _ _
     [0.182] 1.0 0.102 0.613 0.179 1.0 1.0 1.0, 
     > get home with the work .
     [0.211] 1.0 0.102 0.613 0.778 0.794 0.06 0.982, 
     > come home . _ _ _
     [0.158] 1.0 0.033 0.379 0.614 0.999 1.0 1.0, 
     > turn in the house , everyone
     [0.187] 1.0 0.197 0.14 0.687 0.861 0.72 0.123, 
     > turn in the house , but
     [0.172] 1.0 0.197 0.14 0.687 0.861 0.72 0.089]




```python
candidates = new_candidates
```

Go to **Loop** and repeat


```python
[candidate.p for candidate in candidates]
```




    [5.843945205211639,
     6.230512922913476,
     6.656705468893051,
     6.964711159467697,
     7.158780634403229,
     7.212840741303808,
     7.299521863460541,
     7.367884606122971,
     7.726608529686928,
     7.739929771050811,
     8.230677098035812,
     8.269179880619049,
     8.40541797876358,
     8.407620310783386,
     8.447463572025299,
     8.485476091504097,
     8.512250483036041,
     8.549754679203033,
     8.558659171685576,
     8.56688747368753]



Beam search optimisations:
* Length normalisation:
    * arg max prod probs => arg max sum log probs
    * since log is monotonic => maximizing log(probs) => maximizing probs
    
* Normalisation
    * Since we multiply probablities (0, 1) => the product is greater for smaller number of terms
    * the more terms => the more probs => the smallest the ending probabs
    * solution: divide by Ty (number of words in the prediction)
    * even better: normalize by 1 / pow(Ty, alpha) where alpha = 0.7 usually. alpha is a hyperparamerter.
    
* Negative logs
    * since numpy usually sorts ascending, but we want to maximize something
    * we will sum over -log(prob) and sort ascending

Beam width selection:
    * 10 (ok for production)
    * 100 somewhat large
    * 1000, 3000 (only for SoTA papers) 


Beam search error analisys:
* y = target sentence
* y' = predicted sentence
* compute T = P(y|x) and T' = P(y'|x)
    * if T > T' => the target probability is higher than the prediction probability => had the beam search algo had more space, then it would have found out a better solution => This means that we can increase the beam search width to have more candidates generated.
    * if T < T' => the target probability is lower than the predictions => regardless if beam search reached that target prediction or not in the search if wouldn't have outputed it => there is a problem with the RNN
    
Make a table to check for some sample of the translations, how many of the errors are due to the beam search or the RNN and decide what piece needs debuging. 


```python
np.dstack(np.unravel_index(np.argsort(one_hot_words, axis=None)[::-1], (width, one_hot_length)))[0, :width, :].shape
```




    (10, 2)




```python
one_hot_length = one_hot_words.shape[-1]
top_selections = np.argsort(one_hot_words, axis=None)[::-1][:width]
top_candidates = [selection//one_hot_length for selection in top_selections]

top_selections, top_candidates
```




    (array([ 42607,      1,  14203,  28405,  71011,  99415,  56809,  85213,
            113617, 141977]), [3, 0, 1, 2, 5, 7, 4, 6, 8, 9])



## Complete beam_search


```python
def is_not_finished(generated_english_word_ids):
    """
    Checks if we have a stopping event encountered.
    Resons to stop generating words:
    * STOP_WORD encountered
    * max_lenght words generated
    """
    id_last_word_predicted = generated_english_word_ids[-1]
    
    is_stop_word_hit = id_last_word_predicted == en2id[STOP_WORD]
    is_max_length_hit = len(generated_english_word_ids) == max_eng_size
    
    return not is_stop_word_hit and not is_max_length_hit 
```


```python
def beam_search(fre_tokens: List[str], width:int = 3, alpha = 0.7, debug=True) -> List[str]:
    """
    Translates the given sentence from French to English using the beam search approach.
    
    The input of this function is a list of french word tokens.
    
    The width paramter tells how many candidates to search at a time (the beam width).
    The alpha paramter tells how hard to penalize short sentences (the bigger the alpha the less likely we will get shorter sentences).
    """
    # Scan the full french phrase and return the hidden state of the LSTM
    # This will be used to initialize the first step of the LSTM english decoder
    # Note: Because we've trained with fixed length training data, we need 
    # to also supply a fixed length french input. If we pass a variable length one
    # the hidden state after running the french sentence, will not be that accurate!
    french_phrase = fr_input(fre_tokens)[0]
    
    if debug:
        print(f"---------------------------")
        print(f"[0] French: \t{to_str(fr_decode(french_phrase))[:80]}")
        print(f"[0] Indexed: \t{french_phrase[:10]}\n")
    
    # Start with a single START_WORD
    eng_inp = en_indexed([START_WORD])

    # Scan the french phrase and get the hidden state
    [fr_hidden], [fr_const] = encoder.predict([[french_phrase]])
    
    # start the beam search with a single candidates
    # all start with "START_WORD"
    candidates = [Candidate().add_state(word_id=en2id[START_WORD], prob=1, hidden=fr_hidden, const=fr_const)]
    stalled_candidates = []
    
    stop_word_id = en2id[STOP_WORD]
    
    while candidates:
        # Get previous states
        word = [[candidate.words[-1]] for candidate in candidates]
        hidden = [candidate.hidden for candidate in candidates]
        const = [candidate.const for candidate in candidates]
        prior_prob = np.array([[candidate.p] for candidate in candidates]) 
        
        
        # Predict next word for each candidate
        [one_hot_words, hiddens, consts] = decoder.predict([word, hidden, const])
        one_hot_length = one_hot_words.shape[-1]

        # Compute the new probabilities  
        word_prob = np.squeeze(one_hot_words, axis=1)   # reduce middle 1 dimension axis
        word_log_prob = -np.log(word_prob)  # reason on the logs since are more numericaly stable
        joint_prob = word_log_prob + prior_prob  # update the priors with the current word predictions
    
        # Compute the length normalisation trick on the joint probabilities
        candidate_lengths = np.array([[len(strip_indexed(candidate.words))] for candidate in candidates])
        lenght_normalized = joint_prob / np.power(candidate_lengths, alpha)
    
        # Get best candidates
        descending_sorted_matrix = np.argsort(lenght_normalized, axis=None) # reason on (-) so we can use ascending order for getting the bigest elements
        selected_indexes = descending_sorted_matrix[:width] 
        restored_coordinates = np.unravel_index(selected_indexes, (width, one_hot_length))
        candidates_that_generated_best_words, best_k_words = restored_coordinates
        prior_prob = best_join_prob[best_k_words]
        best_word_probab = word_prob[candidates_that_generated_best_words, best_k_words]

        # Update the candidate list
        new_candidates = []
        for i in range(width):
            word_id = best_k_words[i]
            word_probability = best_word_probab[i]
            candidate_parent = candidates_that_generated_best_words[i]
            hidden = hiddens[candidate_parent]
            const = consts[candidate_parent]
            parent = candidates[candidate_parent]
            
            new_candidate = deepcopy(parent).add_state(word_id=word_id, prob=word_probability, hidden=hidden, const=const)

            if word_id == stop_word_id:
                stalled_candidates.append(new_candidate)
                width -=1
            else:
                new_candidates.append(new_candidate)
        
        candidates = new_candidates

    sorted_candidates = sorted([(candidate.score(), candidate) for candidate in stalled_candidates], reverse=True)
    sorted_candidates = list(zip(*sorted_candidates))[1]
    return sorted_candidates
        
# print(beam_search(["Allez", "a", "la", "maison", "avec", "nous", "."]))
# print(beam_search(["Va", "!"]))

sample = 35
print(to_str(en_decode(strip_indexed(decode_input_train[sample]))))
print(beam_search(fr_decode(strip_indexed(encode_input_train[sample])), alpha=0.7, width=40))
```

    > why do i have to go to school ?
    ---------------------------
    [0] French: 	pourquoi me faut-il aller à l'école ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
    [0] Indexed: 	[19664 16074 10028   986 27863 14431   234     1     1     1]
    
    (
    > why do i have to go to school ? _
    [0.905] 1.0 0.998 0.794 0.998 0.89 1.0 0.888 0.996 0.975 0.998 1.0, 
    > why do i want to go to school ? _
    [0.532] 1.0 0.998 0.794 0.998 0.056 1.0 0.99 0.997 0.973 0.992 1.0, 
    > why does i have to go to school ? _
    [0.487] 1.0 0.998 0.052 0.726 0.756 0.998 0.968 0.999 0.992 0.997 1.0, 
    > why do i have to come to school ? _
    [0.456] 1.0 0.998 0.794 0.998 0.89 1.0 0.044 0.657 0.974 0.986 1.0, 
    > why do i need to go to school ? _
    [0.455] 1.0 0.998 0.794 0.998 0.027 0.999 0.958 0.997 0.97 0.99 1.0, 
    > why do i have to go to bed ? _
    [0.385] 1.0 0.998 0.794 0.998 0.89 1.0 0.888 0.996 0.013 0.999 1.0, 
    > why do i have to school ? _
    [0.38] 1.0 0.998 0.794 0.998 0.89 1.0 0.041 0.552 1.0, 
    > why about i have to go to school ? _
    [0.359] 1.0 0.998 0.027 0.655 0.457 0.898 0.968 0.998 0.85 0.998 1.0, 
    > why does he want to go to school ? _
    [0.335] 1.0 0.998 0.052 0.123 0.751 0.999 0.989 0.998 0.901 0.979 1.0, 
    > why does i want to go to school ? _
    [0.327] 1.0 0.998 0.052 0.726 0.101 1.0 0.995 1.0 0.974 0.993 1.0, 
    > why do i have to leave school ? _
    [0.319] 1.0 0.998 0.794 0.998 0.89 1.0 0.012 0.585 0.989 1.0, 
    > why do i have to school to go ? _
    [0.312] 1.0 0.998 0.794 0.998 0.89 1.0 0.041 0.11 0.938 0.995 0.997, 
    > why do i go to school ? _
    [0.304] 1.0 0.998 0.794 0.998 0.009 0.999 0.985 0.885 1.0, 
    > why about i go to school ? _
    [0.297] 1.0 0.998 0.027 0.655 0.325 0.999 0.967 0.998 1.0, 
    > why did i go to school ? _
    [0.295] 1.0 0.998 0.015 0.986 0.385 0.984 0.985 0.978 1.0, 
    > why are i going to school ? _
    [0.295] 1.0 0.998 0.035 0.952 0.232 0.998 0.958 0.714 1.0, 
    > why are i like to go to school ? _
    [0.292] 1.0 0.998 0.035 0.952 0.078 0.985 0.906 0.924 0.979 0.997 1.0, 
    > why do i have to take school ? _
    [0.292] 1.0 0.998 0.794 0.998 0.89 1.0 0.006 0.783 0.995 1.0, 
    > why do i have to school now ? _
    [0.288] 1.0 0.998 0.794 0.998 0.89 1.0 0.041 0.106 1.0 1.0, 
    > why do i take to go to school ? _
    [0.285] 1.0 0.998 0.794 0.998 0.006 0.467 0.975 0.993 0.914 0.999 1.0, 
    > why about me have to go to school ? _
    [0.277] 1.0 0.998 0.027 0.268 0.299 0.997 0.778 0.987 0.972 0.999 1.0, 
    > why would i like to go to school ? _
    [0.277] 1.0 0.998 0.015 0.953 0.131 0.999 0.982 0.99 0.892 0.999 1.0, 
    > why do i have to come early in ? _
    [0.27] 1.0 0.998 0.794 0.998 0.89 1.0 0.044 0.218 0.714 0.293 1.0, 
    > why does she want to go to school ? _
    [0.27] 1.0 0.998 0.052 0.074 0.493 0.999 0.987 0.996 0.778 0.981 1.0, 
    > why are i in school ? _
    [0.268] 1.0 0.998 0.035 0.952 0.384 0.481 0.95 1.0, 
    > why i want to go to school ? _
    [0.263] 1.0 0.998 0.016 0.131 1.0 0.988 0.997 0.997 0.991 1.0, 
    > why would i make go to school ? _
    [0.26] 1.0 0.998 0.015 0.953 0.459 0.317 0.964 0.97 0.997 1.0, 
    > why did i want to go to school ? _
    [0.258] 1.0 0.998 0.015 0.986 0.083 1.0 0.992 0.999 0.958 0.986 1.0, 
    > why would i want to go to school ? _
    [0.257] 1.0 0.998 0.015 0.953 0.083 1.0 0.989 0.998 0.967 0.996 1.0, 
    > why about i do go to school ? _
    [0.254] 1.0 0.998 0.027 0.655 0.112 0.91 0.98 0.966 0.997 1.0, 
    > why do i have to go to my bed ? _
    [0.247] 1.0 0.998 0.794 0.998 0.89 1.0 0.888 0.996 0.005 0.199 1.0 1.0, 
    > why shall i go to school ? _
    [0.246] 1.0 0.998 0.003 0.995 0.935 0.974 0.971 0.993 1.0, 
    > why about me to go to school ? _
    [0.242] 1.0 0.998 0.027 0.268 0.208 0.977 0.939 0.992 0.998 1.0, 
    > why do i have to school early ? _
    [0.235] 1.0 0.998 0.794 0.998 0.89 1.0 0.041 0.041 1.0 1.0, 
    > why should i go to school ? _
    [0.221] 1.0 0.998 0.004 1.0 0.362 0.999 0.98 0.99 1.0, 
    > why would i go to school ? _
    [0.213] 1.0 0.998 0.015 0.953 0.099 0.992 0.986 0.976 1.0, 
    > why do i have to come early in the arrived ? _
    [0.21] 1.0 0.998 0.794 0.998 0.89 1.0 0.044 0.218 0.714 0.395 0.073 0.999 1.0, 
    > why can i go to school ? _
    [0.209] 1.0 0.998 0.009 1.0 0.145 0.968 0.983 0.977 1.0, 
    > why does i go to school ? _
    [0.201] 1.0 0.998 0.052 0.726 0.038 0.998 0.956 0.771 1.0, 
    > why i am in school ? _
    [0.171] 1.0 0.998 0.016 0.329 0.67 0.328 0.905 1.0)



```python
to_str(fr_decode(encode_input_test[0])), to_str(en_decode(decode_output_test[0]))
```




    ('> une pomme tomba au sol . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _',
     'an apple fell to the ground . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _')




```python
12and
122
334

```


```python
fr_input(fr_decode(encode_input_test[0])).shape
```




    (1, 62)




```python
def strip_indexed(tokens):
    """
    Function that strips all the decorations from a indexed training sequence (containing ids).
    
    It removes the following:
    * the initial START_WORD
    * the ending STOP_WORD
    * all the padding words added with fixed_length variant functions

    """
    if type(tokens) != list:
        tokens = tokens.tolist()
        
    # exclude the initial START_WORD, the STOP_WORD and everything after it (padding words)
    stop_word_id = en2id[STOP_WORD]
    if stop_word_id in tokens:
        return tokens[0:tokens.index(en2id[STOP_WORD])]
    else:
        return tokens

def strip_plaintext(tokens):
    """
    Function that strips all the decorations from a plaintext training sequence (containing actual words).
    
    It removes the following:
    * the initial START_WORD
    * the ending STOP_WORD
    * all the padding words added with fixed_length variant functions

    """

    # exclude the initial START_WORD, the STOP_WORD and everything after it (padding words)
    if STOP_WORD in tokens:
        return tokens[0:tokens.index(STOP_WORD)]
    else:
        return tokens

def strip(tokens):
    """
    Function that strips all the decorations from a training sequence.
    It removes the following:
    * the initial START_WORD
    * the ending STOP_WORD
    * all the padding words added with fixed_length variant functions
    
    This function can be applied to both indexed or plaintext sequences.
    """
    
    def __are_plaintext(tokens):
        # Heuristic to test if the tokens are in plaintext or not.
        # Assumed plaintext if known plaintext words (almost always guaranteed to be present) 
        # are included among the tokens.
        # 
        # These signal words are:
        # START_WORD, STOP_WORD
        return STOP_WORD in tokens or START_WORD in tokens 
        
    if __are_plaintext(tokens):
        return strip_plaintext(tokens)
    else:
        return strip_indexed(tokens)

strip(fr_decode(encode_input_test[0]))
```




    ['la',
     'maison',
     'de',
     'tom',
     "n'est",
     'pas',
     'loin',
     'de',
     'celle',
     'de',
     'mary',
     '.']




```python
sample = 22
fr_sentence = encode_input_train[sample]
en_sentence = decode_input_train[sample]

french_sent = to_str(fr_decode(strip_indexed(fr_sentence)))
target_sent = to_str(en_decode(strip_indexed(en_sentence)))
english_sent = to_str(strip_plaintext(translate(fr_decode(strip_indexed(fr_sentence)), debug=False)))

print(f"{french_sent}\n{target_sent}\n{english_sent}")
```

    (14202,) (201,) (201,)
    ()
    (14202,) (201,) (201,)
    ()
    (14202,) (201,) (201,)
    ()
    (14202,) (201,) (201,)
    ()
    (14202,) (201,) (201,)
    ()
    elles aiment tom .
    > they love tom .
    > please keep quiet .


## Model with sparse_categorical_crossentropy training


```python
from keras.layers import Flatten
```


```python
fre = Input(shape=(None,))
eng = Input(shape=(None,))

fre_emb = Embedding(input_dim=len(id2fr), output_dim=10, name="FreEmbeddings")
eng_emb = Embedding(input_dim=len(id2en), output_dim=10, name="EngEmbeddings")

fre_lstm = LSTM(10, name="FreLSTM", return_state=True)
eng_lstm = LSTM(10, name="EngLSTM", return_sequences=True)


# We recieve three things here because we've set return_state=True.
# We also get the internal states of the RNN (hidden, constants)
# 
# hidden = <tf.Tensor 'FreLSTM_4/while/Exit_3:0' shape=(?, 10) dtype=float32>,
# constants = <tf.Tensor 'FreLSTM_4/while/Exit_4:0' shape=(?, 10) dtype=float32>
single_fre_out, fre_hidden_state, fre_constants = fre_lstm(fre_emb(fre))

# At this stage, the single_fre_out is irrelevant to us. What we want to get 
# is only the internal state of the LSTM
_ = single_fre_out

# Initialize the English LSTM with the hidden state that we got by passing through 
# all the words in the French sentence. We basically start from the point that LSTM stopped and we wish to 
# "go back in time" if we can, but reversing in a different language (English).
eng_out = eng_lstm(eng_emb(eng), initial_state=[fre_hidden_state, fre_constants])

# I'm still unsure what's the difference bewteen applying TimeDistributed vs. Dense layer directly.
# Both seem to act the same
output = Dense(1, activation="relu")(eng_out)

model = Model(inputs=[fre, eng], outputs=[output])
model.summary()

model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
```

    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    input_69 (InputLayer)           (None, None)         0                                            
    __________________________________________________________________________________________________
    input_70 (InputLayer)           (None, None)         0                                            
    __________________________________________________________________________________________________
    FreEmbeddings (Embedding)       (None, None, 10)     286490      input_69[0][0]                   
    __________________________________________________________________________________________________
    EngEmbeddings (Embedding)       (None, None, 10)     142050      input_70[0][0]                   
    __________________________________________________________________________________________________
    FreLSTM (LSTM)                  [(None, 10), (None,  840         FreEmbeddings[0][0]              
    __________________________________________________________________________________________________
    EngLSTM (LSTM)                  (None, None, 10)     840         EngEmbeddings[0][0]              
                                                                     FreLSTM[0][1]                    
                                                                     FreLSTM[0][2]                    
    __________________________________________________________________________________________________
    dense_21 (Dense)                (None, None, 1)      11          EngLSTM[0][0]                    
    ==================================================================================================
    Total params: 430,231
    Trainable params: 430,231
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
model.fit(x=[encode_input_train[:100], decode_input_train[:100]], y=np.expand_dims(decode_output_train[:100], -1), validation_split=0.2)
```

    Train on 80 samples, validate on 20 samples
    Epoch 1/1



    ---------------------------------------------------------------------------

    InvalidArgumentError                      Traceback (most recent call last)

    <ipython-input-607-2dc421312db6> in <module>
    ----> 1 model.fit(x=[encode_input_train[:100], decode_input_train[:100]], y=np.expand_dims(decode_output_train[:100], -1), validation_split=0.2)
    

    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
       1035                                         initial_epoch=initial_epoch,
       1036                                         steps_per_epoch=steps_per_epoch,
    -> 1037                                         validation_steps=validation_steps)
       1038 
       1039     def evaluate(self, x=None, y=None,


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
        197                     ins_batch[i] = ins_batch[i].toarray()
        198 
    --> 199                 outs = f(ins_batch)
        200                 outs = to_list(outs)
        201                 for l, o in zip(out_labels, outs):


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)
       2664                 return self._legacy_call(inputs)
       2665 
    -> 2666             return self._call(inputs)
       2667         else:
       2668             if py_any(is_tensor(x) for x in inputs):


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)
       2634                                 symbol_vals,
       2635                                 session)
    -> 2636         fetched = self._callable_fn(*array_vals)
       2637         return fetched[:len(self.outputs)]
       2638 


    ~/Envs/deep3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)
       1397           ret = tf_session.TF_SessionRunCallable(
       1398               self._session._session, self._handle, args, status,
    -> 1399               run_metadata_ptr)
       1400         if run_metadata:
       1401           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)


    ~/Envs/deep3.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
        524             None, None,
        525             compat.as_text(c_api.TF_Message(self.status.status)),
    --> 526             c_api.TF_GetCode(self.status.status))
        527     # Delete the underlying status object from memory otherwise it stays alive
        528     # as there is a reference to status from this from the traceback due to


    InvalidArgumentError: Received a label value of 14169 which is outside the valid range of [0, 1).  Label values: 6902 12872 288 6208 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12 8656 8602 12677 12724 9116 13911 13717 12843 3966 12675 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 5994 27 6009 8602 2224 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 16 14050 6548 1731 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12 13944 12843 6080 14162 6442 14162 13717 7811 12843 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13911 11 1356 12677 5030 12843 3744 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13881 6902 12677 10711 4161 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 2017 8262 3966 12729 8254 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13942 14162 11316 13437 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3743 14162 13760 13145 14151 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12867 6902 735 6726 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12867 7096 7766 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6314 2085 14162 1356 11626 12287 8602 12675 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5994 288 8395 12813 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13911 7623 12729 11691 9468 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6968 13856 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 7033 2017 8262 5524 6914 10583 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 14162 12475 10107 4861 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 7409 14162 980 288 5338 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13321 17 12675 13748 8262 12677 2183 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13924 3981 6914 7662 14162 766 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13108 12843 3022 14169 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 8653 288 1216 5832 6902 5240 17 6914 6902 5945 12843 5524 10572 8602 6914 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12867 5977 12843 5166 8763 10729 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12287 6249 6008 9011 12677 12645 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 14162 917 8488 1992 288 12133 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6008 5977 288 5774 557 14162 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 13717 12843 7662 288 11953 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 11193 522 6159 12843 4862 6177 10995 1458 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12 316 12677 10812 558 980 14162 917 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5354 8506 8652 17 6407 11 13108 12843 6080 14162 13998 12677 14046 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 14026 6442 12867 7379 12843 13467 316 12675 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
    	 [[{{node loss_16/dense_21_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}} = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _class=["loc:@train...s_grad/mul"], _device="/job:localhost/replica:0/task:0/device:CPU:0"](loss_16/dense_21_loss/Reshape_1, loss_16/dense_21_loss/Cast)]]


## Model with PADD_WORD

### Id2Word


```python
from functools import reduce
from tqdm import tqdm_notebook as tqdm
# from tqdm import trange

def flatten(series: list) -> list:
    """
    Flattens the given list and returns a list of (unique) tokens
    """
    return list(reduce(lambda collect, value: collect | set(value), series, set()))

def unique(vocabulary: list) -> list:
    return list(set(vocabulary))

def lower(vocabulary: list) -> list:
    return list(map(str.lower, vocabulary))

id2en = sorted(unique(lower(flatten(tqdm(raw.English_tokens)))))
id2fr = sorted(unique(lower(flatten(tqdm(raw.French_tokens)))))
len(id2en), len(id2fr)
```


    HBox(children=(IntProgress(value=0, max=160872), HTML(value='')))



    HBox(children=(IntProgress(value=0, max=160872), HTML(value='')))





    (14203, 28647)



### Special words


```python
START_WORD = "<START>"
STOP_WORD = "<STOP>"
PADD_WORD = "_"

SPECIAL_WORDS = [START_WORD, STOP_WORD, PADD_WORD]

assert set(SPECIAL_WORDS) & set(id2en) == set()
assert set(SPECIAL_WORDS) & set(id2fr) == set()

id2en = SPECIAL_WORDS + id2en
id2fr = SPECIAL_WORDS + id2fr
```

### Charset


```python
fr_charset = sorted(unique(flatten(id2fr[len(SPECIAL_WORDS):])))
en_charset = sorted(unique(flatten(id2en[len(SPECIAL_WORDS):])))

from pprint import pprint
print("French chars: ", ", ".join(fr_charset))
print("English chars: ", ", ".join(en_charset))
len(fr_charset), len(en_charset)
```

    French chars:  !, $, %, &, ', (, ), +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, ?, `, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, «, », à, á, â, ç, è, é, ê, ë, î, ï, ô, ö, ù, û, œ, с, ​, –, ‘, ’, …, ‽, ₂
    English chars:  !, $, %, &, ', +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, ?, `, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, ­, ç, é, ö, ú, а, –, —, ‘, ’, ₂, €





    (77, 62)



### Reverse lookup


```python
from typing import Dict

def word2id(elements: list) -> Dict[str, int]:
    return {word: _id for _id, word in enumerate(elements)}
    
en2id = word2id(tqdm(id2en))
fr2id = word2id(tqdm(id2fr))

# Test that for the first 100 elements the two datastructures are conssitent
for i in range(100):
    assert fr2id[id2fr[i]] == i
    assert en2id[id2en[i]] == i
    
# Test that we have the special words in front
assert fr2id[START_WORD] == 0
assert en2id[STOP_WORD] == 1
```


    HBox(children=(IntProgress(value=0, max=14206), HTML(value='')))



    HBox(children=(IntProgress(value=0, max=28650), HTML(value='')))


### Training set


```python
from typing import List, Dict

def indexed(tokens: List[str], vocabulary: Dict[str, int]) -> List[int]:
    """
    Returns a vectorized list for the given token list. 
    The vectorisation is done using the given vocabulary mapping. 
    """
    return [vocabulary[word] for word in tokens]

from functools import partial
fr_indexed = partial(indexed, vocabulary=fr2id)
en_indexed = partial(indexed, vocabulary=en2id)

indexed(phrase(['Va', '!']), vocabulary=fr2id), fr_indexed(phrase(['Va', '!']))
```




    ([0, 27039, 3, 1], [0, 27039, 3, 1])




```python
def phrase(tokens: list) -> list:
    """
    Padds the token list with START and STOP words
    """
    return [START_WORD] + lower(tokens) + [STOP_WORD]

# Each phrase increases with 2 more elements so the maxiums change as well
max_eng_size = raw.English_length.max() + 2
max_fre_size = raw.French_length.max() + 2

phrase(['Va', '!'])
```




    ['<START>', 'va', '!', '<STOP>']




```python
def padd_right(tokens: list, fixed_size=3, padd=PADD_WORD)-> list:
    """
    Adds enought padding characters to the right of the given list to as to make it a fixed size == fixed_size
    """
    l = len(tokens)
    to_padd = max(0, fixed_size - l)
    return tokens + ([padd] * to_padd)

padded = padd_right([1, 2, 3], fixed_size=10, padd=0)

fixed_length_fr = partial(padd_right, fixed_size=max_fre_size, padd=PADD_WORD)
fixed_length_en = partial(padd_right, fixed_size=max_eng_size, padd=PADD_WORD)

assert len(padded) == 10
assert padded[-1] == 0
padded
```




    [1, 2, 3, 0, 0, 0, 0, 0, 0, 0]




```python
max_eng_size, max_fre_size
```




    (53, 60)




```python
fixed_length_fre_generator = map(lambda tokens: fr_indexed(fixed_length_fr(phrase(tokens))), tqdm(raw.French_tokens))
fixed_length_eng_generator = map(lambda tokens: en_indexed(fixed_length_en(phrase(tokens))), tqdm(raw.English_tokens))

encode_input = np.array(list(fixed_length_fre_generator), dtype=np.int32)
decode_input = np.array(list(fixed_length_eng_generator), dtype=np.int32)

decode_output = np.full_like(decode_input, fill_value=en2id[PADD_WORD])
decode_output[:,:-1] = decode_input[:, 1:]

# Test that the decode_output is one timestep ahead
assert np.alltrue(decode_output[:, :max_eng_size-1] == decode_input[:, 1:max_eng_size])

encode_input.shape, decode_input.shape, decode_output.shape
```


    HBox(children=(IntProgress(value=0, max=160872), HTML(value='')))



    HBox(children=(IntProgress(value=0, max=160872), HTML(value='')))





    ((160872, 60), (160872, 53), (160872, 53))




```python
from sklearn.model_selection import train_test_split
encode_input_train, encode_input_test, decode_input_train, decode_input_test, decode_output_train, decode_output_test = train_test_split(encode_input, decode_input, decode_output, test_size=0.2, shuffle=True)

encode_input_train.shape, decode_input_train.shape, decode_output_train.shape
```




    ((128697, 60), (128697, 53), (128697, 53))



### Encoding functions


```python
def decode(indexed_tokens: List[int], vocabulary: List[str]) -> List[str]:
    """
    Decodes a list of token ids to into a list of words, using the given vocabulary
    """
    return [vocabulary[_id] for _id in indexed_tokens]

en_decode = partial(decode, vocabulary=id2en)
fr_decode = partial(decode, vocabulary=id2fr)

to_str(fr_decode(encode_input[0]))
```




    '<START> va ! <STOP> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _'



### Model definition


```python
# Note: turns out that we can pass the shape shape=(None,) and this will make the model a variable length one!
fre = Input(shape=(None,), name="FreTokens")
eng = Input(shape=(None,), name="EngTokens")

fre_emb = Embedding(input_dim=len(id2fr), output_dim=20, name="FreEmbeds")
eng_emb = Embedding(input_dim=len(id2en), output_dim=20, name="EngEmbeds")

fre_lstm = LSTM(50, name="FreLSTM", return_state=True)

# Although we don't need in the trainign session, the internal state of the decoder part
# (so we could pass return_state=False) we want to reuse the same layer in the decoder model
# used for inference, so we use retrurn_state=True after all 
eng_lstm = LSTM(50, name="EngLSTM", return_state=True, return_sequences=True)


# We recieve three things here because we've set return_state=True.
# We also get the internal states of the RNN (hidden, constants)
# 
# hidden = <tf.Tensor 'FreLSTM_4/while/Exit_3:0' shape=(?, 10) dtype=float32>,
# constants = <tf.Tensor 'FreLSTM_4/while/Exit_4:0' shape=(?, 10) dtype=float32>
single_fre_out, fre_hidden_state, fre_constants = fre_lstm(fre_emb(fre))

# At this stage, the single_fre_out is irrelevant to us. What we want to get 
# is only the internal state of the LSTM
_ = single_fre_out

# Initialize the English LSTM with the hidden state that we got by passing through 
# all the words in the French sentence. We basically start from the point that LSTM stopped and we wish to 
# "go back in time" if we can, but reversing in a different language (English).
all_eng_out, eng_hidden_state, eng_constants = eng_lstm(eng_emb(eng), initial_state=[fre_hidden_state, fre_constants])

# I'm still unsure what's the difference bewteen applying TimeDistributed vs. Dense layer directly.
# Both seem to act the same
#
# We will need to reuse the trained Dense layer so we can requre it to the inference inputs
eng_decode = Dense(len(id2en), activation="softmax")
output = eng_decode(all_eng_out)

model = Model(inputs=[fre, eng], outputs=[output])
model.summary()

model.compile(optimizer="rmsprop", loss="categorical_crossentropy")
```

    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    FreTokens (InputLayer)          (None, None)         0                                            
    __________________________________________________________________________________________________
    EngTokens (InputLayer)          (None, None)         0                                            
    __________________________________________________________________________________________________
    FreEmbeds (Embedding)           (None, None, 20)     573000      FreTokens[0][0]                  
    __________________________________________________________________________________________________
    EngEmbeds (Embedding)           (None, None, 20)     284120      EngTokens[0][0]                  
    __________________________________________________________________________________________________
    FreLSTM (LSTM)                  [(None, 50), (None,  14200       FreEmbeds[0][0]                  
    __________________________________________________________________________________________________
    EngLSTM (LSTM)                  [(None, None, 50), ( 14200       EngEmbeds[0][0]                  
                                                                     FreLSTM[0][1]                    
                                                                     FreLSTM[0][2]                    
    __________________________________________________________________________________________________
    dense_31 (Dense)                (None, None, 14206)  724506      EngLSTM[0][0]                    
    ==================================================================================================
    Total params: 1,610,026
    Trainable params: 1,610,026
    Non-trainable params: 0
    __________________________________________________________________________________________________


### Model training


```python
english_language_vocabulary_size = len(id2en)

def data_generator(encode_input, decode_input, decode_output, batch_size=32, subsample=10000):
    assert encode_input.shape[0] == decode_input.shape[0] == decode_output.shape[0]
    length = encode_input.shape[0] if not subsample else subsample
    while True:
        for i in range(0, length, batch_size):
            yield [encode_input[i:i+batch_size, :], decode_input[i:i+batch_size, :]], to_categorical(decode_output[i:i+batch_size, :], num_classes=english_language_vocabulary_size)
            
gen = data_generator(encode_input_train, decode_input_train, decode_output_train, batch_size=batch_size)
[inp, tch], out = next(gen)
inp.shape, tch.shape, out.shape
```




    ((64, 60), (64, 53), (64, 53, 14206))




```python
subsample_train = 10000 # encode_input_train.shape[0]
batch_size = 64
subsample_test = 2000  # encode_input_test.shape[0]
epochs = 100

steps_per_epoch_train = subsample_train // batch_size
steps_per_epoch_test = subsample_test // batch_size

train_gen = data_generator(encode_input_train, decode_input_train, decode_output_train, batch_size=batch_size, subsample=subsample_train)
test_gen = data_generator(encode_input_test, decode_input_test, decode_output_test, batch_size=batch_size, subsample=subsample_test)

history = model.fit_generator(train_gen, steps_per_epoch=steps_per_epoch_train, epochs=epochs, validation_data=test_gen, validation_steps=steps_per_epoch_test)
```

    Epoch 1/100
    156/156 [==============================] - 630s 4s/step - loss: 2.9847 - val_loss: 1.0330
    Epoch 2/100
    113/156 [====================>.........] - ETA: 2:46 - loss: 0.9137


    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    <ipython-input-755-53f05e9370fa> in <module>
         10 test_gen = data_generator(encode_input_test, decode_input_test, decode_output_test, batch_size=batch_size, subsample=subsample_test)
         11 
    ---> 12 history = model.fit_generator(train_gen, steps_per_epoch=steps_per_epoch_train, epochs=epochs, validation_data=test_gen, validation_steps=steps_per_epoch_test)
    

    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs)
         89                 warnings.warn('Update your `' + object_name +
         90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
    ---> 91             return func(*args, **kwargs)
         92         wrapper._original_function = func
         93         return wrapper


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
       1413             use_multiprocessing=use_multiprocessing,
       1414             shuffle=shuffle,
    -> 1415             initial_epoch=initial_epoch)
       1416 
       1417     @interfaces.legacy_generator_methods_support


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
        211                 outs = model.train_on_batch(x, y,
        212                                             sample_weight=sample_weight,
    --> 213                                             class_weight=class_weight)
        214 
        215                 outs = to_list(outs)


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight)
       1213             ins = x + y + sample_weights
       1214         self._make_train_function()
    -> 1215         outputs = self.train_function(ins)
       1216         return unpack_singleton(outputs)
       1217 


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)
       2664                 return self._legacy_call(inputs)
       2665 
    -> 2666             return self._call(inputs)
       2667         else:
       2668             if py_any(is_tensor(x) for x in inputs):


    ~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)
       2634                                 symbol_vals,
       2635                                 session)
    -> 2636         fetched = self._callable_fn(*array_vals)
       2637         return fetched[:len(self.outputs)]
       2638 


    ~/Envs/deep3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)
       1397           ret = tf_session.TF_SessionRunCallable(
       1398               self._session._session, self._handle, args, status,
    -> 1399               run_metadata_ptr)
       1400         if run_metadata:
       1401           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)


    KeyboardInterrupt: 


### Inference model


```python
# get encoder states
encoder = Model(inputs=fre, outputs=[fre_hidden_state, fre_constants])
encoder.summary()

# Make the initial states of the decoder, input symbols
hidden_state = Input(shape=(50,), name="EngHiddLSTM")
const_state = Input(shape=(50,), name="EngConsLSTM")

# wire the input states to the trained LSTM
# we are reusing the eng_emb trained layer to convert token ids into embeddings
# (?) If we pass the initial_state to the LSTM do we still need to reuse the already trained one?
# (?) i.e. does the LSTM have any other state only persisted in the trained LSTM instance?
# (?) EDIT: There are some Wx , Wh matrices what are indeed trained and only stored internally
all_eng_out, eng_hidden_state, eng_constants = eng_lstm(eng_emb(eng), initial_state=[hidden_state, const_state])

# wire the all steps output to the trained eng_decode Dense layer
one_hot_words = eng_decode(all_eng_out)

decoder = Model(inputs=[eng, hidden_state, const_state], outputs=[one_hot_words, eng_hidden_state, eng_constants])
decoder.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    FreTokens (InputLayer)       (None, None)              0         
    _________________________________________________________________
    FreEmbeds (Embedding)        (None, None, 20)          573000    
    _________________________________________________________________
    FreLSTM (LSTM)               [(None, 50), (None, 50),  14200     
    =================================================================
    Total params: 587,200
    Trainable params: 587,200
    Non-trainable params: 0
    _________________________________________________________________
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    EngTokens (InputLayer)          (None, None)         0                                            
    __________________________________________________________________________________________________
    EngEmbeds (Embedding)           (None, None, 20)     284120      EngTokens[0][0]                  
    __________________________________________________________________________________________________
    EngHiddLSTM (InputLayer)        (None, 50)           0                                            
    __________________________________________________________________________________________________
    EngConsLSTM (InputLayer)        (None, 50)           0                                            
    __________________________________________________________________________________________________
    EngLSTM (LSTM)                  [(None, None, 50), ( 14200       EngEmbeds[1][0]                  
                                                                     EngHiddLSTM[0][0]                
                                                                     EngConsLSTM[0][0]                
    __________________________________________________________________________________________________
    dense_31 (Dense)                (None, None, 14206)  724506      EngLSTM[1][0]                    
    ==================================================================================================
    Total params: 1,022,826
    Trainable params: 1,022,826
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
def is_not_finished(generated_english_word_ids):
    """
    Checks if we have a stopping event encountered.
    Resons to stop generating words:
    * STOP_WORD encountered
    * max_lenght words generated
    """
    id_last_word_predicted = generated_english_word_ids[-1]
    
    is_stop_word_hit = id_last_word_predicted == en2id[STOP_WORD]
    is_max_length_hit = len(generated_english_word_ids) == max_eng_size
    
    return not is_stop_word_hit and not is_max_length_hit 


def translate(fre_tokens: List[str], debug=True) -> List[str]:
    """
    Translates the given sentence from French to English.
    
    The input of this function is a list of french word tokens.
    """
    if debug:
        # Scan the full french phrase and return the hidden state of the LSTM
        # This will be used to initialize the first step of the LSTM english decoder
        french_phrase = fr_input(fre_tokens)
        print(f"---------------------------")
        print(f"[0] French: \t{to_str(fr_decode(french_phrase[0]))[:80]}")
        print(f"[0] Indexed: \t{french_phrase[0][:10]}\n")
    
    eng_tokens = [START_WORD]
    eng_inp = en_indexed(eng_tokens)

    [hidden], [const] = encoder.predict([french_phrase])
    
    id_last_word_predicted = eng_inp[-1]
    id_stop_word = en2id[PADD_WORD]
    
    # Continously make predictions until we have a stopping event
    while is_not_finished(eng_inp):
#         fixed_eng_inp = fixed_length_en([eng_inp[-1]], padd=id_stop_word)
        fixed_eng_inp = [eng_inp[-1]]
        
        if debug:
            print(f"[{len(eng_inp)}] Hidden:\t{np.round(hidden, 2)[:10]}")
            print(f"[{len(eng_inp)}] Const:\t{np.round(const, 2)[:10]}")
            print(f"[{len(eng_inp)}] Input: \t{to_str(en_decode(fixed_eng_inp[:20]))}")

        # pass the decoder LSTM hidden staten and the currently decoded words
        [one_hot_words], [hidden], [const] = decoder.predict([[fixed_eng_inp], [hidden], [const]])

        # one_hot_words.shape = (53, 14203)
        # predicted_words = (53,) 
        predicted_words = np.argmax(one_hot_words, axis=-1)

        # the last word is the one corresponding to the last seen french word (if we discard the START_WORD) 
        # id_last_word_predicted = predicted_words[len(eng_inp) - 1]
        id_last_word_predicted = predicted_words[0]
        
        if debug:
            # print the top words predicted, for each timestep
            top_words = np.argsort(one_hot_words, axis=-1)[:, ::-1][:3, :3]
            print(f"[{len(eng_inp)}] Top: \t{[[id2en[_id] for _id in _list] for _list in top_words]}")
            print(f"[{len(eng_inp)}] Pred: \t`{to_str(en_decode(eng_inp))}` ~~~: {id2en[id_last_word_predicted]}\n")
            
        eng_inp.append(id_last_word_predicted)
        

    return en_decode(eng_inp)

to_str(translate(["Allez", "a", "la", "maison", "avec", "nous", "."]))
to_str(translate(["Va", "!"]))
```

    ---------------------------
    [0] French: 	<START> allez a la maison avec nous . <STOP> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
    [0] Indexed: 	[    0   993   237 14566 15703  2177 17677    20     1     2]
    
    [1] Hidden:	[-0.96 -0.93  1.    0.82  0.98 -0.88  0.73 -1.    1.   -0.75]
    [1] Const:	[ -8.59 -30.41  26.13  36.02   3.52  -4.46   1.64 -31.39  15.6  -10.36]
    [1] Input: 	<START>
    [1] Top: 	[['i', 'you', '.']]
    [1] Pred: 	`<START>` ~~~: i
    
    [2] Hidden:	[-0.92 -0.8   0.9   0.83  0.76 -0.99  0.8  -0.89  1.   -0.78]
    [2] Const:	[ -8.44 -30.13  25.31  31.19   3.91  -2.85   1.1  -27.49  14.7  -10.87]
    [2] Input: 	i
    [2] Top: 	[['i', '.', 'you']]
    [2] Pred: 	`<START> i` ~~~: i
    
    [3] Hidden:	[-0.8  -0.67  0.76  0.7   0.61 -0.97  0.54 -0.76  0.87 -0.62]
    [3] Const:	[ -8.14 -29.84  23.35  26.71   4.17  -2.06   0.63 -22.14  12.95 -10.97]
    [3] Input: 	i
    [3] Top: 	[['.', 'i', '<STOP>']]
    [3] Pred: 	`<START> i i` ~~~: .
    
    [4] Hidden:	[-0.73 -0.61  0.7   0.67  0.56 -0.82  0.19 -0.71  0.79 -0.58]
    [4] Const:	[ -7.5  -29.54  21.08  22.35   4.24  -1.47   0.22 -17.05  10.99 -10.53]
    [4] Input: 	.
    [4] Top: 	[['<STOP>', '.', '_']]
    [4] Pred: 	`<START> i i .` ~~~: <STOP>
    
    ---------------------------
    [0] French: 	<START> va ! <STOP> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
    [0] Indexed: 	[    0 27039     3     1     2     2     2     2     2     2]
    
    [1] Hidden:	[-0.96 -0.93  1.    0.82  0.98 -0.88  0.73 -1.    1.   -0.75]
    [1] Const:	[ -8.92 -31.24  26.84  36.98   3.52  -4.46   1.64 -32.29  15.76 -10.64]
    [1] Input: 	<START>
    [1] Top: 	[['i', 'you', '.']]
    [1] Pred: 	`<START>` ~~~: i
    
    [2] Hidden:	[-0.92 -0.8   0.9   0.83  0.76 -0.99  0.8  -0.89  1.   -0.78]
    [2] Const:	[ -8.75 -30.96  25.98  32.01   3.92  -2.85   1.1  -28.26  14.84 -11.16]
    [2] Input: 	i
    [2] Top: 	[['i', '.', 'you']]
    [2] Pred: 	`<START> i` ~~~: i
    
    [3] Hidden:	[-0.8  -0.67  0.76  0.7   0.61 -0.97  0.54 -0.76  0.87 -0.62]
    [3] Const:	[ -8.41 -30.66  23.96  27.4    4.17  -2.06   0.63 -22.75  13.07 -11.24]
    [3] Input: 	i
    [3] Top: 	[['.', 'i', '<STOP>']]
    [3] Pred: 	`<START> i i` ~~~: .
    
    [4] Hidden:	[-0.73 -0.61  0.7   0.67  0.56 -0.82  0.19 -0.71  0.79 -0.58]
    [4] Const:	[ -7.74 -30.37  21.61  22.91   4.25  -1.47   0.22 -17.5   11.09 -10.78]
    [4] Input: 	.
    [4] Top: 	[['<STOP>', '.', '_']]
    [4] Pred: 	`<START> i i .` ~~~: <STOP>
    





    '<START> i i . <STOP>'




```python
def strip_indexed(tokens):
    """
    Function that strips all the decorations from a indexed training sequence (containing ids).
    
    It removes the following:
    * the initial START_WORD
    * the ending STOP_WORD
    * all the padding words added with fixed_length variant functions

    """
    tokens = tokens.tolist()
    # exclude the initial START_WORD, the STOP_WORD and everything after it (padding words)
    return tokens[1:tokens.index(en2id[STOP_WORD])]

def strip_plaintext(tokens):
    """
    Function that strips all the decorations from a plaintext training sequence (containing actual words).
    
    It removes the following:
    * the initial START_WORD
    * the ending STOP_WORD
    * all the padding words added with fixed_length variant functions

    """

    # exclude the initial START_WORD, the STOP_WORD and everything after it (padding words)
    return tokens[1:tokens.index(STOP_WORD)]

def strip(tokens):
    """
    Function that strips all the decorations from a training sequence.
    It removes the following:
    * the initial START_WORD
    * the ending STOP_WORD
    * all the padding words added with fixed_length variant functions
    
    This function can be applied to both indexed or plaintext sequences.
    """
    
    def __are_plaintext(tokens):
        # Heuristic to test if the tokens are in plaintext or not.
        # Assumed plaintext if known plaintext words (almost always guaranteed to be present) 
        # are included among the tokens.
        # 
        # These signal words are:
        # START_WORD, STOP_WORD
        return STOP_WORD in tokens or START_WORD in tokens 
        
    if __are_plaintext(tokens):
        return strip_plaintext(tokens)
    else:
        return strip_indexed(tokens)

strip(fr_decode(encode_input_test[1]))
```




    ['nous', "n'avons", 'eu', 'aucun', 'accident', 'majeur', '.']



# Trash and experiments


```python
from keras.preprocessing.sequence import TimeseriesGenerator
gen = TimeseriesGenerator(encode_input_train, decode_input_train, 100, batch_size=32)

x, y = gen[0]
encode_input.shape, decode_input.shape, x.shape, y.shape
```




    ((160872, 60), (160872, 53), (32, 100, 60), (32, 53))


