{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Getting-the-data\" data-toc-modified-id=\"Getting-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Getting the data</a></span></li><li><span><a href=\"#Loading-the-zip-file-object\" data-toc-modified-id=\"Loading-the-zip-file-object-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Loading the zip file object</a></span></li><li><span><a href=\"#Basic-transforms\" data-toc-modified-id=\"Basic-transforms-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Basic transforms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pandas-df\" data-toc-modified-id=\"Pandas-df-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Pandas df</a></span></li><li><span><a href=\"#NLTK-Word-tokenisation\" data-toc-modified-id=\"NLTK-Word-tokenisation-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>NLTK Word tokenisation</a></span></li><li><span><a href=\"#Saving-the-pandas-dataframe\" data-toc-modified-id=\"Saving-the-pandas-dataframe-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Saving the pandas dataframe</a></span></li><li><span><a href=\"#Loading-the-pandas-dataframe\" data-toc-modified-id=\"Loading-the-pandas-dataframe-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Loading the pandas dataframe</a></span></li></ul></li><li><span><a href=\"#Data-processing\" data-toc-modified-id=\"Data-processing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data processing</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Vocabulary\" data-toc-modified-id=\"Vocabulary-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Vocabulary</a></span><ul class=\"toc-item\"><li><span><a href=\"#Id2Word\" data-toc-modified-id=\"Id2Word-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Id2Word</a></span></li><li><span><a href=\"#Special-words\" data-toc-modified-id=\"Special-words-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Special words</a></span></li></ul></li><li><span><a href=\"#Reversed-lookup-dictionary\" data-toc-modified-id=\"Reversed-lookup-dictionary-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Reversed lookup dictionary</a></span></li><li><span><a href=\"#Charset\" data-toc-modified-id=\"Charset-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Charset</a></span></li><li><span><a href=\"#Windowed-data\" data-toc-modified-id=\"Windowed-data-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Windowed data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-deques-experiments\" data-toc-modified-id=\"Using-deques-experiments-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>Using deques experiments</a></span></li><li><span><a href=\"#Rolling-window-functions\" data-toc-modified-id=\"Rolling-window-functions-5.5.2\"><span class=\"toc-item-num\">5.5.2&nbsp;&nbsp;</span>Rolling window functions</a></span></li></ul></li><li><span><a href=\"#Data-formatting\" data-toc-modified-id=\"Data-formatting-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Data formatting</a></span></li><li><span><a href=\"#Debug-functions\" data-toc-modified-id=\"Debug-functions-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Debug functions</a></span></li><li><span><a href=\"#Training-set\" data-toc-modified-id=\"Training-set-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Training set</a></span></li><li><span><a href=\"#Model-implementation\" data-toc-modified-id=\"Model-implementation-5.9\"><span class=\"toc-item-num\">5.9&nbsp;&nbsp;</span>Model implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sparse-categorical-digression\" data-toc-modified-id=\"Sparse-categorical-digression-5.9.1\"><span class=\"toc-item-num\">5.9.1&nbsp;&nbsp;</span>Sparse categorical digression</a></span></li><li><span><a href=\"#Memory-optimsations\" data-toc-modified-id=\"Memory-optimsations-5.9.2\"><span class=\"toc-item-num\">5.9.2&nbsp;&nbsp;</span>Memory optimsations</a></span></li><li><span><a href=\"#Training-(with-generators)\" data-toc-modified-id=\"Training-(with-generators)-5.9.3\"><span class=\"toc-item-num\">5.9.3&nbsp;&nbsp;</span>Training (with generators)</a></span></li><li><span><a href=\"#Plot-the-model\" data-toc-modified-id=\"Plot-the-model-5.9.4\"><span class=\"toc-item-num\">5.9.4&nbsp;&nbsp;</span>Plot the model</a></span></li><li><span><a href=\"#Inference-model\" data-toc-modified-id=\"Inference-model-5.9.5\"><span class=\"toc-item-num\">5.9.5&nbsp;&nbsp;</span>Inference model</a></span></li><li><span><a href=\"#Line-by-line-beam-search\" data-toc-modified-id=\"Line-by-line-beam-search-5.9.6\"><span class=\"toc-item-num\">5.9.6&nbsp;&nbsp;</span>Line by line beam search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loop\" data-toc-modified-id=\"Loop-5.9.6.1\"><span class=\"toc-item-num\">5.9.6.1&nbsp;&nbsp;</span>Loop</a></span></li></ul></li></ul></li><li><span><a href=\"#Complete-beam_search\" data-toc-modified-id=\"Complete-beam_search-5.10\"><span class=\"toc-item-num\">5.10&nbsp;&nbsp;</span>Complete beam_search</a></span></li><li><span><a href=\"#Model-with-sparse_categorical_crossentropy-training\" data-toc-modified-id=\"Model-with-sparse_categorical_crossentropy-training-5.11\"><span class=\"toc-item-num\">5.11&nbsp;&nbsp;</span>Model with sparse_categorical_crossentropy training</a></span></li><li><span><a href=\"#Model-with-PADD_WORD\" data-toc-modified-id=\"Model-with-PADD_WORD-5.12\"><span class=\"toc-item-num\">5.12&nbsp;&nbsp;</span>Model with PADD_WORD</a></span><ul class=\"toc-item\"><li><span><a href=\"#Id2Word\" data-toc-modified-id=\"Id2Word-5.12.1\"><span class=\"toc-item-num\">5.12.1&nbsp;&nbsp;</span>Id2Word</a></span></li><li><span><a href=\"#Special-words\" data-toc-modified-id=\"Special-words-5.12.2\"><span class=\"toc-item-num\">5.12.2&nbsp;&nbsp;</span>Special words</a></span></li><li><span><a href=\"#Charset\" data-toc-modified-id=\"Charset-5.12.3\"><span class=\"toc-item-num\">5.12.3&nbsp;&nbsp;</span>Charset</a></span></li><li><span><a href=\"#Reverse-lookup\" data-toc-modified-id=\"Reverse-lookup-5.12.4\"><span class=\"toc-item-num\">5.12.4&nbsp;&nbsp;</span>Reverse lookup</a></span></li><li><span><a href=\"#Training-set\" data-toc-modified-id=\"Training-set-5.12.5\"><span class=\"toc-item-num\">5.12.5&nbsp;&nbsp;</span>Training set</a></span></li><li><span><a href=\"#Encoding-functions\" data-toc-modified-id=\"Encoding-functions-5.12.6\"><span class=\"toc-item-num\">5.12.6&nbsp;&nbsp;</span>Encoding functions</a></span></li><li><span><a href=\"#Model-definition\" data-toc-modified-id=\"Model-definition-5.12.7\"><span class=\"toc-item-num\">5.12.7&nbsp;&nbsp;</span>Model definition</a></span></li><li><span><a href=\"#Model-training\" data-toc-modified-id=\"Model-training-5.12.8\"><span class=\"toc-item-num\">5.12.8&nbsp;&nbsp;</span>Model training</a></span></li><li><span><a href=\"#Inference-model\" data-toc-modified-id=\"Inference-model-5.12.9\"><span class=\"toc-item-num\">5.12.9&nbsp;&nbsp;</span>Inference model</a></span></li></ul></li></ul></li><li><span><a href=\"#Trash-and-experiments\" data-toc-modified-id=\"Trash-and-experiments-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Trash and experiments</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Data is downloaded from \"http://www.manythings.org/anki/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:07:29.831812Z",
     "start_time": "2019-02-08T15:07:29.821119Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Untitled/keras/course_fast_ai/part 2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:07:34.848058Z",
     "start_time": "2019-02-08T15:07:34.326402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:07:38.868952Z",
     "start_time": "2019-02-08T15:07:36.772938Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-08 17:07:36--  http://www.manythings.org/anki/fra-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196\n",
      "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3305307 (3.2M) [application/zip]\n",
      "Saving to: './data/fra-eng.zip'\n",
      "\n",
      "./data/fra-eng.zip  100%[===================>]   3.15M  2.45MB/s    in 1.3s    \n",
      "\n",
      "2019-02-08 17:07:38 (2.45 MB/s) - './data/fra-eng.zip' saved [3305307/3305307]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"http://www.manythings.org/anki/fra-eng.zip\" -O ./data/fra-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the zip file object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:03.951377Z",
     "start_time": "2019-02-08T15:32:03.944057Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "z = zipfile.ZipFile(\"./data/fra-eng.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the filenames inside the archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:05.202418Z",
     "start_time": "2019-02-08T15:32:05.191906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ZipInfo filename='_about.txt' compress_type=deflate filemode='-rw-r--r--' file_size=1441 compress_size=701>,\n",
       " <ZipInfo filename='fra.txt' compress_type=deflate filemode='-rw-r--r--' file_size=11245636 compress_size=3304398>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from the fra.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:06.533310Z",
     "start_time": "2019-02-08T15:32:06.447431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Go.\\tVa !\\nHi.\\tSalut !\\nRun!\\tCours\\xe2\\x80\\xaf!\\nRun!\\tCourez\\xe2\\x80\\xaf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.read(\"fra.txt\")[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:08.015962Z",
     "start_time": "2019-02-08T15:32:07.520137Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:08.231689Z",
     "start_time": "2019-02-08T15:32:08.018566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/fra.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.extract(\"fra.txt\", \"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:09.429842Z",
     "start_time": "2019-02-08T15:32:09.360170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tVa !',\n",
       " 'Hi.\\tSalut !',\n",
       " 'Run!\\tCours\\u202f!',\n",
       " 'Run!\\tCourez\\u202f!',\n",
       " 'Wow!\\tÇa alors\\u202f!',\n",
       " 'Fire!\\tAu feu !',\n",
       " \"Help!\\tÀ l'aide\\u202f!\",\n",
       " 'Jump.\\tSaute.',\n",
       " 'Stop!\\tÇa suffit\\u202f!',\n",
       " 'Stop!\\tStop\\u202f!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/fra.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    buffer = f.read() \n",
    "    lines = buffer.split(\"\\n\")\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:12.179161Z",
     "start_time": "2019-02-08T15:32:12.174634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160873"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pandas df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Reading the file intor a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:14.414254Z",
     "start_time": "2019-02-08T15:32:14.102771Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English      French\n",
       "0     Go.        Va !\n",
       "1     Hi.     Salut !\n",
       "2    Run!     Cours !\n",
       "3    Run!    Courez !\n",
       "4    Wow!  Ça alors !"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_csv(\"./data/fra.txt\", sep=\"\\t\", header=None, names=[\"English\", \"French\"])\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NLTK Word tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:16.975515Z",
     "start_time": "2019-02-08T15:32:16.190787Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:19.449903Z",
     "start_time": "2019-02-08T15:32:19.430571Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Va', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Va!\", language=\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:38.304987Z",
     "start_time": "2019-02-08T15:32:20.471919Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>French_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>[Va, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>[Salut, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "      <td>[Cours, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "      <td>[Courez, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "      <td>[Ça, alors, !]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English      French   French_tokens\n",
       "0     Go.        Va !         [Va, !]\n",
       "1     Hi.     Salut !      [Salut, !]\n",
       "2    Run!     Cours !      [Cours, !]\n",
       "3    Run!    Courez !     [Courez, !]\n",
       "4    Wow!  Ça alors !  [Ça, alors, !]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw['French_tokens'] = raw.French.transform(lambda x: word_tokenize(x, language=\"french\"))\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:32:58.539517Z",
     "start_time": "2019-02-08T15:32:41.000077Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>French_tokens</th>\n",
       "      <th>English_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>[Va, !]</td>\n",
       "      <td>[Go, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>[Salut, !]</td>\n",
       "      <td>[Hi, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "      <td>[Cours, !]</td>\n",
       "      <td>[Run, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "      <td>[Courez, !]</td>\n",
       "      <td>[Run, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "      <td>[Ça, alors, !]</td>\n",
       "      <td>[Wow, !]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English      French   French_tokens English_tokens\n",
       "0     Go.        Va !         [Va, !]        [Go, .]\n",
       "1     Hi.     Salut !      [Salut, !]        [Hi, .]\n",
       "2    Run!     Cours !      [Cours, !]       [Run, !]\n",
       "3    Run!    Courez !     [Courez, !]       [Run, !]\n",
       "4    Wow!  Ça alors !  [Ça, alors, !]       [Wow, !]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw['English_tokens'] = raw.English.transform(lambda x: word_tokenize(x, language=\"english\"))\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:33:00.476640Z",
     "start_time": "2019-02-08T15:33:00.328565Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>French_tokens</th>\n",
       "      <th>English_tokens</th>\n",
       "      <th>English_length</th>\n",
       "      <th>French_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>[Va, !]</td>\n",
       "      <td>[Go, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>[Salut, !]</td>\n",
       "      <td>[Hi, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "      <td>[Cours, !]</td>\n",
       "      <td>[Run, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "      <td>[Courez, !]</td>\n",
       "      <td>[Run, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "      <td>[Ça, alors, !]</td>\n",
       "      <td>[Wow, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English      French   French_tokens English_tokens  English_length  \\\n",
       "0     Go.        Va !         [Va, !]        [Go, .]               2   \n",
       "1     Hi.     Salut !      [Salut, !]        [Hi, .]               2   \n",
       "2    Run!     Cours !      [Cours, !]       [Run, !]               2   \n",
       "3    Run!    Courez !     [Courez, !]       [Run, !]               2   \n",
       "4    Wow!  Ça alors !  [Ça, alors, !]       [Wow, !]               2   \n",
       "\n",
       "   French_length  \n",
       "0              2  \n",
       "1              2  \n",
       "2              2  \n",
       "3              2  \n",
       "4              3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw['English_length'] = raw.English_tokens.transform(lambda x: len(x))\n",
    "raw['French_length'] = raw.French_tokens.transform(lambda x: len(x))\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Saving the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:33:05.371432Z",
     "start_time": "2019-02-08T15:33:04.359113Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw.to_parquet(\"./data/fra_df.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:10:06.983424Z",
     "start_time": "2019-02-08T15:10:06.312189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>French_tokens</th>\n",
       "      <th>English_tokens</th>\n",
       "      <th>English_length</th>\n",
       "      <th>French_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>[Va, !]</td>\n",
       "      <td>[Go, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>[Salut, !]</td>\n",
       "      <td>[Hi, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "      <td>[Cours, !]</td>\n",
       "      <td>[Run, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "      <td>[Courez, !]</td>\n",
       "      <td>[Run, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "      <td>[Ça, alors, !]</td>\n",
       "      <td>[Wow, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English      French   French_tokens English_tokens  English_length  \\\n",
       "0     Go.        Va !         [Va, !]        [Go, .]               2   \n",
       "1     Hi.     Salut !      [Salut, !]        [Hi, .]               2   \n",
       "2    Run!     Cours !      [Cours, !]       [Run, !]               2   \n",
       "3    Run!    Courez !     [Courez, !]       [Run, !]               2   \n",
       "4    Wow!  Ça alors !  [Ça, alors, !]       [Wow, !]               2   \n",
       "\n",
       "   French_length  \n",
       "0              2  \n",
       "1              2  \n",
       "2              2  \n",
       "3              2  \n",
       "4              3  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_parquet(\"./data/fra_df.hdf5\")\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum sentences lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:33:27.836255Z",
     "start_time": "2019-02-08T15:33:27.829403Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 60)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.English_length.max(), raw.French_length.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of long sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:33:30.189165Z",
     "start_time": "2019-02-08T15:33:30.158217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\",\n",
       "        \"Si quelqu'un qui ne connaît pas vos antécédents dit que vous parlez comme un locuteur natif, cela veut dire qu'il a probablement remarqué quelque chose à propos de votre élocution qui l'a fait prendre conscience que vous n'êtes pas un locuteur natif. En d'autres termes, vous ne parlez pas vraiment comme un locuteur natif.\"],\n",
       "       ['It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.',\n",
       "        \"Il est peut-être impossible d'obtenir un Corpus complètement dénué de fautes, étant donnée la nature de ce type d'entreprise collaborative. Cependant, si nous encourageons les membres à produire des phrases dans leurs propres langues plutôt que d'expérimenter dans les langues qu'ils apprennent, nous pourrions être en mesure de réduire les erreurs.\"]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[raw.English_length == 51][['English', 'French']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of distinct words in each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:33:32.451891Z",
     "start_time": "2019-02-08T15:33:32.434623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>French_tokens</th>\n",
       "      <th>English_tokens</th>\n",
       "      <th>English_length</th>\n",
       "      <th>French_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>[Va, !]</td>\n",
       "      <td>[Go, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>[Salut, !]</td>\n",
       "      <td>[Hi, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "      <td>[Cours, !]</td>\n",
       "      <td>[Run, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "      <td>[Courez, !]</td>\n",
       "      <td>[Run, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "      <td>[Ça, alors, !]</td>\n",
       "      <td>[Wow, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English      French   French_tokens English_tokens  English_length  \\\n",
       "0     Go.        Va !         [Va, !]        [Go, .]               2   \n",
       "1     Hi.     Salut !      [Salut, !]        [Hi, .]               2   \n",
       "2    Run!     Cours !      [Cours, !]       [Run, !]               2   \n",
       "3    Run!    Courez !     [Courez, !]       [Run, !]               2   \n",
       "4    Wow!  Ça alors !  [Ça, alors, !]       [Wow, !]               2   \n",
       "\n",
       "   French_length  \n",
       "0              2  \n",
       "1              2  \n",
       "2              2  \n",
       "3              2  \n",
       "4              3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:33:33.987320Z",
     "start_time": "2019-02-08T15:33:33.982313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Va', '!'],\n",
       " ['Salut', '!'],\n",
       " ['Cours', '!'],\n",
       " ['Courez', '!'],\n",
       " ['Ça', 'alors', '!'],\n",
       " ['Au', 'feu', '!'],\n",
       " ['À', \"l'aide\", '!'],\n",
       " ['Saute', '.'],\n",
       " ['Ça', 'suffit', '!'],\n",
       " ['Stop', '!']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.French_tokens.values[:10].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:33:36.580312Z",
     "start_time": "2019-02-08T15:33:36.573037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 60)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_eng_size = raw.English_length.max()\n",
    "max_fre_size = raw.French_length.max()\n",
    "\n",
    "max_eng_size, max_fre_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id2Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:55:25.428963Z",
     "start_time": "2019-02-08T15:55:25.426243Z"
    }
   },
   "outputs": [],
   "source": [
    "subsample = 10000\n",
    "subsample = -1 # all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:17.616536Z",
     "start_time": "2019-02-08T15:55:26.179575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d2f6d58bc04276bdfe917a366a3d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d92b227e3d34b50afcd67be1898e370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14200, 28614)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm import trange\n",
    "\n",
    "def flatten(series: list) -> list:\n",
    "    \"\"\"\n",
    "    Flattens the given list and returns a list of (unique) tokens\n",
    "    \"\"\"\n",
    "    return list(reduce(lambda collect, value: collect | set(value), series, set()))\n",
    "\n",
    "def unique(vocabulary: list) -> list:\n",
    "    return list(set(vocabulary))\n",
    "\n",
    "def lower(vocabulary: list) -> list:\n",
    "    return list(map(str.lower, vocabulary))\n",
    "\n",
    "id2en = sorted(unique(lower(flatten(tqdm(raw.English_tokens[:subsample])))))\n",
    "id2fr = sorted(unique(lower(flatten(tqdm(raw.French_tokens[:subsample])))))\n",
    "\n",
    "max_eng_size = raw.English_length[:subsample].max()\n",
    "max_fre_size = raw.French_length[:subsample].max()\n",
    "\n",
    "len(id2en), len(id2fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:20.724551Z",
     "start_time": "2019-02-08T15:57:20.716520Z"
    }
   },
   "outputs": [],
   "source": [
    "START_WORD = \">\"\n",
    "STOP_WORD = \"_\"\n",
    "\n",
    "SPECIAL_WORDS = [START_WORD, STOP_WORD]\n",
    "\n",
    "assert set(SPECIAL_WORDS) & set(id2en) == set()\n",
    "assert set(SPECIAL_WORDS) & set(id2fr) == set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the special words as the first elements of each vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:22.165559Z",
     "start_time": "2019-02-08T15:57:22.162059Z"
    }
   },
   "outputs": [],
   "source": [
    "id2en = SPECIAL_WORDS + id2en\n",
    "id2fr = SPECIAL_WORDS + id2fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T09:39:33.041466Z",
     "start_time": "2019-02-11T09:39:32.931713Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savez(\"./data/fr_en_vocabulary.npz\", \n",
    "         id2en=id2en, \n",
    "         id2fr=id2fr, \n",
    "         max_eng_size=max_eng_size, \n",
    "         max_fre_size=max_fre_size, \n",
    "         start_word=START_WORD,\n",
    "         stop_word=STOP_WORD\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reversed lookup dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:23.615169Z",
     "start_time": "2019-02-08T15:57:23.524433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203bf3f1c0054fc7983065acdfbd4976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14202), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f91181940044bacb3c9256ff2587726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28616), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def word2id(elements: list) -> Dict[str, int]:\n",
    "    return {word: _id for _id, word in enumerate(elements)}\n",
    "    \n",
    "en2id = word2id(tqdm(id2en))\n",
    "fr2id = word2id(tqdm(id2fr))\n",
    "\n",
    "# Test that for the first 100 elements the two datastructures are conssitent\n",
    "for i in range(100):\n",
    "    assert fr2id[id2fr[i]] == i\n",
    "    assert en2id[id2en[i]] == i\n",
    "    \n",
    "# Test that we have the special words in front\n",
    "assert fr2id[START_WORD] == 0\n",
    "assert en2id[STOP_WORD] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:24.656447Z",
     "start_time": "2019-02-08T15:57:24.652254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>', '_', '!', '$', '%', '&', \"'\", \"''\", \"'maison\", \"'oublie\"]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2fr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:26.043056Z",
     "start_time": "2019-02-08T15:57:25.964088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French chars:  !, $, %, &, ', (, ), +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, >, ?, _, `, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, «, », à, á, â, ç, è, é, ê, ë, î, ï, ô, ö, ù, û, œ, с, ​, –, ‘, ’, …, ‽, ₂\n",
      "English chars:  !, $, %, &, ', +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, >, ?, _, `, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, ­, ç, é, ö, ú, а, –, —, ‘, ’, ₂, €\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(79, 64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_charset = sorted(unique(flatten(id2fr)))\n",
    "en_charset = sorted(unique(flatten(id2en)))\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"French chars: \", \", \".join(fr_charset))\n",
    "print(\"English chars: \", \", \".join(en_charset))\n",
    "len(fr_charset), len(en_charset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Windowed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Using deques experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([5, 6, 7, 8])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = deque([1, 2, 3, 4, 5, 6, 7, 8], maxlen=4)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([6, 7, 8, 9])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.append(9)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Rolling window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4, '_', '_'], [3, 4, '_'], [2, 3, 4], [1, 2, 3]]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def window(values: list, size: int=3, padding=\"_\"):\n",
    "    print(type(size))\n",
    "    size = int(size)   # np.int64 won't do!\n",
    "    padded_list = [padding] * (size + 1)\n",
    "    d = deque(padded_list, maxlen=size)\n",
    "    for element in values[::-1]:\n",
    "        d.appendleft(element)\n",
    "        yield list(d)\n",
    "\n",
    "list(window([1, 2, 3, 4], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['>', 'Va', '!'], ['Va', '!', '_'], ['!', '_', '_']]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_windows(tokens: list, size:int = 3) -> list:\n",
    "    \"\"\"\n",
    "    Generates all the windows with the given size, from the initial token list\n",
    "    \"\"\"\n",
    "    all_windows = list(window(tokens, size=size, padding=STOP_WORD))\n",
    "    windows_excluding_the_all_stop_list = all_windows[1:]\n",
    "    windows_in_natural_order = windows_excluding_the_all_stop_list[::-1]\n",
    "    return windows_in_natural_order\n",
    "\n",
    "gen_windows([START_WORD, 'Va', '!', STOP_WORD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:31.881273Z",
     "start_time": "2019-02-08T15:57:31.875924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>', 'va', '!', '_']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phrase(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Padds the token list with START and STOP words\n",
    "    \"\"\"\n",
    "    return [START_WORD] + lower(tokens) + [STOP_WORD]\n",
    "\n",
    "# Each phrase increases with 2 more elements so the maxiums change as well\n",
    "max_eng_size = max_eng_size + 2\n",
    "max_fre_size = max_fre_size + 2\n",
    "\n",
    "phrase(['Va', '!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:32.778739Z",
     "start_time": "2019-02-08T15:57:32.771718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 27005, 2, 1], [0, 27005, 2, 1])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def indexed(tokens: List[str], vocabulary: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Returns a vectorized list for the given token list. \n",
    "    The vectorisation is done using the given vocabulary mapping. \n",
    "    \"\"\"\n",
    "    return [vocabulary[word] for word in tokens]\n",
    "\n",
    "from functools import partial\n",
    "fr_indexed = partial(indexed, vocabulary=fr2id)\n",
    "en_indexed = partial(indexed, vocabulary=en2id)\n",
    "\n",
    "indexed(phrase(['Va', '!']), vocabulary=fr2id), fr_indexed(phrase(['Va', '!']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:33.563431Z",
     "start_time": "2019-02-08T15:57:33.556250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def padd_right(tokens: list, fixed_size=3, padd=\"|\")-> list:\n",
    "    \"\"\"\n",
    "    Adds enought padding characters to the right of the given list to as to make it a fixed size == fixed_size\n",
    "    \"\"\"\n",
    "    l = len(tokens)\n",
    "    to_padd = max(0, fixed_size - l)\n",
    "    return list(tokens) + ([padd] * to_padd)\n",
    "\n",
    "padded = padd_right([1, 2, 3], fixed_size=10, padd=0)\n",
    "\n",
    "fixed_length_fr = partial(padd_right, fixed_size=max_fre_size, padd=STOP_WORD)\n",
    "fixed_length_en = partial(padd_right, fixed_size=max_eng_size, padd=STOP_WORD)\n",
    "\n",
    "assert len(padded) == 10\n",
    "assert padded[-1] == 0\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:35.191649Z",
     "start_time": "2019-02-08T15:57:35.187590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n"
     ]
    }
   ],
   "source": [
    "def to_str(tokens: List[object]):\n",
    "    \"\"\"\n",
    "    Pretty prints a list of tokens\n",
    "    \"\"\"\n",
    "    return \" \".join(map(str, tokens))\n",
    "    \n",
    "print(to_str(fixed_length_fr([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:35.799465Z",
     "start_time": "2019-02-08T15:57:35.793662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coincés ! _ _ _ _ _ _ _ _ _ _ _ _ _'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(indexed_tokens: List[int], vocabulary: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Decodes a list of token ids to into a list of words, using the given vocabulary\n",
    "    \"\"\"\n",
    "    return [vocabulary[_id] for _id in indexed_tokens]\n",
    "\n",
    "en_decode = partial(decode, vocabulary=id2en)\n",
    "fr_decode = partial(decode, vocabulary=id2fr)\n",
    "\n",
    "to_str(fr_decode(encode_input[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:42.644158Z",
     "start_time": "2019-02-08T15:57:37.173361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acea558546f84589bfaa5682cb0f86f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2410b18c9304deaa86f1eea573af24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((160871, 62), (160871, 53))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_length_fre_generator = map(lambda tokens: fr_indexed(fixed_length_fr(lower(tokens))), tqdm(raw.French_tokens[:subsample]))\n",
    "fixed_length_eng_generator = map(lambda tokens: en_indexed(fixed_length_en(phrase(tokens))), tqdm(raw.English_tokens[:subsample]))\n",
    "\n",
    "encode_input = np.array(list(fixed_length_fre_generator), dtype=np.int32)\n",
    "decode_input = np.array(list(fixed_length_eng_generator), dtype=np.int32)\n",
    "\n",
    "encode_input.shape, decode_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:45.845214Z",
     "start_time": "2019-02-08T15:57:45.796735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160871, 53)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_output = np.full_like(decode_input, fill_value=en2id[STOP_WORD])\n",
    "\n",
    "decode_output[:,:-1] = decode_input[:, 1:]\n",
    "\n",
    "# Test that the decode_output is one timestep ahead\n",
    "assert np.alltrue(decode_output[:, :max_eng_size-1] == decode_input[:, 1:max_eng_size])\n",
    "\n",
    "decode_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:46.921168Z",
     "start_time": "2019-02-08T15:57:46.916629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0, 5597,   20,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32),\n",
       " array([5597,   20,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_input[0], decode_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model needs to learn that:\n",
    "```\n",
    "0 -> 5599\n",
    "5599 -> 20\n",
    "20 -> 1\n",
    "1 -> 1, etc.\n",
    "```\n",
    "all in one go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:50.814717Z",
     "start_time": "2019-02-08T15:57:50.715231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128696, 62), (128696, 53), (128696, 53))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "encode_input_train, encode_input_test, decode_input_train, decode_input_test, decode_output_train, decode_output_test = train_test_split(encode_input, decode_input, decode_output, test_size=0.2, shuffle=True)\n",
    "\n",
    "encode_input_train.shape, decode_input_train.shape, decode_output_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:57.214259Z",
     "start_time": "2019-02-08T15:57:57.211030Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:57:58.834357Z",
     "start_time": "2019-02-08T15:57:58.327012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "FreTokens (InputLayer)          (None, 62)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngTokens (InputLayer)          (None, 53)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FreEmbeds (Embedding)           (None, 62, 200)      5723200     FreTokens[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "EngEmbeds (Embedding)           (None, 53, 200)      2840400     EngTokens[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FreLSTM (LSTM)                  [(None, 201), (None, 323208      FreEmbeds[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "EngLSTM (LSTM)                  [(None, 53, 201), (N 323208      EngEmbeds[0][0]                  \n",
      "                                                                 FreLSTM[0][1]                    \n",
      "                                                                 FreLSTM[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 53, 14202)    2868804     EngLSTM[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,078,820\n",
      "Trainable params: 12,078,820\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Note: turns out that we can pass the shape shape=(None,) and this will make the model a variable length one!\n",
    "fre = Input(shape=(max_fre_size,), name=\"FreTokens\")\n",
    "eng = Input(shape=(max_eng_size,), name=\"EngTokens\")\n",
    "\n",
    "fre_emb = Embedding(input_dim=len(id2fr), output_dim=200, name=\"FreEmbeds\")\n",
    "eng_emb = Embedding(input_dim=len(id2en), output_dim=200, name=\"EngEmbeds\")\n",
    "\n",
    "# TODO: this shouldn't have used return_state=True!!\n",
    "fre_lstm = LSTM(201, name=\"FreLSTM\", return_state=True)\n",
    "\n",
    "# Although we don't need in the trainign session, the internal state of the decoder part\n",
    "# (so we could pass return_state=False) we want to reuse the same layer in the decoder model\n",
    "# used for inference, so we use retrurn_state=True after all \n",
    "eng_lstm = LSTM(201, name=\"EngLSTM\", return_state=True, return_sequences=True)\n",
    "\n",
    "\n",
    "# We recieve three things here because we've set return_state=True.\n",
    "# We also get the internal states of the RNN (hidden, constants)\n",
    "# \n",
    "# hidden = <tf.Tensor 'FreLSTM_4/while/Exit_3:0' shape=(?, 10) dtype=float32>,\n",
    "# constants = <tf.Tensor 'FreLSTM_4/while/Exit_4:0' shape=(?, 10) dtype=float32>\n",
    "single_fre_out, fre_hidden_state, fre_constants = fre_lstm(fre_emb(fre))\n",
    "\n",
    "# At this stage, the single_fre_out is irrelevant to us. What we want to get \n",
    "# is only the internal state of the LSTM\n",
    "_ = single_fre_out\n",
    "\n",
    "# Initialize the English LSTM with the hidden state that we got by passing through \n",
    "# all the words in the French sentence. We basically start from the point that LSTM stopped and we wish to \n",
    "# \"go back in time\" if we can, but reversing in a different language (English).\n",
    "all_eng_out, eng_hidden_state, eng_constants = eng_lstm(eng_emb(eng), initial_state=[fre_hidden_state, fre_constants])\n",
    "\n",
    "# I'm still unsure what's the difference bewteen applying TimeDistributed vs. Dense layer directly.\n",
    "# Both seem to act the same\n",
    "#\n",
    "# We will need to reuse the trained Dense layer so we can requre it to the inference inputs\n",
    "eng_decode = Dense(len(id2en), activation=\"softmax\")\n",
    "output = eng_decode(all_eng_out)\n",
    "\n",
    "model = Model(inputs=[fre, eng], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:58:01.952092Z",
     "start_time": "2019-02-08T15:58:01.948206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'EngLSTM_2/while/Exit_4:0' shape=(?, 201) dtype=float32>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sparse categorical digression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Depending on what we end up getting in the training data, the one-hot-encoding will miss some rare words (from the end of the vocabulary) that will be found among the decoder_output values, so (probably) the \"sparse_categorical_crossentropy\" won't work (because most probably it inferes the right dimensions of the inlined one-hot-encoding from the input data).\n",
    "\n",
    "This is why (I believe) that it was suggested ([here](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)) to one-hot-encode the outputs by hand. \n",
    "\n",
    "It seems we can't really do this on this computer because we get OutOfMemory error.\n",
    "\n",
    "If this is the case, the only other option would be to prune from the decoder_output train set, all the rows that contain ids which are nowhere to be found in the decode_inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 53, 14091), (200, 53, 14163))"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "decode_output.shape\n",
    "to_categorical(decode_output[:10]).shape, to_categorical(decode_output[:200]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We want to test the above hypotesis and see how many ids only exist in either decode_input or decoder_output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0}, set())"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_in_output = set(np.unique(decode_output.flatten()))\n",
    "ids_in_input = set(np.unique(decode_input.flatten()))\n",
    "\n",
    "only_in_input = ids_in_input - ids_in_output\n",
    "only_in_output = ids_in_output - ids_in_input\n",
    "\n",
    "only_in_input, only_in_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory optimsations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit 100 samples, but going above this, will lead for memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:58:15.408203Z",
     "start_time": "2019-02-08T15:58:07.132925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 7s 83ms/step - loss: 9.4169 - val_loss: 7.5384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x150f52ef0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "model.fit(x=[encode_input_train[:100], decode_input_train[:100]], y=to_categorical(decode_output_train[:100], num_classes=len(id2en)), validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full training data (lables), when one-hot-encoded take arround 11GB of RAM space, which is larger than we might want. We use a generator to split batches and one-hot-encode a batch at a time. This is more time consuming but way more space efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:58:22.542869Z",
     "start_time": "2019-02-08T15:58:22.524757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 62), (64, 53), (64, 53, 14202))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_language_vocabulary_size = len(id2en)\n",
    "batch_size = 64\n",
    "\n",
    "def data_generator(encode_input, decode_input, decode_output, batch_size=32, subsample=10000):\n",
    "    assert encode_input.shape[0] == decode_input.shape[0] == decode_output.shape[0]\n",
    "    length = encode_input.shape[0] if not subsample else subsample\n",
    "    while True:\n",
    "        for i in range(0, length, batch_size):\n",
    "            yield [encode_input[i:i+batch_size, :], decode_input[i:i+batch_size, :]], to_categorical(decode_output[i:i+batch_size, :], num_classes=english_language_vocabulary_size)\n",
    "            \n",
    "gen = data_generator(encode_input_train, decode_input_train, decode_output_train, batch_size=batch_size)\n",
    "[inp, tch], out = next(gen)\n",
    "inp.shape, tch.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:58:24.476685Z",
     "start_time": "2019-02-08T15:58:24.468219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tom nous doit une faveur . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _',\n",
       " '> tom owes us a favor . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ',\n",
       " 'tom owes us a favor . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 11\n",
    "\n",
    "def _fr(data):\n",
    "    return to_str(fr_decode(data))[:100]\n",
    "\n",
    "def _en(data):\n",
    "    return to_str(en_decode(data))[:100]\n",
    "\n",
    "_fr(inp[sample]), _en(tch[sample]), _en(np.argmax(out[sample], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:58:40.423262Z",
     "start_time": "2019-02-08T15:58:40.404258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion CORRECT!\n"
     ]
    }
   ],
   "source": [
    "for t in range(decode_input_test.shape[1] - 1):\n",
    "    assert np.alltrue(decode_input_test[:, t+1] == decode_output_test[:, t])\n",
    "print(\"Assertion CORRECT!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (with generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T14:42:37.443397Z",
     "start_time": "2019-02-10T14:28:11.900968Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 279/2010 [===>..........................] - ETA: 1:29:08 - loss: 0.1640"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-513-a57de0249da2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_output_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubsample_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhistory2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subsample_train = None # encode_input_train.shape[0]\n",
    "batch_size = 64\n",
    "subsample_test = None  # encode_input_test.shape[0]\n",
    "epochs = 100\n",
    "\n",
    "steps_per_epoch_train = subsample_train if subsample_train else encode_input_train.shape[0] // batch_size\n",
    "steps_per_epoch_test = subsample_test if subsample_test else encode_input_test.shape[0] // batch_size\n",
    "\n",
    "train_gen = data_generator(encode_input_train, decode_input_train, decode_output_train, batch_size=batch_size, subsample=subsample_train)\n",
    "test_gen = data_generator(encode_input_test, decode_input_test, decode_output_test, batch_size=batch_size, subsample=subsample_test)\n",
    "\n",
    "history2 = model.fit_generator(train_gen, steps_per_epoch=steps_per_epoch_train, epochs=epochs, validation_data=test_gen, validation_steps=steps_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:43:18.387960Z",
     "start_time": "2019-02-08T15:43:18.102412Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"./data/fre_eng_lstm_fixed.ep100.subsampled.hdf5\",overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./data/fre_eng_lstm_fixed.ep100.subsampled.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T12:04:19.383307Z",
     "start_time": "2019-02-10T12:04:18.767568Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"./data/fre_eng_lstm_fixed.ep19.hdf5\",overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./data/fre_eng_lstm_fixed.ep8.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:02:23.203257Z",
     "start_time": "2019-02-10T15:02:23.186125Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-521-18c84a4b0e8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T13:53:43.840455Z",
     "start_time": "2019-02-10T13:53:43.823742Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-509-87dc1cd5c61b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plot training & validation accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history2' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "history = history2\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T10:10:07.739905Z",
     "start_time": "2019-02-09T10:10:07.732751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[992,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1]]),\n",
       " array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"Allez\", \"!\"]\n",
    "\n",
    "def fr_input(tokens):\n",
    "    return np.expand_dims(np.array(fr_indexed(fixed_length_fr(lower(tokens)))), 0)\n",
    "\n",
    "def en_input(tokens: list):\n",
    "    return np.expand_dims(np.array(en_indexed(fixed_length_en(tokens))), 0)\n",
    "\n",
    "fr_input(tokens), en_input([START_WORD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to manually add one word at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T12:12:24.853668Z",
     "start_time": "2019-02-10T12:12:24.849011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vous', 'ne', 'semblez', 'pas', 'si', 'maligne', '.']"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T14:46:21.053742Z",
     "start_time": "2019-02-10T14:46:20.786108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nous sommes tellement contentes de vous avoir ici !',\n",
       " \"> we 're so glad to have you here . _\")"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = [\"Allez\", \"a\", \"la\", \"voiture\", \"avec\", \"nous\", \".\"]\n",
    "tokens = encode_input_train[12]\n",
    "tokens = strip_plaintext(fr_decode(tokens))\n",
    "\n",
    "pred = [START_WORD]\n",
    "while pred[-1] != STOP_WORD:\n",
    "    p = np.argmax(model.predict(x=[fr_input(tokens), en_input(pred)]), axis=-1)[0][len(pred)-1]\n",
    "    pred.append(id2en[p])\n",
    "\" \".join(tokens), \" \".join(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T20:25:22.131757Z",
     "start_time": "2019-02-09T20:25:21.924356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'what', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['get', \"'s\", '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['get', \"'s\", 'car', 'to', '_', '_', '_', '_', '_', '_']\n",
      "['get', \"'s\", 'car', 'car', 'let', '_', '_', '_', '_', '_']\n",
      "['get', \"'s\", 'car', 'car', 'car', 'felt', '_', '_', '_', '_']\n",
      "['get', \"'s\", 'car', 'car', 'car', 'children', 'arrived', '_', '_', '_']\n",
      "['get', \"'s\", 'car', 'car', 'car', 'children', '_', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"C'est\", \"une\", \"jeune\"]\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\", \"is\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\", \"is\", \"a\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\", \"is\", \"a\", \"little\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\", \"is\", \"a\", \"little\", \"young\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\", \"is\", \"a\", \"little\", \"young\", \".\"])]), axis=-1)[0])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T20:17:23.576188Z",
     "start_time": "2019-02-09T20:17:23.341753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['why', 'why', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['why', 'did', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['why', 'did', 'you', '_', '_', '_', '_', '_', '_', '_']\n",
      "['why', 'did', 'you', 'keep', '_', '_', '_', '_', '_', '_']\n",
      "['why', 'did', 'you', 'keep', 'it', '_', '_', '_', '_', '_']\n",
      "['why', 'did', 'you', 'keep', 'it', 'a', '_', '_', '_', '_']\n",
      "['why', 'did', 'you', 'keep', 'it', 'a', 'secret', '_', '_', '_']\n",
      "['why', 'did', 'you', 'keep', 'it', 'a', 'secret', '?', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"pourquoi\", \"l'as-tu\", \"gardé\", \"secret\", \"?\"] # taken from training set\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\", \"you\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\", \"you\", \"keep\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\", \"you\", \"keep\", \"it\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\", \"you\", \"keep\", \"it\", \"a\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\", \"you\", \"keep\", \"it\", \"a\", \"secret\"])]), axis=-1)[0])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not bad at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get the dimensions wrong and get an error, even if you solve it, because you probabbly was able to execute the eng_lstm call before the error was raised, the lstm bound the erroneus inputs in it's dependency graph and this will make subsequent calls fail (requiring more parameters than needed).\n",
    "\n",
    "The only solution is to reinitialize the lstm (reinstantiate it from the training code), load the model weights (which you hopefully have saved already) and redo the correct bindings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:23:00.487962Z",
     "start_time": "2019-02-10T15:23:00.216673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FreVariableToken (InputLayer (None, None)              0         \n",
      "_________________________________________________________________\n",
      "FreEmbeds (Embedding)        multiple                  5723200   \n",
      "_________________________________________________________________\n",
      "FreLSTM (LSTM)               [(None, 201), (None, 201) 323208    \n",
      "=================================================================\n",
      "Total params: 6,046,408\n",
      "Trainable params: 6,046,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "EngSingleToken (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngEmbeds (Embedding)           multiple             2840400     EngSingleToken[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "EngHiddLSTM (InputLayer)        (None, 201)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngConsLSTM (InputLayer)        (None, 201)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngLSTM (LSTM)                  multiple             323208      EngEmbeds[5][0]                  \n",
      "                                                                 EngHiddLSTM[0][0]                \n",
      "                                                                 EngConsLSTM[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 multiple             2868804     EngLSTM[5][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,032,412\n",
      "Trainable params: 6,032,412\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get encoder states\n",
    "fre_var = Input(shape=(None,), name=\"FreVariableToken\")\n",
    "_, fre_hidden_var, fre_const_var = fre_lstm(fre_emb(fre_var))\n",
    "encoder = Model(inputs=fre_var, outputs=[fre_hidden_var, fre_const_var])\n",
    "encoder.summary()\n",
    "\n",
    "# Make the initial states of the decoder, input symbols\n",
    "eng_one = Input(shape=(1,), name=\"EngSingleToken\")\n",
    "hidden_state = Input(shape=(201,), name=\"EngHiddLSTM\")\n",
    "const_state = Input(shape=(201,), name=\"EngConsLSTM\")\n",
    "\n",
    "# wire the input states to the trained LSTM\n",
    "# we are reusing the eng_emb trained layer to convert token ids into embeddings\n",
    "# (?) If we pass the initial_state to the LSTM do we still need to reuse the already trained one?\n",
    "# (?) i.e. does the LSTM have any other state only persisted in the trained LSTM instance?\n",
    "# (?) EDIT: There are some Wx , Wh matrices what are indeed trained and only stored internally\n",
    "all_eng_out, eng_hidden_state, eng_constants = eng_lstm(eng_emb(eng_one), initial_state=[hidden_state, const_state])\n",
    "\n",
    "# wire the all steps output to the trained eng_decode Dense layer\n",
    "one_hot_words = eng_decode(all_eng_out)\n",
    "\n",
    "decoder = Model(inputs=[eng_one, hidden_state, const_state], outputs=[one_hot_words, eng_hidden_state, eng_constants])\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T10:15:25.168686Z",
     "start_time": "2019-02-09T10:15:24.924220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((201,), (201,))"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[hidden], [const] = encoder.predict(fr_input(tokens))\n",
    "hidden.shape, const.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:16:09.308185Z",
     "start_time": "2019-02-10T15:16:09.303884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_eng_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:17:22.849095Z",
     "start_time": "2019-02-10T15:17:22.779391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "[0] French: \tallez a la maison avec nous . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "[0] Indexed: \t[  992   236 14552 15685  2176 17655    19     1     1     1]\n",
      "\n",
      "[0] Hidden:\t[ 0.07  0.27  0.   -0.    0.27 -0.4   0.21 -0.79  0.   -0.  ]\n",
      "[0] Const:\t[ 0.16  0.72  0.32 -0.53  9.13 -0.74  0.47 -3.32  1.8  -0.  ]\n",
      "[0] Input: \t>\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[0] Top: \t['go', 'turn', 'leave', 'get', 'come', 'keep', 'look', 'stay', 'their', 'take']\n",
      "[0] Score: \t[0.41, 0.2, 0.1, 0.1, 0.03, 0.02, 0.01, 0.01, 0.01, 0.01]\n",
      "[0] Pred: \t`>` ~~~: go\n",
      "\n",
      "[1] Hidden:\t[ 0.09  0.42 -0.55  0.55  0.16 -0.54  0.34 -0.73  0.47  0.11]\n",
      "[1] Const:\t[ 0.1   0.44 -0.62  0.62  8.19 -0.6   0.36 -2.84  0.52  0.11]\n",
      "[1] Input: \tgo\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[1] Top: \t['home', 'to', 'with', 'away', 'into', 'and', 'out', 'back', 'at', 'around']\n",
      "[1] Score: \t[0.92, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[1] Pred: \t`> go` ~~~: home\n",
      "\n",
      "[2] Hidden:\t[ 0.44 -0.52 -0.27 -0.    0.21 -0.72 -0.34 -0.86  0.85  0.  ]\n",
      "[2] Const:\t[ 0.48 -0.58 -0.91 -0.35  9.18 -0.91 -1.   -3.26  1.41  1.  ]\n",
      "[2] Input: \thome\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[2] Top: \t['.', 'with', ',', 'and', '!', 'to', 'by', 'on', 'he', 'without']\n",
      "[2] Score: \t[0.71, 0.14, 0.08, 0.04, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[2] Pred: \t`> go home` ~~~: .\n",
      "\n",
      "[3] Hidden:\t[ 0.76 -0.68 -0.22 -0.26  0.25 -0.4  -0.41 -0.96  0.71  0.39]\n",
      "[3] Const:\t[ 1.   -0.83 -0.22 -1.18  1.92 -0.42 -0.91 -2.    1.77  1.  ]\n",
      "[3] Input: \t.\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[3] Top: \t['_', 'let', 'everyone', '``', 'he', \"''\", 'get', 'please', 'they', 'these']\n",
      "[3] Score: \t[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[3] Pred: \t`> go home .` ~~~: _\n",
      "\n",
      "---------------------------\n",
      "[0] French: \tva ! _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "[0] Indexed: \t[27005     2     1     1     1     1     1     1     1     1]\n",
      "\n",
      "[0] Hidden:\t[ 0.37 -0.07  0.   -0.01  0.46  0.17  0.45 -0.09  0.36 -0.  ]\n",
      "[0] Const:\t[ 0.67 -0.65  1.04 -0.49 22.77  0.18  1.87 -0.09  2.74 -0.  ]\n",
      "[0] Input: \t>\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[0] Top: \t['go', 'get', 'take', 'make', 'run', 'stand', 'have', 'see', 'keep', 'turn']\n",
      "[0] Score: \t[0.83, 0.11, 0.02, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0] Pred: \t`>` ~~~: go\n",
      "\n",
      "[1] Hidden:\t[ 0.5   0.26  0.23  0.51  0.17  0.39  0.96 -0.33  0.82  0.04]\n",
      "[1] Const:\t[ 0.55  0.27  0.24  0.62 21.    0.42  1.92 -0.75  1.15  0.04]\n",
      "[1] Input: \tgo\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[1] Top: \t['on', 'down', 'away', 'off', 'showed', 'together', 'ahead', '!', '.', 'out']\n",
      "[1] Score: \t[0.31, 0.11, 0.07, 0.07, 0.04, 0.03, 0.03, 0.03, 0.02, 0.02]\n",
      "[1] Pred: \t`> go` ~~~: on\n",
      "\n",
      "[2] Hidden:\t[ 0.78  0.69 -0.21 -0.    0.24 -0.04 -0.21 -0.13  0.91  0.  ]\n",
      "[2] Const:\t[ 1.04  1.04 -0.82 -1.   22.   -0.04 -0.65 -0.67  1.54  0.99]\n",
      "[2] Input: \ton\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[2] Top: \t['.', '!', 'your', ',', 'it', '?', 'to', 'her', 'his', 'a']\n",
      "[2] Score: \t[0.93, 0.04, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[2] Pred: \t`> go on` ~~~: .\n",
      "\n",
      "[3] Hidden:\t[ 0.58  0.68  0.25 -0.    0.59 -0.32 -0.64 -0.48  0.96  0.93]\n",
      "[3] Const:\t[ 0.69  1.58  0.26 -1.   23.   -0.34 -0.75 -0.77  1.95  1.64]\n",
      "[3] Input: \t.\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[3] Top: \t['_', \"''\", 'get', 'have', 'go', 'tell', 'take', 'let', 'make', 'come']\n",
      "[3] Score: \t[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[3] Pred: \t`> go on .` ~~~: _\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'> go on . _'"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_not_finished(generated_english_word_ids):\n",
    "    \"\"\"\n",
    "    Checks if we have a stopping event encountered.\n",
    "    Resons to stop generating words:\n",
    "    * STOP_WORD encountered\n",
    "    * max_lenght words generated\n",
    "    \"\"\"\n",
    "    id_last_word_predicted = generated_english_word_ids[-1]\n",
    "    \n",
    "    is_stop_word_hit = id_last_word_predicted == en2id[STOP_WORD]\n",
    "    is_max_length_hit = len(generated_english_word_ids) == max_eng_size\n",
    "    \n",
    "    return not is_stop_word_hit and not is_max_length_hit \n",
    "\n",
    "\n",
    "def translate(fre_tokens: List[str], debug=True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Translates the given sentence from French to English.\n",
    "    \n",
    "    The input of this function is a list of french word tokens.\n",
    "    \"\"\"\n",
    "    # Scan the full french phrase and return the hidden state of the LSTM\n",
    "    # This will be used to initialize the first step of the LSTM english decoder\n",
    "    french_phrase = fr_input(fre_tokens)[0]\n",
    "    \n",
    "    # Somehow it matters to have the full padded string as input\n",
    "    # Sending only the actuall tokens without the padding considerably degrades the performance\n",
    "#     french_phrase = fr_indexed(lower(fre_tokens + [STOP_WORD] * 45))\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"---------------------------\")\n",
    "        print(f\"[0] French: \\t{to_str(fr_decode(french_phrase))[:80]}\")\n",
    "        print(f\"[0] Indexed: \\t{french_phrase[:10]}\\n\")\n",
    "    \n",
    "    eng_tokens = [START_WORD]\n",
    "    eng_inp = en_indexed(lower(eng_tokens))\n",
    "\n",
    "    [hidden], [const] = encoder.predict([[french_phrase]])\n",
    "#     print(hidden.shape, const.shape)\n",
    "#     print(f\"[0] Hidden:\\t{np.round(hidden, 2)[:10]}\")\n",
    "#     print(f\"[0] Const:\\t{np.round(const, 2)[:10]}\")\n",
    "\n",
    "#     print(hidden[:, 0], const[:, 0])\n",
    "        \n",
    "    id_last_word_predicted = eng_inp[-1]\n",
    "#     id_stop_word = en2id[STOP_WORD]\n",
    "    \n",
    "    # Continously make predictions until we have a stopping event\n",
    "    while is_not_finished(eng_inp):\n",
    "#         fixed_eng_inp = fixed_length_en([eng_inp[-1]], padd=id_stop_word)\n",
    "        fixed_eng_inp = [eng_inp[-1]]\n",
    "    \n",
    "        if debug:\n",
    "            print(f\"[{len(eng_inp)-1}] Hidden:\\t{np.round(hidden, 2)[:10]}\")\n",
    "            print(f\"[{len(eng_inp)-1}] Const:\\t{np.round(const, 2)[:10]}\")\n",
    "            print(f\"[{len(eng_inp)-1}] Input: \\t{to_str(en_decode(fixed_eng_inp))}\")\n",
    "\n",
    "        # pass the decoder LSTM hidden staten and the last word we've just predicted\n",
    "        [[[one_hot_words]], [hidden], [const]] = decoder.predict([[fixed_eng_inp], [hidden], [const]])\n",
    "#         [[[one_hot_words], [o2]], [hidden, h1], [const, c1]] = decoder.predict([[fixed_eng_inp, fixed_eng_inp], [hidden, hidden], [const, const]])\n",
    "        print(one_hot_words.shape, hidden.shape, const.shape)\n",
    "        \n",
    "        # one_hot_words.shape = (53, 14203)\n",
    "        # predicted_words = (53,) \n",
    "        predicted_words = np.argmax(one_hot_words, axis=-1)\n",
    "        print(predicted_words.shape)\n",
    "        \n",
    "        # the last word is the one corresponding to the last seen french word (if we discard the START_WORD) \n",
    "        # id_last_word_predicted = predicted_words[len(eng_inp) - 1]\n",
    "        id_last_word_predicted = predicted_words\n",
    "        \n",
    "        if debug:\n",
    "            # print the top words predicted, for each timestep\n",
    "            top_words = np.argsort(one_hot_words)[::-1][:10]\n",
    "            print(f\"[{len(eng_inp)-1}] Top: \\t{[id2en[_id] for _id in top_words]}\")\n",
    "            print(f\"[{len(eng_inp)-1}] Score: \\t{[np.round(one_hot_words[_id], 2) for _id in top_words]}\")\n",
    "            print(f\"[{len(eng_inp)-1}] Pred: \\t`{to_str(en_decode(eng_inp))}` ~~~: {id2en[id_last_word_predicted]}\\n\")\n",
    "            \n",
    "        eng_inp.append(id_last_word_predicted)\n",
    "        \n",
    "\n",
    "    return en_decode(eng_inp)\n",
    "\n",
    "to_str(translate([\"Allez\", \"a\", \"la\", \"maison\", \"avec\", \"nous\", \".\"]))\n",
    "to_str(translate([\"Va\", \"!\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T16:23:26.586764Z",
     "start_time": "2019-02-10T16:23:26.573636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "> _\n",
       "[1.0] 1 1"
      ]
     },
     "execution_count": 971,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Candidate():\n",
    "    def __init__(self):\n",
    "        self.words = list()\n",
    "        self.prob = list()\n",
    "        self.p = 0\n",
    "        self.hidden = None\n",
    "        self.const = None\n",
    "        \n",
    "    def __add_word(word_id):\n",
    "        self.words.append(word_id)\n",
    "    \n",
    "    def __update_probability(self, prob: float):\n",
    "        p_log = -np.log(prob)\n",
    "        self.p += p_log\n",
    "        self.prob.append(prob)\n",
    "    \n",
    "    def __update_current_state(self, hidden, const):\n",
    "        self.hidden = hidden\n",
    "        self.const = const\n",
    "\n",
    "    def add_state(self, word_id, prob, hidden, const):\n",
    "        self.words.append(word_id)\n",
    "        self.__update_probability(prob)\n",
    "        self.__update_current_state(hidden, const)\n",
    "        return self\n",
    "    \n",
    "    def score(self):\n",
    "        return np.exp(np.sum(np.log(self.prob)) / (len(strip_indexed(self.words))**alpha))\n",
    "\n",
    "    def __repr__(self):\n",
    "        score = self.score()\n",
    "        return f\"\\n{to_str(en_decode(self.words))}\\n[{np.round(score, 3)}] {' '.join([str(val) for val in np.round(self.prob, 3)])}\"\n",
    "    \n",
    "Candidate().add_state(en2id[START_WORD], 1, [], []).add_state(en2id[STOP_WORD], 1, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:34.986408Z",
     "start_time": "2019-02-10T15:47:34.981203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[992, 236, 14552, 15685, 2176, 17655, 19]"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"monte dans la camionnette !\".split(\" \")\n",
    "french_phrase = fr_indexed(lower([\"Allez\", \"a\", \"la\", \"maison\", \"avec\", \"nous\", \".\"]))\n",
    "french_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:35.156105Z",
     "start_time": "2019-02-10T15:47:35.148798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((201,), (201,))"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fr_hidden], [fr_const] = encoder.predict([[french_phrase]])\n",
    "fr_hidden.shape, fr_const.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:35.326172Z",
     "start_time": "2019-02-10T15:47:35.320717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " >\n",
       " [1.0] 1, \n",
       " >\n",
       " [1.0] 1, \n",
       " >\n",
       " [1.0] 1, \n",
       " >\n",
       " [1.0] 1, \n",
       " >\n",
       " [1.0] 1]"
      ]
     },
     "execution_count": 888,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width = 5\n",
    "candidates = [Candidate().add_state(word_id=en2id[START_WORD], prob=1, hidden=fr_hidden, const=fr_const)] * width\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:35.482491Z",
     "start_time": "2019-02-10T15:47:35.476493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 1), (5, 201), (5, 201))"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [[candidate.words[-1]] for candidate in candidates]\n",
    "hidden = [candidate.hidden for candidate in candidates]\n",
    "const = [candidate.const for candidate in candidates]\n",
    "np.array(word).shape, np.array(hidden).shape, np.array(const).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:35.662521Z",
     "start_time": "2019-02-10T15:47:35.653025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 1, 14202), (5, 201), (5, 201))"
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[one_hot_words, hiddens, consts] = decoder.predict([word, hidden, const])\n",
    "one_hot_words.shape, hiddens.shape, consts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:35.942079Z",
     "start_time": "2019-02-10T15:47:35.935746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 0, 0])"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([[1, 2, 1, 1, 1], [2, 1, 1, 0, 0], [0, 0, 2, 0, 0]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:36.202336Z",
     "start_time": "2019-02-10T15:47:36.197359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 1, 14202), (5, 14202))"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_words.shape, np.squeeze(one_hot_words, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:36.352031Z",
     "start_time": "2019-02-10T15:47:36.347419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[candidate.p] for candidate in candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:36.517677Z",
     "start_time": "2019-02-10T15:47:36.511783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5,  6,  7],\n",
       "       [ 4,  5,  6,  7,  8,  9],\n",
       "       [ 6,  7,  8,  9, 10, 11]])"
      ]
     },
     "execution_count": 894,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([[1, 2, 3, 4, 5, 6], [2, 3, 4, 5, 6, 7], [3, 4, 5, 6, 7, 8]]) + np.array([[1], [2], [3]])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:36.673852Z",
     "start_time": "2019-02-10T15:47:36.668710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 8, 8, 7])"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[[0, 2, 1, 0], [0, 2, 4, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step of the beam search.. we need unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:36.925698Z",
     "start_time": "2019-02-10T15:47:36.922555Z"
    }
   },
   "outputs": [],
   "source": [
    "prior_prob = np.array([[candidate.p] for candidate in candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:37.277891Z",
     "start_time": "2019-02-10T15:47:37.273890Z"
    }
   },
   "outputs": [],
   "source": [
    "word_prob = np.squeeze(one_hot_words, axis=1)   # reduce middle 1 dimension axis\n",
    "word_log_prob = -np.log(word_prob)  # reason on the negative logs since are more numericaly stable\n",
    "joint_prob = word_log_prob + prior_prob  # update the priors with the current word predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:37.494054Z",
     "start_time": "2019-02-10T15:47:37.484985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([21.92240715, 21.88848305, 21.49939156, 21.30414391, 21.00562477]),\n",
       " array([13953,  7127, 12435, 14157,  2290]),\n",
       " array([0, 0, 0, 0, 0]),\n",
       " array([3.0145272e-10, 3.1185465e-10, 4.6018508e-10, 5.5940652e-10,\n",
       "        7.5400242e-10], dtype=float32))"
      ]
     },
     "execution_count": 898,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_with_max_joint_prob = np.argmax(joint_prob, axis=0)  # get the dimensions on which each word scores best\n",
    "best_join_prob = np.max(joint_prob, axis=0)     # actually compute the best scores for each word\n",
    "\n",
    "best_k_words = np.argsort(best_join_prob)[::-1][:5]\n",
    "prior_prob = best_join_prob[best_k_words]\n",
    "candidates_that_generated_best_words = candidate_with_max_joint_prob[best_k_words]\n",
    "best_word_probab = word_prob[candidates_that_generated_best_words, best_k_words]\n",
    "\n",
    "prior_prob, best_k_words, candidates_that_generated_best_words, best_word_probab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line by line beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step. From this step on, we don't need unicity since we're guaranteed to have started from unque words, ..unless, we choose two continuations from the same root.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:38.475579Z",
     "start_time": "2019-02-10T15:47:38.472619Z"
    }
   },
   "outputs": [],
   "source": [
    "width = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:38.629340Z",
     "start_time": "2019-02-10T15:47:38.623112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  992,   236, 14552, 15685,  2176, 17655,    19,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 900,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"monte dans la camionnette !\".split(\" \")\n",
    "phrase = [\"Allez\", \"a\", \"la\", \"maison\", \"avec\", \"nous\", \".\"]\n",
    "french_phrase = fr_indexed(lower(phrase))\n",
    "french_phrase = fr_input(phrase)[0]\n",
    "french_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:38.784496Z",
     "start_time": "2019-02-10T15:47:38.770988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((201,), (201,))"
      ]
     },
     "execution_count": 901,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fr_hidden], [fr_const] = encoder.predict([[french_phrase]])\n",
    "fr_hidden.shape, fr_const.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:47:38.956904Z",
     "start_time": "2019-02-10T15:47:38.952219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " >\n",
       " [1.0] 1]"
      ]
     },
     "execution_count": 902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = [Candidate().add_state(word_id=en2id[START_WORD], prob=1, hidden=fr_hidden, const=fr_const)]\n",
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:49:00.484179Z",
     "start_time": "2019-02-10T15:49:00.477321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 1), (20, 201), (20, 201))"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = [[candidate.words[-1]] for candidate in candidates]\n",
    "hidden = [candidate.hidden for candidate in candidates]\n",
    "const = [candidate.const for candidate in candidates]\n",
    "np.array(word).shape, np.array(hidden).shape, np.array(const).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:49:00.633685Z",
     "start_time": "2019-02-10T15:49:00.622280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 1, 14202), (20, 201), (20, 201), 14202)"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[one_hot_words, hiddens, consts] = decoder.predict([word, hidden, const])\n",
    "one_hot_length = one_hot_words.shape[-1]\n",
    "\n",
    "\n",
    "one_hot_words.shape, hiddens.shape, consts.shape, one_hot_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:49:00.823744Z",
     "start_time": "2019-02-10T15:49:00.818242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.32064765],\n",
       "       [3.74089378],\n",
       "       [3.98072754],\n",
       "       [4.1721368 ],\n",
       "       [4.4435384 ],\n",
       "       [4.62313444],\n",
       "       [4.71460426],\n",
       "       [4.80957479],\n",
       "       [4.89995852],\n",
       "       [5.12437141],\n",
       "       [5.12614423],\n",
       "       [5.15888573],\n",
       "       [4.49333486],\n",
       "       [5.44525023],\n",
       "       [5.48002739],\n",
       "       [5.63113855],\n",
       "       [4.87495226],\n",
       "       [5.82714534],\n",
       "       [5.98053499],\n",
       "       [6.06416011]])"
      ]
     },
     "execution_count": 950,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_prob = np.array([[candidate.p] for candidate in candidates])\n",
    "prior_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:49:00.966452Z",
     "start_time": "2019-02-10T15:49:00.961578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1)"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_lengths = np.array([[len(strip_indexed(candidate.words))] for candidate in candidates])\n",
    "candidate_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:49:01.185704Z",
     "start_time": "2019-02-10T15:49:01.176706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42.55307334,  1.32064765, 34.61363929, ..., 42.82775443,\n",
       "        40.76403373, 42.54854148],\n",
       "       [36.43469185,  3.74090046, 25.03494018, ..., 36.63613075,\n",
       "        29.9799723 , 36.24504036],\n",
       "       [35.19212089,  3.98072921, 26.74879967, ..., 35.10809074,\n",
       "        30.93763481, 35.04446922],\n",
       "       ...,\n",
       "       [39.13041377,  5.82715285, 27.97051692, ..., 39.09188533,\n",
       "        29.54118037, 38.81853938],\n",
       "       [24.64738412, 13.0365438 , 19.89150186, ..., 24.64166971,\n",
       "        20.35563226, 24.41324563],\n",
       "       [24.8362205 , 35.2090385 , 12.82446361, ..., 24.84001994,\n",
       "        28.52832103, 24.65184093]])"
      ]
     },
     "execution_count": 952,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_prob = np.squeeze(one_hot_words, axis=1)   # reduce middle 1 dimension axis\n",
    "word_log_prob = -np.log(word_prob)  # reason on the negative logs since are more numericaly stable\n",
    "joint_prob = word_log_prob + prior_prob  # update the priors with the current word predictions\n",
    "joint_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:49:01.495208Z",
     "start_time": "2019-02-10T15:49:01.485358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.12459955,  0.50043188, 13.11611662, ..., 16.22868422,\n",
       "        15.44668031, 16.1228823 ],\n",
       "       [10.39463456,  1.06726011,  7.14234267, ..., 10.45210406,\n",
       "         8.5531355 , 10.34052794],\n",
       "       [10.04013531,  1.13568205,  7.63129817, ..., 10.01616193,\n",
       "         8.8263518 ,  9.99801103],\n",
       "       ...,\n",
       "       [11.16371048,  1.66245744,  7.97984797, ..., 11.15271851,\n",
       "         8.42795036, 11.07473428],\n",
       "       [ 7.03177488,  3.71926046,  5.67494556, ...,  7.03014458,\n",
       "         5.80735963,  6.96497634],\n",
       "       [ 7.08564895, 10.04496181,  3.65875504, ...,  7.08673291,\n",
       "         8.13898668,  7.03304638]])"
      ]
     },
     "execution_count": 953,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.7\n",
    "lenght_normalized = 1 / np.power(candidate_lengths, alpha) * joint_prob\n",
    "lenght_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:49:01.905379Z",
     "start_time": "2019-02-10T15:49:01.871334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12.8642416 , 12.8642416 , 12.8642416 , 12.8642416 , 12.8642416 ,\n",
       "        10.17941952, 12.8642416 , 10.17941952, 10.17941952, 12.8642416 ,\n",
       "         7.86781597, 10.17941952, 12.8642416 , 11.17679501,  3.8466444 ,\n",
       "        12.8642416 , 10.17941952, 12.8642416 ,  6.02731228,  9.36805248]),\n",
       " array([    1,     1,     1,     1,     1,    20,     1,    20,    20,\n",
       "            1,  6006,    20,     1, 12680,  7344,     1,    20,     1,\n",
       "         4583,  1992]),\n",
       " array([ 0,  1,  2,  3,  5,  6,  7, 10,  9,  8, 11, 15, 17, 13,  4, 12, 19,\n",
       "        16,  4,  4]),\n",
       " array([1.        , 0.9999933 , 0.99999833, 0.9999968 , 0.9999831 ,\n",
       "        0.9907782 , 0.9999981 , 0.86585575, 0.803639  , 1.        ,\n",
       "        0.5720617 , 0.8412858 , 0.9999925 , 0.613074  , 0.22260535,\n",
       "        1.        , 0.98199445, 1.        , 0.12272349, 0.088715  ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 954,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descending_sorted_matrix = np.argsort(lenght_normalized, axis=None)\n",
    "selected_indexes = descending_sorted_matrix[:width] \n",
    "restored_coordinates = np.unravel_index(selected_indexes, (width, one_hot_length))\n",
    "candidates_that_generated_best_words, best_k_words = restored_coordinates\n",
    "prior_prob = best_join_prob[best_k_words]\n",
    "best_word_probab = word_prob[candidates_that_generated_best_words, best_k_words]\n",
    "\n",
    "prior_prob, best_k_words, candidates_that_generated_best_words, best_word_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:49:03.803646Z",
     "start_time": "2019-02-10T15:49:03.792458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " > go home . _ _ _\n",
       " [0.606] 1.0 0.407 0.921 0.713 1.0 1.0 1.0, \n",
       " > go home with us . _\n",
       " [0.344] 1.0 0.407 0.921 0.145 0.439 0.999 1.0, \n",
       " > leave house with them . _\n",
       " [0.321] 1.0 0.102 0.343 0.856 0.646 0.962 1.0, \n",
       " > leave home with them . _\n",
       " [0.304] 1.0 0.102 0.342 0.931 0.476 0.996 1.0, \n",
       " > go home with them . _\n",
       " [0.267] 1.0 0.407 0.921 0.145 0.182 0.999 1.0, \n",
       " > get home with the time .\n",
       " [0.298] 1.0 0.102 0.613 0.778 0.794 0.232 0.991, \n",
       " > leave home with us . _\n",
       " [0.254] 1.0 0.102 0.342 0.931 0.25 0.999 1.0, \n",
       " > turn up with the house .\n",
       " [0.259] 1.0 0.197 0.051 0.992 0.866 0.688 0.866, \n",
       " > get home with the one .\n",
       " [0.255] 1.0 0.102 0.613 0.778 0.794 0.154 0.804, \n",
       " > turn at home . _ _\n",
       " [0.204] 1.0 0.197 0.075 0.652 0.775 1.0 1.0, \n",
       " > go home , and what he\n",
       " [0.231] 1.0 0.407 0.921 0.083 0.381 0.487 0.572, \n",
       " > turn out with the day .\n",
       " [0.226] 1.0 0.197 0.09 0.883 0.566 0.404 0.841, \n",
       " > get home with the . _\n",
       " [0.19] 1.0 0.102 0.613 0.778 0.794 0.076 1.0, \n",
       " > go home , he gave them\n",
       " [0.219] 1.0 0.407 0.921 0.083 0.374 0.373 0.613, \n",
       " > turn in the house , let\n",
       " [0.218] 1.0 0.197 0.14 0.687 0.861 0.72 0.223, \n",
       " > get home . _ _ _\n",
       " [0.182] 1.0 0.102 0.613 0.179 1.0 1.0 1.0, \n",
       " > get home with the work .\n",
       " [0.211] 1.0 0.102 0.613 0.778 0.794 0.06 0.982, \n",
       " > come home . _ _ _\n",
       " [0.158] 1.0 0.033 0.379 0.614 0.999 1.0 1.0, \n",
       " > turn in the house , everyone\n",
       " [0.187] 1.0 0.197 0.14 0.687 0.861 0.72 0.123, \n",
       " > turn in the house , but\n",
       " [0.172] 1.0 0.197 0.14 0.687 0.861 0.72 0.089]"
      ]
     },
     "execution_count": 955,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "new_candidates = []\n",
    "for i in range(width):\n",
    "    word_id = best_k_words[i]\n",
    "    word_probability = best_word_probab[i]\n",
    "    candidate_parent = candidates_that_generated_best_words[i]\n",
    "    hidden = hiddens[candidate_parent]\n",
    "    const = consts[candidate_parent]\n",
    "    parent = candidates[candidate_parent]\n",
    "    new_candidates.append(deepcopy(parent).add_state(word_id=word_id, prob=word_probability, hidden=hidden, const=const))\n",
    "new_candidates    \n",
    "# candidates = [deepcopy(candidates[candidate_id]).add_state(word_id=best_k_words[i], prob=best_word_probab[i], hidden=hidden[i], const=fr_const) for i, candidate_id in enumerate(candidates_that_generated_best_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:48:57.361970Z",
     "start_time": "2019-02-10T15:48:57.359154Z"
    }
   },
   "outputs": [],
   "source": [
    "candidates = new_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to **Loop** and repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T20:20:58.179649Z",
     "start_time": "2019-02-09T20:20:58.174056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.843945205211639,\n",
       " 6.230512922913476,\n",
       " 6.656705468893051,\n",
       " 6.964711159467697,\n",
       " 7.158780634403229,\n",
       " 7.212840741303808,\n",
       " 7.299521863460541,\n",
       " 7.367884606122971,\n",
       " 7.726608529686928,\n",
       " 7.739929771050811,\n",
       " 8.230677098035812,\n",
       " 8.269179880619049,\n",
       " 8.40541797876358,\n",
       " 8.407620310783386,\n",
       " 8.447463572025299,\n",
       " 8.485476091504097,\n",
       " 8.512250483036041,\n",
       " 8.549754679203033,\n",
       " 8.558659171685576,\n",
       " 8.56688747368753]"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[candidate.p for candidate in candidates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search optimisations:\n",
    "* Length normalisation:\n",
    "    * arg max prod probs => arg max sum log probs\n",
    "    * since log is monotonic => maximizing log(probs) => maximizing probs\n",
    "    \n",
    "* Normalisation\n",
    "    * Since we multiply probablities (0, 1) => the product is greater for smaller number of terms\n",
    "    * the more terms => the more probs => the smallest the ending probabs\n",
    "    * solution: divide by Ty (number of words in the prediction)\n",
    "    * even better: normalize by 1 / pow(Ty, alpha) where alpha = 0.7 usually. alpha is a hyperparamerter.\n",
    "    \n",
    "* Negative logs\n",
    "    * since numpy usually sorts ascending, but we want to maximize something\n",
    "    * we will sum over -log(prob) and sort ascending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam width selection:\n",
    "    * 10 (ok for production)\n",
    "    * 100 somewhat large\n",
    "    * 1000, 3000 (only for SoTA papers) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search error analisys:\n",
    "* y = target sentence\n",
    "* y' = predicted sentence\n",
    "* compute T = P(y|x) and T' = P(y'|x)\n",
    "    * if T > T' => the target probability is higher than the prediction probability => had the beam search algo had more space, then it would have found out a better solution => This means that we can increase the beam search width to have more candidates generated.\n",
    "    * if T < T' => the target probability is lower than the predictions => regardless if beam search reached that target prediction or not in the search if wouldn't have outputed it => there is a problem with the RNN\n",
    "    \n",
    "Make a table to check for some sample of the translations, how many of the errors are due to the beam search or the RNN and decide what piece needs debuging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:02:21.168734Z",
     "start_time": "2019-02-08T18:02:21.143301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dstack(np.unravel_index(np.argsort(one_hot_words, axis=None)[::-1], (width, one_hot_length)))[0, :width, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:02:33.689168Z",
     "start_time": "2019-02-08T18:02:33.672007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 42607,      1,  14203,  28405,  71011,  99415,  56809,  85213,\n",
       "        113617, 141977]), [3, 0, 1, 2, 5, 7, 4, 6, 8, 9])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_length = one_hot_words.shape[-1]\n",
    "top_selections = np.argsort(one_hot_words, axis=None)[::-1][:width]\n",
    "top_candidates = [selection//one_hot_length for selection in top_selections]\n",
    "\n",
    "top_selections, top_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:26:40.362973Z",
     "start_time": "2019-02-08T18:26:40.359400Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_not_finished(generated_english_word_ids):\n",
    "    \"\"\"\n",
    "    Checks if we have a stopping event encountered.\n",
    "    Resons to stop generating words:\n",
    "    * STOP_WORD encountered\n",
    "    * max_lenght words generated\n",
    "    \"\"\"\n",
    "    id_last_word_predicted = generated_english_word_ids[-1]\n",
    "    \n",
    "    is_stop_word_hit = id_last_word_predicted == en2id[STOP_WORD]\n",
    "    is_max_length_hit = len(generated_english_word_ids) == max_eng_size\n",
    "    \n",
    "    return not is_stop_word_hit and not is_max_length_hit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T20:53:00.697858Z",
     "start_time": "2019-02-10T20:53:00.026211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> why do i have to go to school ?\n",
      "---------------------------\n",
      "[0] French: \tpourquoi me faut-il aller à l'école ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "[0] Indexed: \t[19664 16074 10028   986 27863 14431   234     1     1     1]\n",
      "\n",
      "(\n",
      "> why do i have to go to school ? _\n",
      "[0.905] 1.0 0.998 0.794 0.998 0.89 1.0 0.888 0.996 0.975 0.998 1.0, \n",
      "> why do i want to go to school ? _\n",
      "[0.532] 1.0 0.998 0.794 0.998 0.056 1.0 0.99 0.997 0.973 0.992 1.0, \n",
      "> why does i have to go to school ? _\n",
      "[0.487] 1.0 0.998 0.052 0.726 0.756 0.998 0.968 0.999 0.992 0.997 1.0, \n",
      "> why do i have to come to school ? _\n",
      "[0.456] 1.0 0.998 0.794 0.998 0.89 1.0 0.044 0.657 0.974 0.986 1.0, \n",
      "> why do i need to go to school ? _\n",
      "[0.455] 1.0 0.998 0.794 0.998 0.027 0.999 0.958 0.997 0.97 0.99 1.0, \n",
      "> why do i have to go to bed ? _\n",
      "[0.385] 1.0 0.998 0.794 0.998 0.89 1.0 0.888 0.996 0.013 0.999 1.0, \n",
      "> why do i have to school ? _\n",
      "[0.38] 1.0 0.998 0.794 0.998 0.89 1.0 0.041 0.552 1.0, \n",
      "> why about i have to go to school ? _\n",
      "[0.359] 1.0 0.998 0.027 0.655 0.457 0.898 0.968 0.998 0.85 0.998 1.0, \n",
      "> why does he want to go to school ? _\n",
      "[0.335] 1.0 0.998 0.052 0.123 0.751 0.999 0.989 0.998 0.901 0.979 1.0, \n",
      "> why does i want to go to school ? _\n",
      "[0.327] 1.0 0.998 0.052 0.726 0.101 1.0 0.995 1.0 0.974 0.993 1.0, \n",
      "> why do i have to leave school ? _\n",
      "[0.319] 1.0 0.998 0.794 0.998 0.89 1.0 0.012 0.585 0.989 1.0, \n",
      "> why do i have to school to go ? _\n",
      "[0.312] 1.0 0.998 0.794 0.998 0.89 1.0 0.041 0.11 0.938 0.995 0.997, \n",
      "> why do i go to school ? _\n",
      "[0.304] 1.0 0.998 0.794 0.998 0.009 0.999 0.985 0.885 1.0, \n",
      "> why about i go to school ? _\n",
      "[0.297] 1.0 0.998 0.027 0.655 0.325 0.999 0.967 0.998 1.0, \n",
      "> why did i go to school ? _\n",
      "[0.295] 1.0 0.998 0.015 0.986 0.385 0.984 0.985 0.978 1.0, \n",
      "> why are i going to school ? _\n",
      "[0.295] 1.0 0.998 0.035 0.952 0.232 0.998 0.958 0.714 1.0, \n",
      "> why are i like to go to school ? _\n",
      "[0.292] 1.0 0.998 0.035 0.952 0.078 0.985 0.906 0.924 0.979 0.997 1.0, \n",
      "> why do i have to take school ? _\n",
      "[0.292] 1.0 0.998 0.794 0.998 0.89 1.0 0.006 0.783 0.995 1.0, \n",
      "> why do i have to school now ? _\n",
      "[0.288] 1.0 0.998 0.794 0.998 0.89 1.0 0.041 0.106 1.0 1.0, \n",
      "> why do i take to go to school ? _\n",
      "[0.285] 1.0 0.998 0.794 0.998 0.006 0.467 0.975 0.993 0.914 0.999 1.0, \n",
      "> why about me have to go to school ? _\n",
      "[0.277] 1.0 0.998 0.027 0.268 0.299 0.997 0.778 0.987 0.972 0.999 1.0, \n",
      "> why would i like to go to school ? _\n",
      "[0.277] 1.0 0.998 0.015 0.953 0.131 0.999 0.982 0.99 0.892 0.999 1.0, \n",
      "> why do i have to come early in ? _\n",
      "[0.27] 1.0 0.998 0.794 0.998 0.89 1.0 0.044 0.218 0.714 0.293 1.0, \n",
      "> why does she want to go to school ? _\n",
      "[0.27] 1.0 0.998 0.052 0.074 0.493 0.999 0.987 0.996 0.778 0.981 1.0, \n",
      "> why are i in school ? _\n",
      "[0.268] 1.0 0.998 0.035 0.952 0.384 0.481 0.95 1.0, \n",
      "> why i want to go to school ? _\n",
      "[0.263] 1.0 0.998 0.016 0.131 1.0 0.988 0.997 0.997 0.991 1.0, \n",
      "> why would i make go to school ? _\n",
      "[0.26] 1.0 0.998 0.015 0.953 0.459 0.317 0.964 0.97 0.997 1.0, \n",
      "> why did i want to go to school ? _\n",
      "[0.258] 1.0 0.998 0.015 0.986 0.083 1.0 0.992 0.999 0.958 0.986 1.0, \n",
      "> why would i want to go to school ? _\n",
      "[0.257] 1.0 0.998 0.015 0.953 0.083 1.0 0.989 0.998 0.967 0.996 1.0, \n",
      "> why about i do go to school ? _\n",
      "[0.254] 1.0 0.998 0.027 0.655 0.112 0.91 0.98 0.966 0.997 1.0, \n",
      "> why do i have to go to my bed ? _\n",
      "[0.247] 1.0 0.998 0.794 0.998 0.89 1.0 0.888 0.996 0.005 0.199 1.0 1.0, \n",
      "> why shall i go to school ? _\n",
      "[0.246] 1.0 0.998 0.003 0.995 0.935 0.974 0.971 0.993 1.0, \n",
      "> why about me to go to school ? _\n",
      "[0.242] 1.0 0.998 0.027 0.268 0.208 0.977 0.939 0.992 0.998 1.0, \n",
      "> why do i have to school early ? _\n",
      "[0.235] 1.0 0.998 0.794 0.998 0.89 1.0 0.041 0.041 1.0 1.0, \n",
      "> why should i go to school ? _\n",
      "[0.221] 1.0 0.998 0.004 1.0 0.362 0.999 0.98 0.99 1.0, \n",
      "> why would i go to school ? _\n",
      "[0.213] 1.0 0.998 0.015 0.953 0.099 0.992 0.986 0.976 1.0, \n",
      "> why do i have to come early in the arrived ? _\n",
      "[0.21] 1.0 0.998 0.794 0.998 0.89 1.0 0.044 0.218 0.714 0.395 0.073 0.999 1.0, \n",
      "> why can i go to school ? _\n",
      "[0.209] 1.0 0.998 0.009 1.0 0.145 0.968 0.983 0.977 1.0, \n",
      "> why does i go to school ? _\n",
      "[0.201] 1.0 0.998 0.052 0.726 0.038 0.998 0.956 0.771 1.0, \n",
      "> why i am in school ? _\n",
      "[0.171] 1.0 0.998 0.016 0.329 0.67 0.328 0.905 1.0)\n"
     ]
    }
   ],
   "source": [
    "def beam_search(fre_tokens: List[str], width:int = 3, alpha = 0.7, debug=True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Translates the given sentence from French to English using the beam search approach.\n",
    "    \n",
    "    The input of this function is a list of french word tokens.\n",
    "    \n",
    "    The width paramter tells how many candidates to search at a time (the beam width).\n",
    "    The alpha paramter tells how hard to penalize short sentences (the bigger the alpha the less likely we will get shorter sentences).\n",
    "    \"\"\"\n",
    "    # Scan the full french phrase and return the hidden state of the LSTM\n",
    "    # This will be used to initialize the first step of the LSTM english decoder\n",
    "    # Note: Because we've trained with fixed length training data, we need \n",
    "    # to also supply a fixed length french input. If we pass a variable length one\n",
    "    # the hidden state after running the french sentence, will not be that accurate!\n",
    "    french_phrase = fr_input(fre_tokens)[0]\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"---------------------------\")\n",
    "        print(f\"[0] French: \\t{to_str(fr_decode(french_phrase))[:80]}\")\n",
    "        print(f\"[0] Indexed: \\t{french_phrase[:10]}\\n\")\n",
    "    \n",
    "    # Start with a single START_WORD\n",
    "    eng_inp = en_indexed([START_WORD])\n",
    "\n",
    "    # Scan the french phrase and get the hidden state\n",
    "    [fr_hidden], [fr_const] = encoder.predict([[french_phrase]])\n",
    "    \n",
    "    # start the beam search with a single candidates\n",
    "    # all start with \"START_WORD\"\n",
    "    candidates = [Candidate().add_state(word_id=en2id[START_WORD], prob=1, hidden=fr_hidden, const=fr_const)]\n",
    "    stalled_candidates = []\n",
    "    \n",
    "    stop_word_id = en2id[STOP_WORD]\n",
    "    \n",
    "    while candidates:\n",
    "        # Get previous states\n",
    "        word = [[candidate.words[-1]] for candidate in candidates]\n",
    "        hidden = [candidate.hidden for candidate in candidates]\n",
    "        const = [candidate.const for candidate in candidates]\n",
    "        prior_prob = np.array([[candidate.p] for candidate in candidates]) \n",
    "        \n",
    "        \n",
    "        # Predict next word for each candidate\n",
    "        [one_hot_words, hiddens, consts] = decoder.predict([word, hidden, const])\n",
    "        one_hot_length = one_hot_words.shape[-1]\n",
    "\n",
    "        # Compute the new probabilities  \n",
    "        word_prob = np.squeeze(one_hot_words, axis=1)   # reduce middle 1 dimension axis\n",
    "        word_log_prob = -np.log(word_prob)  # reason on the logs since are more numericaly stable\n",
    "        joint_prob = word_log_prob + prior_prob  # update the priors with the current word predictions\n",
    "    \n",
    "        # Compute the length normalisation trick on the joint probabilities\n",
    "        candidate_lengths = np.array([[len(strip_indexed(candidate.words))] for candidate in candidates])\n",
    "        lenght_normalized = joint_prob / np.power(candidate_lengths, alpha)\n",
    "    \n",
    "        # Get best candidates\n",
    "        descending_sorted_matrix = np.argsort(lenght_normalized, axis=None) # reason on (-) so we can use ascending order for getting the bigest elements\n",
    "        selected_indexes = descending_sorted_matrix[:width] \n",
    "        restored_coordinates = np.unravel_index(selected_indexes, (width, one_hot_length))\n",
    "        candidates_that_generated_best_words, best_k_words = restored_coordinates\n",
    "        prior_prob = best_join_prob[best_k_words]\n",
    "        best_word_probab = word_prob[candidates_that_generated_best_words, best_k_words]\n",
    "\n",
    "        # Update the candidate list\n",
    "        new_candidates = []\n",
    "        for i in range(width):\n",
    "            word_id = best_k_words[i]\n",
    "            word_probability = best_word_probab[i]\n",
    "            candidate_parent = candidates_that_generated_best_words[i]\n",
    "            hidden = hiddens[candidate_parent]\n",
    "            const = consts[candidate_parent]\n",
    "            parent = candidates[candidate_parent]\n",
    "            \n",
    "            new_candidate = deepcopy(parent).add_state(word_id=word_id, prob=word_probability, hidden=hidden, const=const)\n",
    "\n",
    "            if word_id == stop_word_id:\n",
    "                stalled_candidates.append(new_candidate)\n",
    "                width -=1\n",
    "            else:\n",
    "                new_candidates.append(new_candidate)\n",
    "        \n",
    "        candidates = new_candidates\n",
    "\n",
    "    sorted_candidates = sorted([(candidate.score(), candidate) for candidate in stalled_candidates], reverse=True)\n",
    "    sorted_candidates = list(zip(*sorted_candidates))[1]\n",
    "    return sorted_candidates\n",
    "        \n",
    "# print(beam_search([\"Allez\", \"a\", \"la\", \"maison\", \"avec\", \"nous\", \".\"]))\n",
    "# print(beam_search([\"Va\", \"!\"]))\n",
    "\n",
    "sample = 35\n",
    "print(to_str(en_decode(strip_indexed(decode_input_train[sample]))))\n",
    "print(beam_search(fr_decode(strip_indexed(encode_input_train[sample])), alpha=0.7, width=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('> une pomme tomba au sol . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _',\n",
       " 'an apple fell to the ground . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _')"
      ]
     },
     "execution_count": 902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_str(fr_decode(encode_input_test[0])), to_str(en_decode(decode_output_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12and\n",
    "122\n",
    "334\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 62)"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_input(fr_decode(encode_input_test[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T15:27:55.545361Z",
     "start_time": "2019-02-10T15:27:55.534932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['la',\n",
       " 'maison',\n",
       " 'de',\n",
       " 'tom',\n",
       " \"n'est\",\n",
       " 'pas',\n",
       " 'loin',\n",
       " 'de',\n",
       " 'celle',\n",
       " 'de',\n",
       " 'mary',\n",
       " '.']"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_indexed(tokens):\n",
    "    \"\"\"\n",
    "    Function that strips all the decorations from a indexed training sequence (containing ids).\n",
    "    \n",
    "    It removes the following:\n",
    "    * the initial START_WORD\n",
    "    * the ending STOP_WORD\n",
    "    * all the padding words added with fixed_length variant functions\n",
    "\n",
    "    \"\"\"\n",
    "    if type(tokens) != list:\n",
    "        tokens = tokens.tolist()\n",
    "        \n",
    "    # exclude the initial START_WORD, the STOP_WORD and everything after it (padding words)\n",
    "    stop_word_id = en2id[STOP_WORD]\n",
    "    if stop_word_id in tokens:\n",
    "        return tokens[0:tokens.index(en2id[STOP_WORD])]\n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "def strip_plaintext(tokens):\n",
    "    \"\"\"\n",
    "    Function that strips all the decorations from a plaintext training sequence (containing actual words).\n",
    "    \n",
    "    It removes the following:\n",
    "    * the initial START_WORD\n",
    "    * the ending STOP_WORD\n",
    "    * all the padding words added with fixed_length variant functions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # exclude the initial START_WORD, the STOP_WORD and everything after it (padding words)\n",
    "    if STOP_WORD in tokens:\n",
    "        return tokens[0:tokens.index(STOP_WORD)]\n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "def strip(tokens):\n",
    "    \"\"\"\n",
    "    Function that strips all the decorations from a training sequence.\n",
    "    It removes the following:\n",
    "    * the initial START_WORD\n",
    "    * the ending STOP_WORD\n",
    "    * all the padding words added with fixed_length variant functions\n",
    "    \n",
    "    This function can be applied to both indexed or plaintext sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __are_plaintext(tokens):\n",
    "        # Heuristic to test if the tokens are in plaintext or not.\n",
    "        # Assumed plaintext if known plaintext words (almost always guaranteed to be present) \n",
    "        # are included among the tokens.\n",
    "        # \n",
    "        # These signal words are:\n",
    "        # START_WORD, STOP_WORD\n",
    "        return STOP_WORD in tokens or START_WORD in tokens \n",
    "        \n",
    "    if __are_plaintext(tokens):\n",
    "        return strip_plaintext(tokens)\n",
    "    else:\n",
    "        return strip_indexed(tokens)\n",
    "\n",
    "strip(fr_decode(encode_input_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T12:11:13.458780Z",
     "start_time": "2019-02-10T12:11:13.441569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14202,) (201,) (201,)\n",
      "()\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "elles aiment tom .\n",
      "> they love tom .\n",
      "> please keep quiet .\n"
     ]
    }
   ],
   "source": [
    "sample = 22\n",
    "fr_sentence = encode_input_train[sample]\n",
    "en_sentence = decode_input_train[sample]\n",
    "\n",
    "french_sent = to_str(fr_decode(strip_indexed(fr_sentence)))\n",
    "target_sent = to_str(en_decode(strip_indexed(en_sentence)))\n",
    "english_sent = to_str(strip_plaintext(translate(fr_decode(strip_indexed(fr_sentence)), debug=False)))\n",
    "\n",
    "print(f\"{french_sent}\\n{target_sent}\\n{english_sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with sparse_categorical_crossentropy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_69 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_70 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FreEmbeddings (Embedding)       (None, None, 10)     286490      input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "EngEmbeddings (Embedding)       (None, None, 10)     142050      input_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "FreLSTM (LSTM)                  [(None, 10), (None,  840         FreEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EngLSTM (LSTM)                  (None, None, 10)     840         EngEmbeddings[0][0]              \n",
      "                                                                 FreLSTM[0][1]                    \n",
      "                                                                 FreLSTM[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, None, 1)      11          EngLSTM[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 430,231\n",
      "Trainable params: 430,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fre = Input(shape=(None,))\n",
    "eng = Input(shape=(None,))\n",
    "\n",
    "fre_emb = Embedding(input_dim=len(id2fr), output_dim=10, name=\"FreEmbeddings\")\n",
    "eng_emb = Embedding(input_dim=len(id2en), output_dim=10, name=\"EngEmbeddings\")\n",
    "\n",
    "fre_lstm = LSTM(10, name=\"FreLSTM\", return_state=True)\n",
    "eng_lstm = LSTM(10, name=\"EngLSTM\", return_sequences=True)\n",
    "\n",
    "\n",
    "# We recieve three things here because we've set return_state=True.\n",
    "# We also get the internal states of the RNN (hidden, constants)\n",
    "# \n",
    "# hidden = <tf.Tensor 'FreLSTM_4/while/Exit_3:0' shape=(?, 10) dtype=float32>,\n",
    "# constants = <tf.Tensor 'FreLSTM_4/while/Exit_4:0' shape=(?, 10) dtype=float32>\n",
    "single_fre_out, fre_hidden_state, fre_constants = fre_lstm(fre_emb(fre))\n",
    "\n",
    "# At this stage, the single_fre_out is irrelevant to us. What we want to get \n",
    "# is only the internal state of the LSTM\n",
    "_ = single_fre_out\n",
    "\n",
    "# Initialize the English LSTM with the hidden state that we got by passing through \n",
    "# all the words in the French sentence. We basically start from the point that LSTM stopped and we wish to \n",
    "# \"go back in time\" if we can, but reversing in a different language (English).\n",
    "eng_out = eng_lstm(eng_emb(eng), initial_state=[fre_hidden_state, fre_constants])\n",
    "\n",
    "# I'm still unsure what's the difference bewteen applying TimeDistributed vs. Dense layer directly.\n",
    "# Both seem to act the same\n",
    "output = Dense(1, activation=\"relu\")(eng_out)\n",
    "\n",
    "model = Model(inputs=[fre, eng], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Received a label value of 14169 which is outside the valid range of [0, 1).  Label values: 6902 12872 288 6208 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12 8656 8602 12677 12724 9116 13911 13717 12843 3966 12675 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 5994 27 6009 8602 2224 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 16 14050 6548 1731 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12 13944 12843 6080 14162 6442 14162 13717 7811 12843 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13911 11 1356 12677 5030 12843 3744 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13881 6902 12677 10711 4161 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 2017 8262 3966 12729 8254 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13942 14162 11316 13437 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3743 14162 13760 13145 14151 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12867 6902 735 6726 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12867 7096 7766 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6314 2085 14162 1356 11626 12287 8602 12675 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5994 288 8395 12813 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13911 7623 12729 11691 9468 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6968 13856 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 7033 2017 8262 5524 6914 10583 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 14162 12475 10107 4861 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 7409 14162 980 288 5338 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13321 17 12675 13748 8262 12677 2183 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13924 3981 6914 7662 14162 766 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13108 12843 3022 14169 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 8653 288 1216 5832 6902 5240 17 6914 6902 5945 12843 5524 10572 8602 6914 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12867 5977 12843 5166 8763 10729 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12287 6249 6008 9011 12677 12645 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 14162 917 8488 1992 288 12133 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6008 5977 288 5774 557 14162 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 13717 12843 7662 288 11953 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 11193 522 6159 12843 4862 6177 10995 1458 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12 316 12677 10812 558 980 14162 917 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5354 8506 8652 17 6407 11 13108 12843 6080 14162 13998 12677 14046 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 14026 6442 12867 7379 12843 13467 316 12675 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\t [[{{node loss_16/dense_21_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}} = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _class=[\"loc:@train...s_grad/mul\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_16/dense_21_loss/Reshape_1, loss_16/dense_21_loss/Cast)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-607-2dc421312db6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencode_input_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_input_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_output_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Received a label value of 14169 which is outside the valid range of [0, 1).  Label values: 6902 12872 288 6208 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12 8656 8602 12677 12724 9116 13911 13717 12843 3966 12675 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 5994 27 6009 8602 2224 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 16 14050 6548 1731 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12 13944 12843 6080 14162 6442 14162 13717 7811 12843 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13911 11 1356 12677 5030 12843 3744 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13881 6902 12677 10711 4161 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 2017 8262 3966 12729 8254 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13942 14162 11316 13437 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3743 14162 13760 13145 14151 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12867 6902 735 6726 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12867 7096 7766 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6314 2085 14162 1356 11626 12287 8602 12675 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5994 288 8395 12813 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13911 7623 12729 11691 9468 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6968 13856 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 7033 2017 8262 5524 6914 10583 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 14162 12475 10107 4861 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 7409 14162 980 288 5338 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13321 17 12675 13748 8262 12677 2183 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13924 3981 6914 7662 14162 766 286 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 13108 12843 3022 14169 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 8653 288 1216 5832 6902 5240 17 6914 6902 5945 12843 5524 10572 8602 6914 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12867 5977 12843 5166 8763 10729 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12287 6249 6008 9011 12677 12645 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 14162 917 8488 1992 288 12133 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6008 5977 288 5774 557 14162 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 13717 12843 7662 288 11953 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 11193 522 6159 12843 4862 6177 10995 1458 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 12 316 12677 10812 558 980 14162 917 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5354 8506 8652 17 6407 11 13108 12843 6080 14162 13998 12677 14046 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6407 14026 6442 12867 7379 12843 13467 316 12675 20 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\t [[{{node loss_16/dense_21_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}} = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _class=[\"loc:@train...s_grad/mul\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_16/dense_21_loss/Reshape_1, loss_16/dense_21_loss/Cast)]]"
     ]
    }
   ],
   "source": [
    "model.fit(x=[encode_input_train[:100], decode_input_train[:100]], y=np.expand_dims(decode_output_train[:100], -1), validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with PADD_WORD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id2Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc305ca1839472a9fc2b09624155acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160872), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d064ce5542664e3981ed378d6e625eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160872), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(14203, 28647)"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm import trange\n",
    "\n",
    "def flatten(series: list) -> list:\n",
    "    \"\"\"\n",
    "    Flattens the given list and returns a list of (unique) tokens\n",
    "    \"\"\"\n",
    "    return list(reduce(lambda collect, value: collect | set(value), series, set()))\n",
    "\n",
    "def unique(vocabulary: list) -> list:\n",
    "    return list(set(vocabulary))\n",
    "\n",
    "def lower(vocabulary: list) -> list:\n",
    "    return list(map(str.lower, vocabulary))\n",
    "\n",
    "id2en = sorted(unique(lower(flatten(tqdm(raw.English_tokens)))))\n",
    "id2fr = sorted(unique(lower(flatten(tqdm(raw.French_tokens)))))\n",
    "len(id2en), len(id2fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_WORD = \"<START>\"\n",
    "STOP_WORD = \"<STOP>\"\n",
    "PADD_WORD = \"_\"\n",
    "\n",
    "SPECIAL_WORDS = [START_WORD, STOP_WORD, PADD_WORD]\n",
    "\n",
    "assert set(SPECIAL_WORDS) & set(id2en) == set()\n",
    "assert set(SPECIAL_WORDS) & set(id2fr) == set()\n",
    "\n",
    "id2en = SPECIAL_WORDS + id2en\n",
    "id2fr = SPECIAL_WORDS + id2fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French chars:  !, $, %, &, ', (, ), +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, ?, `, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, «, », à, á, â, ç, è, é, ê, ë, î, ï, ô, ö, ù, û, œ, с, ​, –, ‘, ’, …, ‽, ₂\n",
      "English chars:  !, $, %, &, ', +, ,, -, ., /, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, :, ;, ?, `, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, ­, ç, é, ö, ú, а, –, —, ‘, ’, ₂, €\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77, 62)"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_charset = sorted(unique(flatten(id2fr[len(SPECIAL_WORDS):])))\n",
    "en_charset = sorted(unique(flatten(id2en[len(SPECIAL_WORDS):])))\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"French chars: \", \", \".join(fr_charset))\n",
    "print(\"English chars: \", \", \".join(en_charset))\n",
    "len(fr_charset), len(en_charset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02124f513756482480ba360d991b1e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14206), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92674ea9f1794e95ad640a53cd057c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28650), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def word2id(elements: list) -> Dict[str, int]:\n",
    "    return {word: _id for _id, word in enumerate(elements)}\n",
    "    \n",
    "en2id = word2id(tqdm(id2en))\n",
    "fr2id = word2id(tqdm(id2fr))\n",
    "\n",
    "# Test that for the first 100 elements the two datastructures are conssitent\n",
    "for i in range(100):\n",
    "    assert fr2id[id2fr[i]] == i\n",
    "    assert en2id[id2en[i]] == i\n",
    "    \n",
    "# Test that we have the special words in front\n",
    "assert fr2id[START_WORD] == 0\n",
    "assert en2id[STOP_WORD] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 27039, 3, 1], [0, 27039, 3, 1])"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def indexed(tokens: List[str], vocabulary: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Returns a vectorized list for the given token list. \n",
    "    The vectorisation is done using the given vocabulary mapping. \n",
    "    \"\"\"\n",
    "    return [vocabulary[word] for word in tokens]\n",
    "\n",
    "from functools import partial\n",
    "fr_indexed = partial(indexed, vocabulary=fr2id)\n",
    "en_indexed = partial(indexed, vocabulary=en2id)\n",
    "\n",
    "indexed(phrase(['Va', '!']), vocabulary=fr2id), fr_indexed(phrase(['Va', '!']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START>', 'va', '!', '<STOP>']"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phrase(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Padds the token list with START and STOP words\n",
    "    \"\"\"\n",
    "    return [START_WORD] + lower(tokens) + [STOP_WORD]\n",
    "\n",
    "# Each phrase increases with 2 more elements so the maxiums change as well\n",
    "max_eng_size = raw.English_length.max() + 2\n",
    "max_fre_size = raw.French_length.max() + 2\n",
    "\n",
    "phrase(['Va', '!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def padd_right(tokens: list, fixed_size=3, padd=PADD_WORD)-> list:\n",
    "    \"\"\"\n",
    "    Adds enought padding characters to the right of the given list to as to make it a fixed size == fixed_size\n",
    "    \"\"\"\n",
    "    l = len(tokens)\n",
    "    to_padd = max(0, fixed_size - l)\n",
    "    return tokens + ([padd] * to_padd)\n",
    "\n",
    "padded = padd_right([1, 2, 3], fixed_size=10, padd=0)\n",
    "\n",
    "fixed_length_fr = partial(padd_right, fixed_size=max_fre_size, padd=PADD_WORD)\n",
    "fixed_length_en = partial(padd_right, fixed_size=max_eng_size, padd=PADD_WORD)\n",
    "\n",
    "assert len(padded) == 10\n",
    "assert padded[-1] == 0\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 60)"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_eng_size, max_fre_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bbe71aba7d40a596c9e70bc4ce24dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160872), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b2df1bb6874430ad97eb7263738620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160872), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((160872, 60), (160872, 53), (160872, 53))"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_length_fre_generator = map(lambda tokens: fr_indexed(fixed_length_fr(phrase(tokens))), tqdm(raw.French_tokens))\n",
    "fixed_length_eng_generator = map(lambda tokens: en_indexed(fixed_length_en(phrase(tokens))), tqdm(raw.English_tokens))\n",
    "\n",
    "encode_input = np.array(list(fixed_length_fre_generator), dtype=np.int32)\n",
    "decode_input = np.array(list(fixed_length_eng_generator), dtype=np.int32)\n",
    "\n",
    "decode_output = np.full_like(decode_input, fill_value=en2id[PADD_WORD])\n",
    "decode_output[:,:-1] = decode_input[:, 1:]\n",
    "\n",
    "# Test that the decode_output is one timestep ahead\n",
    "assert np.alltrue(decode_output[:, :max_eng_size-1] == decode_input[:, 1:max_eng_size])\n",
    "\n",
    "encode_input.shape, decode_input.shape, decode_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128697, 60), (128697, 53), (128697, 53))"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "encode_input_train, encode_input_test, decode_input_train, decode_input_test, decode_output_train, decode_output_test = train_test_split(encode_input, decode_input, decode_output, test_size=0.2, shuffle=True)\n",
    "\n",
    "encode_input_train.shape, decode_input_train.shape, decode_output_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> va ! <STOP> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _'"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(indexed_tokens: List[int], vocabulary: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Decodes a list of token ids to into a list of words, using the given vocabulary\n",
    "    \"\"\"\n",
    "    return [vocabulary[_id] for _id in indexed_tokens]\n",
    "\n",
    "en_decode = partial(decode, vocabulary=id2en)\n",
    "fr_decode = partial(decode, vocabulary=id2fr)\n",
    "\n",
    "to_str(fr_decode(encode_input[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "FreTokens (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngTokens (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FreEmbeds (Embedding)           (None, None, 20)     573000      FreTokens[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "EngEmbeds (Embedding)           (None, None, 20)     284120      EngTokens[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FreLSTM (LSTM)                  [(None, 50), (None,  14200       FreEmbeds[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "EngLSTM (LSTM)                  [(None, None, 50), ( 14200       EngEmbeds[0][0]                  \n",
      "                                                                 FreLSTM[0][1]                    \n",
      "                                                                 FreLSTM[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, None, 14206)  724506      EngLSTM[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,610,026\n",
      "Trainable params: 1,610,026\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Note: turns out that we can pass the shape shape=(None,) and this will make the model a variable length one!\n",
    "fre = Input(shape=(None,), name=\"FreTokens\")\n",
    "eng = Input(shape=(None,), name=\"EngTokens\")\n",
    "\n",
    "fre_emb = Embedding(input_dim=len(id2fr), output_dim=20, name=\"FreEmbeds\")\n",
    "eng_emb = Embedding(input_dim=len(id2en), output_dim=20, name=\"EngEmbeds\")\n",
    "\n",
    "fre_lstm = LSTM(50, name=\"FreLSTM\", return_state=True)\n",
    "\n",
    "# Although we don't need in the trainign session, the internal state of the decoder part\n",
    "# (so we could pass return_state=False) we want to reuse the same layer in the decoder model\n",
    "# used for inference, so we use retrurn_state=True after all \n",
    "eng_lstm = LSTM(50, name=\"EngLSTM\", return_state=True, return_sequences=True)\n",
    "\n",
    "\n",
    "# We recieve three things here because we've set return_state=True.\n",
    "# We also get the internal states of the RNN (hidden, constants)\n",
    "# \n",
    "# hidden = <tf.Tensor 'FreLSTM_4/while/Exit_3:0' shape=(?, 10) dtype=float32>,\n",
    "# constants = <tf.Tensor 'FreLSTM_4/while/Exit_4:0' shape=(?, 10) dtype=float32>\n",
    "single_fre_out, fre_hidden_state, fre_constants = fre_lstm(fre_emb(fre))\n",
    "\n",
    "# At this stage, the single_fre_out is irrelevant to us. What we want to get \n",
    "# is only the internal state of the LSTM\n",
    "_ = single_fre_out\n",
    "\n",
    "# Initialize the English LSTM with the hidden state that we got by passing through \n",
    "# all the words in the French sentence. We basically start from the point that LSTM stopped and we wish to \n",
    "# \"go back in time\" if we can, but reversing in a different language (English).\n",
    "all_eng_out, eng_hidden_state, eng_constants = eng_lstm(eng_emb(eng), initial_state=[fre_hidden_state, fre_constants])\n",
    "\n",
    "# I'm still unsure what's the difference bewteen applying TimeDistributed vs. Dense layer directly.\n",
    "# Both seem to act the same\n",
    "#\n",
    "# We will need to reuse the trained Dense layer so we can requre it to the inference inputs\n",
    "eng_decode = Dense(len(id2en), activation=\"softmax\")\n",
    "output = eng_decode(all_eng_out)\n",
    "\n",
    "model = Model(inputs=[fre, eng], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 60), (64, 53), (64, 53, 14206))"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_language_vocabulary_size = len(id2en)\n",
    "\n",
    "def data_generator(encode_input, decode_input, decode_output, batch_size=32, subsample=10000):\n",
    "    assert encode_input.shape[0] == decode_input.shape[0] == decode_output.shape[0]\n",
    "    length = encode_input.shape[0] if not subsample else subsample\n",
    "    while True:\n",
    "        for i in range(0, length, batch_size):\n",
    "            yield [encode_input[i:i+batch_size, :], decode_input[i:i+batch_size, :]], to_categorical(decode_output[i:i+batch_size, :], num_classes=english_language_vocabulary_size)\n",
    "            \n",
    "gen = data_generator(encode_input_train, decode_input_train, decode_output_train, batch_size=batch_size)\n",
    "[inp, tch], out = next(gen)\n",
    "inp.shape, tch.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 630s 4s/step - loss: 2.9847 - val_loss: 1.0330\n",
      "Epoch 2/100\n",
      "113/156 [====================>.........] - ETA: 2:46 - loss: 0.9137"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-755-53f05e9370fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_output_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubsample_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/deep3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subsample_train = 10000 # encode_input_train.shape[0]\n",
    "batch_size = 64\n",
    "subsample_test = 2000  # encode_input_test.shape[0]\n",
    "epochs = 100\n",
    "\n",
    "steps_per_epoch_train = subsample_train // batch_size\n",
    "steps_per_epoch_test = subsample_test // batch_size\n",
    "\n",
    "train_gen = data_generator(encode_input_train, decode_input_train, decode_output_train, batch_size=batch_size, subsample=subsample_train)\n",
    "test_gen = data_generator(encode_input_test, decode_input_test, decode_output_test, batch_size=batch_size, subsample=subsample_test)\n",
    "\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=steps_per_epoch_train, epochs=epochs, validation_data=test_gen, validation_steps=steps_per_epoch_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FreTokens (InputLayer)       (None, None)              0         \n",
      "_________________________________________________________________\n",
      "FreEmbeds (Embedding)        (None, None, 20)          573000    \n",
      "_________________________________________________________________\n",
      "FreLSTM (LSTM)               [(None, 50), (None, 50),  14200     \n",
      "=================================================================\n",
      "Total params: 587,200\n",
      "Trainable params: 587,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "EngTokens (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngEmbeds (Embedding)           (None, None, 20)     284120      EngTokens[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "EngHiddLSTM (InputLayer)        (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngConsLSTM (InputLayer)        (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngLSTM (LSTM)                  [(None, None, 50), ( 14200       EngEmbeds[1][0]                  \n",
      "                                                                 EngHiddLSTM[0][0]                \n",
      "                                                                 EngConsLSTM[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, None, 14206)  724506      EngLSTM[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,022,826\n",
      "Trainable params: 1,022,826\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get encoder states\n",
    "encoder = Model(inputs=fre, outputs=[fre_hidden_state, fre_constants])\n",
    "encoder.summary()\n",
    "\n",
    "# Make the initial states of the decoder, input symbols\n",
    "hidden_state = Input(shape=(50,), name=\"EngHiddLSTM\")\n",
    "const_state = Input(shape=(50,), name=\"EngConsLSTM\")\n",
    "\n",
    "# wire the input states to the trained LSTM\n",
    "# we are reusing the eng_emb trained layer to convert token ids into embeddings\n",
    "# (?) If we pass the initial_state to the LSTM do we still need to reuse the already trained one?\n",
    "# (?) i.e. does the LSTM have any other state only persisted in the trained LSTM instance?\n",
    "# (?) EDIT: There are some Wx , Wh matrices what are indeed trained and only stored internally\n",
    "all_eng_out, eng_hidden_state, eng_constants = eng_lstm(eng_emb(eng), initial_state=[hidden_state, const_state])\n",
    "\n",
    "# wire the all steps output to the trained eng_decode Dense layer\n",
    "one_hot_words = eng_decode(all_eng_out)\n",
    "\n",
    "decoder = Model(inputs=[eng, hidden_state, const_state], outputs=[one_hot_words, eng_hidden_state, eng_constants])\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "[0] French: \t<START> allez a la maison avec nous . <STOP> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "[0] Indexed: \t[    0   993   237 14566 15703  2177 17677    20     1     2]\n",
      "\n",
      "[1] Hidden:\t[-0.96 -0.93  1.    0.82  0.98 -0.88  0.73 -1.    1.   -0.75]\n",
      "[1] Const:\t[ -8.59 -30.41  26.13  36.02   3.52  -4.46   1.64 -31.39  15.6  -10.36]\n",
      "[1] Input: \t<START>\n",
      "[1] Top: \t[['i', 'you', '.']]\n",
      "[1] Pred: \t`<START>` ~~~: i\n",
      "\n",
      "[2] Hidden:\t[-0.92 -0.8   0.9   0.83  0.76 -0.99  0.8  -0.89  1.   -0.78]\n",
      "[2] Const:\t[ -8.44 -30.13  25.31  31.19   3.91  -2.85   1.1  -27.49  14.7  -10.87]\n",
      "[2] Input: \ti\n",
      "[2] Top: \t[['i', '.', 'you']]\n",
      "[2] Pred: \t`<START> i` ~~~: i\n",
      "\n",
      "[3] Hidden:\t[-0.8  -0.67  0.76  0.7   0.61 -0.97  0.54 -0.76  0.87 -0.62]\n",
      "[3] Const:\t[ -8.14 -29.84  23.35  26.71   4.17  -2.06   0.63 -22.14  12.95 -10.97]\n",
      "[3] Input: \ti\n",
      "[3] Top: \t[['.', 'i', '<STOP>']]\n",
      "[3] Pred: \t`<START> i i` ~~~: .\n",
      "\n",
      "[4] Hidden:\t[-0.73 -0.61  0.7   0.67  0.56 -0.82  0.19 -0.71  0.79 -0.58]\n",
      "[4] Const:\t[ -7.5  -29.54  21.08  22.35   4.24  -1.47   0.22 -17.05  10.99 -10.53]\n",
      "[4] Input: \t.\n",
      "[4] Top: \t[['<STOP>', '.', '_']]\n",
      "[4] Pred: \t`<START> i i .` ~~~: <STOP>\n",
      "\n",
      "---------------------------\n",
      "[0] French: \t<START> va ! <STOP> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "[0] Indexed: \t[    0 27039     3     1     2     2     2     2     2     2]\n",
      "\n",
      "[1] Hidden:\t[-0.96 -0.93  1.    0.82  0.98 -0.88  0.73 -1.    1.   -0.75]\n",
      "[1] Const:\t[ -8.92 -31.24  26.84  36.98   3.52  -4.46   1.64 -32.29  15.76 -10.64]\n",
      "[1] Input: \t<START>\n",
      "[1] Top: \t[['i', 'you', '.']]\n",
      "[1] Pred: \t`<START>` ~~~: i\n",
      "\n",
      "[2] Hidden:\t[-0.92 -0.8   0.9   0.83  0.76 -0.99  0.8  -0.89  1.   -0.78]\n",
      "[2] Const:\t[ -8.75 -30.96  25.98  32.01   3.92  -2.85   1.1  -28.26  14.84 -11.16]\n",
      "[2] Input: \ti\n",
      "[2] Top: \t[['i', '.', 'you']]\n",
      "[2] Pred: \t`<START> i` ~~~: i\n",
      "\n",
      "[3] Hidden:\t[-0.8  -0.67  0.76  0.7   0.61 -0.97  0.54 -0.76  0.87 -0.62]\n",
      "[3] Const:\t[ -8.41 -30.66  23.96  27.4    4.17  -2.06   0.63 -22.75  13.07 -11.24]\n",
      "[3] Input: \ti\n",
      "[3] Top: \t[['.', 'i', '<STOP>']]\n",
      "[3] Pred: \t`<START> i i` ~~~: .\n",
      "\n",
      "[4] Hidden:\t[-0.73 -0.61  0.7   0.67  0.56 -0.82  0.19 -0.71  0.79 -0.58]\n",
      "[4] Const:\t[ -7.74 -30.37  21.61  22.91   4.25  -1.47   0.22 -17.5   11.09 -10.78]\n",
      "[4] Input: \t.\n",
      "[4] Top: \t[['<STOP>', '.', '_']]\n",
      "[4] Pred: \t`<START> i i .` ~~~: <STOP>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<START> i i . <STOP>'"
      ]
     },
     "execution_count": 757,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_not_finished(generated_english_word_ids):\n",
    "    \"\"\"\n",
    "    Checks if we have a stopping event encountered.\n",
    "    Resons to stop generating words:\n",
    "    * STOP_WORD encountered\n",
    "    * max_lenght words generated\n",
    "    \"\"\"\n",
    "    id_last_word_predicted = generated_english_word_ids[-1]\n",
    "    \n",
    "    is_stop_word_hit = id_last_word_predicted == en2id[STOP_WORD]\n",
    "    is_max_length_hit = len(generated_english_word_ids) == max_eng_size\n",
    "    \n",
    "    return not is_stop_word_hit and not is_max_length_hit \n",
    "\n",
    "\n",
    "def translate(fre_tokens: List[str], debug=True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Translates the given sentence from French to English.\n",
    "    \n",
    "    The input of this function is a list of french word tokens.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        # Scan the full french phrase and return the hidden state of the LSTM\n",
    "        # This will be used to initialize the first step of the LSTM english decoder\n",
    "        french_phrase = fr_input(fre_tokens)\n",
    "        print(f\"---------------------------\")\n",
    "        print(f\"[0] French: \\t{to_str(fr_decode(french_phrase[0]))[:80]}\")\n",
    "        print(f\"[0] Indexed: \\t{french_phrase[0][:10]}\\n\")\n",
    "    \n",
    "    eng_tokens = [START_WORD]\n",
    "    eng_inp = en_indexed(eng_tokens)\n",
    "\n",
    "    [hidden], [const] = encoder.predict([french_phrase])\n",
    "    \n",
    "    id_last_word_predicted = eng_inp[-1]\n",
    "    id_stop_word = en2id[PADD_WORD]\n",
    "    \n",
    "    # Continously make predictions until we have a stopping event\n",
    "    while is_not_finished(eng_inp):\n",
    "#         fixed_eng_inp = fixed_length_en([eng_inp[-1]], padd=id_stop_word)\n",
    "        fixed_eng_inp = [eng_inp[-1]]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[{len(eng_inp)}] Hidden:\\t{np.round(hidden, 2)[:10]}\")\n",
    "            print(f\"[{len(eng_inp)}] Const:\\t{np.round(const, 2)[:10]}\")\n",
    "            print(f\"[{len(eng_inp)}] Input: \\t{to_str(en_decode(fixed_eng_inp[:20]))}\")\n",
    "\n",
    "        # pass the decoder LSTM hidden staten and the currently decoded words\n",
    "        [one_hot_words], [hidden], [const] = decoder.predict([[fixed_eng_inp], [hidden], [const]])\n",
    "\n",
    "        # one_hot_words.shape = (53, 14203)\n",
    "        # predicted_words = (53,) \n",
    "        predicted_words = np.argmax(one_hot_words, axis=-1)\n",
    "\n",
    "        # the last word is the one corresponding to the last seen french word (if we discard the START_WORD) \n",
    "        # id_last_word_predicted = predicted_words[len(eng_inp) - 1]\n",
    "        id_last_word_predicted = predicted_words[0]\n",
    "        \n",
    "        if debug:\n",
    "            # print the top words predicted, for each timestep\n",
    "            top_words = np.argsort(one_hot_words, axis=-1)[:, ::-1][:3, :3]\n",
    "            print(f\"[{len(eng_inp)}] Top: \\t{[[id2en[_id] for _id in _list] for _list in top_words]}\")\n",
    "            print(f\"[{len(eng_inp)}] Pred: \\t`{to_str(en_decode(eng_inp))}` ~~~: {id2en[id_last_word_predicted]}\\n\")\n",
    "            \n",
    "        eng_inp.append(id_last_word_predicted)\n",
    "        \n",
    "\n",
    "    return en_decode(eng_inp)\n",
    "\n",
    "to_str(translate([\"Allez\", \"a\", \"la\", \"maison\", \"avec\", \"nous\", \".\"]))\n",
    "to_str(translate([\"Va\", \"!\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nous', \"n'avons\", 'eu', 'aucun', 'accident', 'majeur', '.']"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_indexed(tokens):\n",
    "    \"\"\"\n",
    "    Function that strips all the decorations from a indexed training sequence (containing ids).\n",
    "    \n",
    "    It removes the following:\n",
    "    * the initial START_WORD\n",
    "    * the ending STOP_WORD\n",
    "    * all the padding words added with fixed_length variant functions\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = tokens.tolist()\n",
    "    # exclude the initial START_WORD, the STOP_WORD and everything after it (padding words)\n",
    "    return tokens[1:tokens.index(en2id[STOP_WORD])]\n",
    "\n",
    "def strip_plaintext(tokens):\n",
    "    \"\"\"\n",
    "    Function that strips all the decorations from a plaintext training sequence (containing actual words).\n",
    "    \n",
    "    It removes the following:\n",
    "    * the initial START_WORD\n",
    "    * the ending STOP_WORD\n",
    "    * all the padding words added with fixed_length variant functions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # exclude the initial START_WORD, the STOP_WORD and everything after it (padding words)\n",
    "    return tokens[1:tokens.index(STOP_WORD)]\n",
    "\n",
    "def strip(tokens):\n",
    "    \"\"\"\n",
    "    Function that strips all the decorations from a training sequence.\n",
    "    It removes the following:\n",
    "    * the initial START_WORD\n",
    "    * the ending STOP_WORD\n",
    "    * all the padding words added with fixed_length variant functions\n",
    "    \n",
    "    This function can be applied to both indexed or plaintext sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __are_plaintext(tokens):\n",
    "        # Heuristic to test if the tokens are in plaintext or not.\n",
    "        # Assumed plaintext if known plaintext words (almost always guaranteed to be present) \n",
    "        # are included among the tokens.\n",
    "        # \n",
    "        # These signal words are:\n",
    "        # START_WORD, STOP_WORD\n",
    "        return STOP_WORD in tokens or START_WORD in tokens \n",
    "        \n",
    "    if __are_plaintext(tokens):\n",
    "        return strip_plaintext(tokens)\n",
    "    else:\n",
    "        return strip_indexed(tokens)\n",
    "\n",
    "strip(fr_decode(encode_input_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash and experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160872, 60), (160872, 53), (32, 100, 60), (32, 53))"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "gen = TimeseriesGenerator(encode_input_train, decode_input_train, 100, batch_size=32)\n",
    "\n",
    "x, y = gen[0]\n",
    "encode_input.shape, decode_input.shape, x.shape, y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "231px",
    "width": "259px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216.469px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
