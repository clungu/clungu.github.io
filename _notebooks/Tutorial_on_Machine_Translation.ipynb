{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:13:27.139779Z",
     "start_time": "2019-09-05T15:13:27.136526Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The Babel fish is small, yellow, leech-like - and probably the oddest thing in the universe. It feeds on brain wave energy, absorbing all unconscious frequencies and then excreting telepathically a matrix formed from the conscious frequencies and nerve signals picked up from the speech centres of the brain, the practical upshot of which is that if you stick one in your ear, you can instantly understand anything said to you in any form of language: the speech you hear decodes the brain wave matrix. [The Hitchhiker's Guide to the Galaxy]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will try to build (several) translation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://miro.medium.com/max/1780/1*YtTQh-C2_DgiPk_0T1xBlw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://miro.medium.com/max/1732/1*XuR_iuPOuY-8i5A3cGmcBw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is downloaded from \"http://www.manythings.org/anki/\". It's a set of English phrases and the equivalent French translations.\n",
    "\n",
    "The goal of course would be to create a system that takes one of them and outputs the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:36:20.335648Z",
     "start_time": "2019-09-05T14:36:19.906462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>French_tokens</th>\n",
       "      <th>English_tokens</th>\n",
       "      <th>English_length</th>\n",
       "      <th>French_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>[Va, !]</td>\n",
       "      <td>[Go, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>[Salut, !]</td>\n",
       "      <td>[Hi, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "      <td>[Cours, !]</td>\n",
       "      <td>[Run, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "      <td>[Courez, !]</td>\n",
       "      <td>[Run, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "      <td>[Ça, alors, !]</td>\n",
       "      <td>[Wow, !]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English      French   French_tokens English_tokens  English_length  \\\n",
       "0     Go.        Va !         [Va, !]        [Go, .]               2   \n",
       "1     Hi.     Salut !      [Salut, !]        [Hi, .]               2   \n",
       "2    Run!     Cours !      [Cours, !]       [Run, !]               2   \n",
       "3    Run!    Courez !     [Courez, !]       [Run, !]               2   \n",
       "4    Wow!  Ça alors !  [Ça, alors, !]       [Wow, !]               2   \n",
       "\n",
       "   French_length  \n",
       "0              2  \n",
       "1              2  \n",
       "2              2  \n",
       "3              2  \n",
       "4              3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_parquet(\"./_data/fra_df.hdf5\")\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Word dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first implementation is a really simple one: \n",
    "* Find a real \"dictionary\" (a set of Franch to English words)\n",
    "* Parse the French phrase and substitute all the words with the English correspondents.\n",
    "\n",
    "It will yield rubish translations of course, but it's a good starting place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary is taken from [here](https://github.com/pquentin/wiktionary-translations). Btw, it took me a great deal of time to really find this dictionary. You'd think that a 1-1 mapping of words between 2 really common languages should be easy especially for a pair of really popular languages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T20:37:39.256880Z",
     "start_time": "2019-09-04T20:37:33.382732Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-04 23:37:33--  https://raw.githubusercontent.com/pquentin/wiktionary-translations/master/frwiktionary-20140612-euradicfmt.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2950339 (2,8M) [text/plain]\n",
      "Saving to: ‘./_data/fre_eng.csv’\n",
      "\n",
      "./_data/fre_eng.csv 100%[===================>]   2,81M   526KB/s    in 5,2s    \n",
      "\n",
      "2019-09-04 23:37:39 (552 KB/s) - ‘./_data/fre_eng.csv’ saved [2950339/2950339]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/pquentin/wiktionary-translations/master/frwiktionary-20140612-euradicfmt.csv -O ./_data/fre_eng.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load the dataset to see what we can work with. Note that we have to figure out some hoops in order to properly load this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:36:23.270860Z",
     "start_time": "2019-09-05T14:36:23.141392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FrePOS</th>\n",
       "      <th>Dict</th>\n",
       "      <th>English</th>\n",
       "      <th>EngPOS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000e</th>\n",
       "      <td>J</td>\n",
       "      <td>TR-FR-EN</td>\n",
       "      <td>1000th</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100e</th>\n",
       "      <td>J</td>\n",
       "      <td>TR-FR-EN</td>\n",
       "      <td>100th</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 mètres haies</th>\n",
       "      <td>S</td>\n",
       "      <td>TR-FR-EN</td>\n",
       "      <td>100 metre hurdles</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 mètres</th>\n",
       "      <td>S</td>\n",
       "      <td>TR-FR-EN</td>\n",
       "      <td>100 metres</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 Downing Street</th>\n",
       "      <td>S</td>\n",
       "      <td>TR-FR-EN</td>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  FrePOS      Dict            English EngPOS\n",
       "French                                                      \n",
       "1000e                  J  TR-FR-EN             1000th      J\n",
       "100e                   J  TR-FR-EN              100th      J\n",
       "100 mètres haies       S  TR-FR-EN  100 metre hurdles      S\n",
       "100 mètres             S  TR-FR-EN         100 metres      S\n",
       "10 Downing Street      S  TR-FR-EN  10 Downing Street      S"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fre_eng = pd.read_csv(\"./_data/fre_eng.csv\", sep=\";\", header=None, index_col=0, names=[\"French\", \"FrePOS\", \"Dict\", \"English\", \"EngPOS\", \"UNK\"]).drop(columns=[\"UNK\"])\n",
    "df_fre_eng.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use the dictionary, let's load an example from the `to-be-translated` dataset to see an example. The snippet bellow also sketches a bit what we're going to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:36:25.715433Z",
     "start_time": "2019-09-05T14:36:25.708266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est-ce\n",
      "tellement\n",
      "mal\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.randint(raw.shape[0])\n",
    "\n",
    "for word in raw.iloc[sample].French_tokens:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying some formalism and good software engineering, we're going to define some functions for later use, and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:30:40.968353Z",
     "start_time": "2019-09-05T15:30:40.959178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['À', 'quelle', 'heure', \"l'atterrissage\", 'de', \"l'avion\",\n",
       "       'est-il', 'prévu', '?'], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_sample():\n",
    "    \"\"\"\n",
    "    Returns a random sample id from the database\n",
    "    \"\"\"\n",
    "    return np.random.randint(raw.shape[0])\n",
    "\n",
    "def get_sample(sample_id=None):\n",
    "    \"\"\"\n",
    "    Returns the tokens of a random sample specified by the given sample id.\n",
    "    \"\"\"\n",
    "    if sample_id is None:\n",
    "        sample_id = random_sample()\n",
    "    return raw.iloc[sample_id].French_tokens\n",
    "\n",
    "get_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the datasets and we have a index on the `French` column, we can use it to query for a translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:36:29.731399Z",
     "start_time": "2019-09-05T14:36:29.709843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FrePOS</th>\n",
       "      <th>Dict</th>\n",
       "      <th>English</th>\n",
       "      <th>EngPOS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>maison</th>\n",
       "      <td>J</td>\n",
       "      <td>TR-FR-EN</td>\n",
       "      <td>homemade</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maison</th>\n",
       "      <td>S</td>\n",
       "      <td>TR-FR-EN</td>\n",
       "      <td>home</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maison</th>\n",
       "      <td>S</td>\n",
       "      <td>TR-FR-EN</td>\n",
       "      <td>house</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maison</th>\n",
       "      <td>S</td>\n",
       "      <td>TR-FR-EN</td>\n",
       "      <td>outfit</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FrePOS      Dict   English EngPOS\n",
       "French                                  \n",
       "maison      J  TR-FR-EN  homemade      J\n",
       "maison      S  TR-FR-EN      home      S\n",
       "maison      S  TR-FR-EN     house      S\n",
       "maison      S  TR-FR-EN    outfit      S"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fre_eng.loc['maison']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get our first major `problem`: **polysemy**\n",
    "\n",
    "> the coexistence of many possible meanings for a word or phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you deal with this?\n",
    "\n",
    "* The dictionary also has a `part-of-speach` column that we can *maybe* use (if we really want to go on that route and learn a lot grammar rules) \n",
    "* Me, I've chosen the first value :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:35:29.905872Z",
     "start_time": "2019-09-05T15:35:29.893048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?>'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_equivalent(fre_word):\n",
    "    \"\"\"\n",
    "    Returns the equivalent word translation of the given french word, \n",
    "    using the loaded vocabulary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return df_fre_eng.loc[fre_word, \"English\"].iloc[0]\n",
    "    except:\n",
    "        return f\"<?>\"\n",
    "    \n",
    "get_equivalent(\"Maison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second `problem`: **capitalisation**\n",
    "\n",
    "Of course we can't expect to have all possible captalisation forms *a Word CAN hAVE, in oUr* dictionary so we're going to need a normalization scheme.\n",
    "\n",
    "Keeping it simple, we will convert everything to lower-case, and strip all the leading and ending space chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:36:41.932476Z",
     "start_time": "2019-09-05T15:36:41.925750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'si'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(word):\n",
    "    \"\"\"\n",
    "    Performs a simple normalization.\n",
    "    \"\"\"\n",
    "    return word.lower().strip()\n",
    "\n",
    "normalize(\"Si\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to try our translation \"model\". We will simply substitute all the words in french with their english equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:36:50.287373Z",
     "start_time": "2019-09-05T15:36:50.259460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Je', '<?>'),\n",
       " ('veux', '<?>'),\n",
       " ('acheter', 'buy'),\n",
       " ('une', '<?>'),\n",
       " ('montre', 'watch'),\n",
       " ('plus', 'and'),\n",
       " ('chère', 'cheer'),\n",
       " ('.', '<?>')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fre_tokens = get_sample()\n",
    "eng_tokens = [get_equivalent(normalize(word)) for word in fre_tokens]\n",
    "list(zip(fre_tokens, eng_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with a better dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe a better, more comprehensive dictionary would eliminate most of those `<?>` words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T08:34:07.265817Z",
     "start_time": "2019-09-05T08:34:05.431470Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-05 11:34:05--  https://raw.githubusercontent.com/freedict/fd-dictionaries/master/eng-fra/eng-fra.tei\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3329108 (3,2M) [text/plain]\n",
      "Saving to: ‘./_data/eng-fra.tei’\n",
      "\n",
      "./_data/eng-fra.tei 100%[===================>]   3,17M  5,57MB/s    in 0,6s    \n",
      "\n",
      "2019-09-05 11:34:07 (5,57 MB/s) - ‘./_data/eng-fra.tei’ saved [3329108/3329108]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/freedict/fd-dictionaries/master/eng-fra/eng-fra.tei -O ./_data/eng-fra.tei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T08:37:40.862013Z",
     "start_time": "2019-09-05T08:37:40.259869Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-05 11:37:40--  https://raw.githubusercontent.com/freedict/fd-dictionaries/master/shared/freedict-dictionary.css\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4783 (4,7K) [text/plain]\n",
      "Saving to: ‘./_data/freedict-dictionary.css’\n",
      "\n",
      "./_data/freedict-di 100%[===================>]   4,67K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-09-05 11:37:40 (41,1 MB/s) - ‘./_data/freedict-dictionary.css’ saved [4783/4783]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/freedict/fd-dictionaries/master/shared/freedict-dictionary.css -O ./_data/freedict-dictionary.css"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:36:37.320732Z",
     "start_time": "2019-09-05T14:36:37.140122Z"
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et \n",
    "xtree = et.parse(\"_data/eng-fra.xml\")\n",
    "xroot = xtree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:36:38.959236Z",
     "start_time": "2019-09-05T14:36:38.953686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element '{http://www.tei-c.org/ns/1.0}entry' at 0x7feb4618a548>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_entries():\n",
    "    \"\"\"\n",
    "    I know, it's XML..\n",
    "    \"\"\"\n",
    "    for entry in xroot.findall(\"{http://www.tei-c.org/ns/1.0}text\")[0].getchildren()[0].findall(\"{http://www.tei-c.org/ns/1.0}entry\"):\n",
    "        yield entry\n",
    "        \n",
    "entry = next(get_entries())\n",
    "entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:36:39.602833Z",
     "start_time": "2019-09-05T14:36:39.596687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abc', 'abc')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pairs(entry):\n",
    "    \"\"\"\n",
    "    XML parsing function. Yields pairs of words (french, english)\n",
    "    \"\"\"\n",
    "    form  = entry.find(\"{http://www.tei-c.org/ns/1.0}form\")\n",
    "    senses = entry.findall(\"{http://www.tei-c.org/ns/1.0}sense\")\n",
    "    orth = form.find(\"{http://www.tei-c.org/ns/1.0}orth\")\n",
    "    english = orth.text\n",
    "    for sense in senses:\n",
    "        try:\n",
    "            french = sense.find(\"{http://www.tei-c.org/ns/1.0}cit\").find(\"{http://www.tei-c.org/ns/1.0}quote\").text\n",
    "            yield normalize(french), normalize(english)\n",
    "        except:\n",
    "            print(sense)\n",
    "list(get_pairs(entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:36:40.949882Z",
     "start_time": "2019-09-05T14:36:40.906577Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = np.array([pair for entry in get_entries() for pair in get_pairs(entry)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's encapsulate these in a `Pandas` `DataFrame` and set the index on the `French` column so we have fast lookup time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:36:41.606036Z",
     "start_time": "2019-09-05T14:36:41.588529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abc</th>\n",
       "      <td>abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abécédaire</th>\n",
       "      <td>abc book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sida</th>\n",
       "      <td>aids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abkhasie</th>\n",
       "      <td>abkhazia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abkhasien</th>\n",
       "      <td>abkhazian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              English\n",
       "French               \n",
       "abc               abc\n",
       "abécédaire   abc book\n",
       "sida             aids\n",
       "abkhasie     abkhazia\n",
       "abkhasien   abkhazian"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fre_eng_2 = pd.DataFrame(data=pairs, columns=[\"French\", \"English\"]).set_index(\"French\")\n",
    "df_fre_eng_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to write the translation function again, using this new datastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:37:58.460241Z",
     "start_time": "2019-09-05T15:37:58.445958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?>'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_equivalent(fre_word):\n",
    "    \"\"\"\n",
    "    Returns the equivalent word translation of the given french word, \n",
    "    using the loaded vocabulary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return df_fre_eng.loc[fre_word, \"English\"].iloc[0]\n",
    "    except:\n",
    "        try:\n",
    "            return df_fre_eng_2.loc[fre_word, \"English\"].iloc[0]\n",
    "        except:\n",
    "            return \"<?>\"\n",
    "    \n",
    "get_equivalent(\"nous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it doesn't work any better.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:38:06.264049Z",
     "start_time": "2019-09-05T15:38:06.241639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Je', '<?>'), ('suis', '<?>'), ('fiable', 'reliable'), ('.', '<?>')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fre_tokens = get_sample()\n",
    "eng_tokens = [get_equivalent(normalize(word)) for word in fre_tokens]\n",
    "list(zip(fre_tokens, eng_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Problems`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we can't find all the words\n",
    "    * upgrate to a better dictionary\n",
    "    * still won't be able to include all the irregular forms a word will have \n",
    "* can't handle polisemy\n",
    "   \n",
    "**Conclusions**\n",
    "* this approach is not that feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words from the vocabulary are mapped to vectors of real numbers.\n",
    "\n",
    "This allows words that are used in similar ways to result in having similar representations, naturally capturing their meaning. This can be contrasted with the crisp but fragile representation in a bag of words model where, unless explicitly managed, different words have different representations, regardless of how they are used.\n",
    "\n",
    "Word embeddings are such a common appearence in NLP that it's not worth describind them in more details. There are a ton of online resources that can give you more details about them.\n",
    "\n",
    "The main idea of using them is that since an embedding encapsulates **semantic** representation, then we can solve the **polysemi** problem of the Model 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:57:41.426590Z",
     "start_time": "2019-09-05T15:57:41.409133Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DictNumpyEmbeddings():\n",
    "    def __init__(self, id2word, embed):\n",
    "        self._id2word = id2word\n",
    "        self.embed = embed\n",
    "        self.word2id = {word: id for id, word in enumerate(id2word)}\n",
    "        self._n_words = embed.shape[0]\n",
    "        self._n_dim = embed.shape[1]\n",
    "        assert len(id2word) == embed.shape[\n",
    "            0], 'The length of the vocabulary should be equal to the number of the embeddings'\n",
    "\n",
    "    @property\n",
    "    def matrix(self) -> np.array:\n",
    "        return self.embed\n",
    "\n",
    "    @property\n",
    "    def n_words(self):\n",
    "        return self._n_words\n",
    "\n",
    "    @property\n",
    "    def n_dim(self):\n",
    "        return self._n_dim\n",
    "\n",
    "    def __iter__(self):\n",
    "        for word in self._id2word:\n",
    "            yield word\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.word2id\n",
    "\n",
    "    def __getitem__(self, word: str) -> np.array:\n",
    "        if word not in self.word2id:\n",
    "            return None\n",
    "        return self.embed[self.word2id[word]]\n",
    "\n",
    "    @property\n",
    "    def id2word(self) -> np.array:\n",
    "        return self._id2word\n",
    "\n",
    "    def apply_transform(self, transform: str):\n",
    "        \"\"\"\n",
    "        Apply the given transformation to the vector space\n",
    "\n",
    "        Right-multiplies given transform with embeddings E:\n",
    "            E = E * transform\n",
    "\n",
    "        Transform can either be a string with a filename to a\n",
    "        text file containing a ndarray (compat. with np.loadtxt)\n",
    "        or a numpy ndarray.\n",
    "        \"\"\"\n",
    "        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n",
    "        self.embed = np.matmul(self.embed, transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:36:48.711541Z",
     "start_time": "2019-09-05T14:36:48.706349Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class NumpyReader():\n",
    "    def __init__(self, vector_file=''):\n",
    "        self.vector_file = vector_file\n",
    "\n",
    "    def load(self):\n",
    "        with np.load(self.vector_file) as store:\n",
    "            return DictNumpyEmbeddings(store['vocab'], store['embed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:36:51.196520Z",
     "start_time": "2019-09-05T14:36:49.499061Z"
    }
   },
   "outputs": [],
   "source": [
    "french_embeddings = NumpyReader(\"./_data/wiki.fr.small.npz\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T15:58:05.743402Z",
     "start_time": "2019-09-05T15:58:05.737052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_embeddings['je'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above that each of the (we use FastText) embeddings has 300 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we take a few words (somewhat) at random, and print them on the first few dimensions you can start notice that for pairs of words that are similar, some dimensions \"line-up\" that is, have roughly the same values (i.e. the same color on the plot). This means that on those dimensions (whatever they may represent, no one really knows..) these words have are really similar.\n",
    "\n",
    "For example `me` and `you` have lots of close values on lots of dimensions, some possibly describing the `refers to humans` capacity, or other common charateristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:37:10.192889Z",
     "start_time": "2019-09-05T14:37:09.991666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 960x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_words(words):\n",
    "    emb_matrix = [english_embeddings[word][:30].astype(np.float32) for word in words]\n",
    "    plt.matshow(emb_matrix)\n",
    "    plt.yticks(np.arange(len(words)), labels=words)\n",
    "    \n",
    "words = [\"curious\", \"google\", \"me\", \"you\", \"colorado\", \"donkey\", \"slum\", \"snake\",\"microsoft\"]\n",
    "show_words(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few experiments with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:37:12.649308Z",
     "start_time": "2019-09-05T14:37:12.214640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('curious', 'colorado'),\n",
       " ('google', 'me'),\n",
       " ('me', 'you'),\n",
       " ('you', 'me'),\n",
       " ('colorado', 'snake'),\n",
       " ('donkey', 'colorado'),\n",
       " ('slum', 'snake'),\n",
       " ('snake', 'colorado'),\n",
       " ('microsoft', 'google')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_matirx(words):\n",
    "    word_emb = [english_embeddings[word][:30].astype(np.float32) for word in words]\n",
    "    return cosine_similarity(word_emb, word_emb)\n",
    "\n",
    "matrix = cosine_matirx(words)\n",
    "matrix[np.arange(matrix.shape[0]), np.arange(matrix.shape[0])] = 0\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.matshow(matrix)\n",
    "plt.xticks(np.arange(len(words)), labels=words, rotation=90)\n",
    "plt.yticks(np.arange(len(words)), labels=words);\n",
    "\n",
    "[(words[i], words[amax]) for i, amax in enumerate(np.argmax(matrix, axis=0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good measure for comparing two embeddings is by using the `cosine_similarity` metric. We use it above to compute a squared similarity matrix by comparing each word by the others. The highest values (max. is 1) represents words really similar. Close to 0 are words that have not relation, and close to -1 are words that should represent opposite concepts.\n",
    "\n",
    "*Note: Although `cosine_similarity(word, word) == 1` so our fist diagonal should be 1, we've set it to 0 so to highlight the other similarities.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, in order to make a translation we need both languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:37:09.507203Z",
     "start_time": "2019-09-05T14:37:05.757594Z"
    }
   },
   "outputs": [],
   "source": [
    "english_embeddings = NumpyReader(\"./_data/wiki.en.small.npz\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of our second algorithm is a follows:\n",
    "* convert the french phrase to word tokens \n",
    "* convert the tokens to embeddings\n",
    "* for each embedding search the corresponding embedding in the english language that is the closest to the current one\n",
    "* do this for all french tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that embeddings are trained **independently** for each language. So the resulting embedding don't overlap really well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about this as viewing the same object (the world - expressed / shaped by roughly the same words) but viewed from diffrent angles.\n",
    "\n",
    "Each training, depending from where the random weights were initialized from, will end up viewing the world from a slightly different angle so the language (through the words that describe it) will be a little tilted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately for us we recently [have a way](https://arxiv.org/pdf/1804.07745.pdf) to align the two spaces, and have a common bilingual (or multilingual for that matter) embeddings space. There's even a set of word embeddings already alligned for you by Facebook on the original [FastText site](https://fasttext.cc/docs/en/aligned-vectors.html).\n",
    "\n",
    "We're also using some pieaces of code from [here](https://github.com/babylonhealth/fastText_multilingual/blob/master/fasttext.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:19:37.204442Z",
     "start_time": "2019-09-05T16:19:37.187958Z"
    }
   },
   "outputs": [],
   "source": [
    "def lazy_property(fn):\n",
    "    '''\n",
    "    Decorator that makes a property lazy-evaluated.\n",
    "    '''\n",
    "    attr_name = '_lazy_' + fn.__name__\n",
    "\n",
    "    @property\n",
    "    def _lazy_property(self):\n",
    "        if not hasattr(self, attr_name):\n",
    "            setattr(self, attr_name, fn(self))\n",
    "        return getattr(self, attr_name)\n",
    "\n",
    "    return _lazy_property\n",
    "\n",
    "class EmbeddingSpace:\n",
    "    def __init__(self, embed):\n",
    "        self.embeddings = embed\n",
    "\n",
    "    @lazy_property\n",
    "    def embeddings_normalized(self):\n",
    "        return EmbeddingSpace.normalised(self.embeddings.matrix)\n",
    "\n",
    "    def translate_nearest_neighbour(self, source_vector):\n",
    "        \"\"\"Obtain translation of source_vector using nearest neighbour retrieval\"\"\"\n",
    "        similarity_vector = np.matmul(self.embeddings_normalized, source_vector)\n",
    "        print(self.embeddings_normalized.shape, source_vector.shape, similarity_vector.shape)\n",
    "        target_ids = np.argsort(similarity_vector)[::-1][:10]\n",
    "        print(target_ids)\n",
    "        return [self.embeddings.id2word[target_id] for target_id in target_ids]\n",
    "\n",
    "    @classmethod\n",
    "    def normalised(cls, mat, axis=-1, order=2):\n",
    "        \"\"\"Utility function to normalise the rows of a numpy array.\"\"\"\n",
    "        norm = np.linalg.norm(\n",
    "            mat, axis=axis, ord=order, keepdims=True)\n",
    "        norm[norm == 0] = 1\n",
    "        return mat / norm\n",
    "\n",
    "    @classmethod\n",
    "    def cosine_similarity(cls, vec_a, vec_b):\n",
    "        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "        return np.dot(vec_a, vec_b) / \\\n",
    "               (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "\n",
    "french_space = EmbeddingSpace(french_embeddings)\n",
    "english_space = EmbeddingSpace(english_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to thest this out.. Let's take the word `je` from French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:15:51.588336Z",
     "start_time": "2019-09-05T16:15:51.581632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_word = french_embeddings['je']\n",
    "target_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the `dot product` between it and all the embeddings of the english language will yield the top part of the `cosine_similarity` formula.\n",
    "\n",
    "K(X, Y) = <X, Y> / (||X||*||Y||)\n",
    "\n",
    "Note that we're using embeddings already normalized by the l2 norm of each word (i.e. Emb(Y) = Orig(X) / ||Orig(X)||, and the l2 norm of Y (`je` the word we're comparing all the vocabulary with) has the same norm for all words (constant).\n",
    "\n",
    "This means that the order of similarity is the same only by doing the `dot product` without further dividing by the (constant) ||Y||. That's why we do an `argsort` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:17:13.098595Z",
     "start_time": "2019-09-05T16:17:01.277560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2519368,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_vector = np.matmul(english_space.embeddings_normalized, target_word)\n",
    "similarity_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:18:02.056730Z",
     "start_time": "2019-09-05T16:18:01.795496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2411,  8284,    48,    26,  9424, 19584, 15942,   281,   898,\n",
       "         492])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_sorted = np.argsort(similarity_vector)[::-1]\n",
    "ids_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most similar indexes of word in the english vocabulary, similar to `je`. These are represented by the list of words bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:18:53.430047Z",
     "start_time": "2019-09-05T16:18:53.422553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['myself',\n",
       " 'btw',\n",
       " 'you',\n",
       " 'i',\n",
       " 'honestly',\n",
       " 'fwiw',\n",
       " 'anyways',\n",
       " 'think',\n",
       " 'sure',\n",
       " 've']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[english_space.embeddings.id2word[target_id] for target_id in ids_sorted[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:20:28.559767Z",
     "start_time": "2019-09-05T16:19:42.536070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2519368, 300) (300,) (2519368,)\n",
      "[ 2411  8284    48    26  9424 19584 15942   281   898   492]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['myself',\n",
       " 'btw',\n",
       " 'you',\n",
       " 'i',\n",
       " 'honestly',\n",
       " 'fwiw',\n",
       " 'anyways',\n",
       " 'think',\n",
       " 'sure',\n",
       " 've']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_space.translate_nearest_neighbour(french_embeddings['je'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way for jumping from one language to the other with the help of alligned ambeddings, let's demonstrate how our (naive) algorithm (version 2) works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:38:10.877212Z",
     "start_time": "2019-09-05T14:38:10.872316Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_equivalent(word):\n",
    "    return english_space.translate_nearest_neighbour(french_embeddings[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T22:51:21.017072Z",
     "start_time": "2019-09-04T22:49:10.205092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e826abe0685040c28a1e4ff101800cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Il', 'he'),\n",
       " ('y', '¿quíeres'),\n",
       " ('a', 'had'),\n",
       " ('un', 'a'),\n",
       " ('parc', 'park'),\n",
       " ('au', 'the'),\n",
       " ('milieu', 'mid'),\n",
       " ('de', 'of'),\n",
       " ('la', 'the'),\n",
       " ('ville', 'city'),\n",
       " ('.', '⁃')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "fre_tokens = get_sample()\n",
    "eng_tokens = [get_equivalent(normalize(word))[0] for word in tqdm(fre_tokens)]\n",
    "list(zip(fre_tokens, eng_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solves:\n",
    "* every word should have a corespondant translated (no more <?> words)\n",
    "    * because the vocabulary is really large now\n",
    "* compund words\n",
    "* synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "* slow! (2 min / phrase)\n",
    "* some words are not perfectly alligned\n",
    "* some words are not english (`¿quíeres`)\n",
    "* 1-1 translation doesn't work (`au` -> `<in> the`)\n",
    "* the sequencing of words doesn't have any role in the translation (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a model whose main idea is to **predict the next word**, given:\n",
    "* a **full** french phrase (eg. \"Il fait bien!\")\n",
    "* some initial english words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to allocate some weights and these should be fixed, we need to know, in advance the **maximum** size of a phrase, in each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:38:14.211718Z",
     "start_time": "2019-09-05T14:38:14.200491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 60)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_eng_size = raw.English_length.max()\n",
    "max_fre_size = raw.French_length.max()\n",
    "\n",
    "max_eng_size, max_fre_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to build some auxiliary datastructures that will allow us to:\n",
    "* vectorize\n",
    "* un-vectorize\n",
    "* normalize words\n",
    "* add padding\n",
    "* un-padd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id2Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:38:17.279003Z",
     "start_time": "2019-09-05T14:38:17.274918Z"
    }
   },
   "outputs": [],
   "source": [
    "subsample = 10000\n",
    "subsample = -1 # all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:39:45.615737Z",
     "start_time": "2019-09-05T14:38:18.754248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fffad6021e447b0930ae02851cd4642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9594afbf2d4a4982ca8c1c7c82d912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14200, 28614)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm import trange\n",
    "\n",
    "def flatten(series: list) -> list:\n",
    "    \"\"\"\n",
    "    Flattens the given list and returns a list of (unique) tokens\n",
    "    \"\"\"\n",
    "    return list(reduce(lambda collect, value: collect | set(value), series, set()))\n",
    "\n",
    "def unique(vocabulary: list) -> list:\n",
    "    return list(set(vocabulary))\n",
    "\n",
    "def lower(vocabulary: list) -> list:\n",
    "    return list(map(str.lower, vocabulary))\n",
    "\n",
    "id2en = sorted(unique(lower(flatten(tqdm(raw.English_tokens[:subsample])))))\n",
    "id2fr = sorted(unique(lower(flatten(tqdm(raw.French_tokens[:subsample])))))\n",
    "\n",
    "max_eng_size = raw.English_length[:subsample].max()\n",
    "max_fre_size = raw.French_length[:subsample].max()\n",
    "\n",
    "len(id2en), len(id2fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will rely on a set of special words that will be used to signal the start, ending and possibly padding characters which we will add to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:39:47.418625Z",
     "start_time": "2019-09-05T14:39:47.408880Z"
    }
   },
   "outputs": [],
   "source": [
    "START_WORD = \">\"\n",
    "STOP_WORD = \"_\"\n",
    "\n",
    "SPECIAL_WORDS = [START_WORD, STOP_WORD]\n",
    "\n",
    "assert set(SPECIAL_WORDS) & set(id2en) == set()\n",
    "assert set(SPECIAL_WORDS) & set(id2fr) == set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the special words as the first elements of each vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:39:48.897939Z",
     "start_time": "2019-09-05T14:39:48.892321Z"
    }
   },
   "outputs": [],
   "source": [
    "id2en = SPECIAL_WORDS + id2en\n",
    "id2fr = SPECIAL_WORDS + id2fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to save the vocabulary to ensure that reruning the notebook will not result in a reordering of the words (and invalidate subsequently trained and saved models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T09:39:33.041466Z",
     "start_time": "2019-02-11T09:39:32.931713Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savez(\"./_data/fr_en_vocabulary.npz\", \n",
    "         id2en=id2en, \n",
    "         id2fr=id2fr, \n",
    "         max_eng_size=max_eng_size, \n",
    "         max_fre_size=max_fre_size, \n",
    "         start_word=START_WORD,\n",
    "         stop_word=STOP_WORD\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reversed lookup dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:39:51.366829Z",
     "start_time": "2019-09-05T14:39:51.291366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba470acbbaf44bda0d35c9a51359e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14202), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c878bb197ad4884b49ef060fcbf1c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28616), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def word2id(elements: list) -> Dict[str, int]:\n",
    "    return {word: _id for _id, word in enumerate(elements)}\n",
    "    \n",
    "en2id = word2id(tqdm(id2en))\n",
    "fr2id = word2id(tqdm(id2fr))\n",
    "\n",
    "# Test that for the first 100 elements the two datastructures are conssitent\n",
    "for i in range(100):\n",
    "    assert fr2id[id2fr[i]] == i\n",
    "    assert en2id[id2en[i]] == i\n",
    "    \n",
    "# Test that we have the special words in front\n",
    "assert fr2id[START_WORD] == 0\n",
    "assert en2id[STOP_WORD] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:39:53.562400Z",
     "start_time": "2019-09-05T14:39:53.556351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>', '_', '!', '$', '%', '&', \"'\", \"''\", \"'maison\", \"'oublie\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2fr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mark a \"phrase\" as a sequence of words, encapsulated by the `START_WORD` and `STOP_WORD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:39:54.967744Z",
     "start_time": "2019-09-05T14:39:54.958610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>', 'va', '!', '_']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phrase(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Padds the token list with START and STOP words\n",
    "    \"\"\"\n",
    "    return [START_WORD] + lower(tokens) + [STOP_WORD]\n",
    "\n",
    "# Each phrase increases with 2 more elements so the maxiums change as well\n",
    "max_eng_size = max_eng_size + 2\n",
    "max_fre_size = max_fre_size + 2\n",
    "\n",
    "phrase(['Va', '!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:50:09.259131Z",
     "start_time": "2019-09-05T16:50:09.245437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 27005, 2, 1], [0, 4402, 3149, 27005, 234, 1])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def indexed(tokens: List[str], vocabulary: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Returns a vectorized list for the given token list. \n",
    "    The vectorisation is done using the given vocabulary mapping. \n",
    "    \"\"\"\n",
    "    return [vocabulary[word] for word in tokens]\n",
    "\n",
    "from functools import partial\n",
    "fr_indexed = partial(indexed, vocabulary=fr2id)\n",
    "en_indexed = partial(indexed, vocabulary=en2id)\n",
    "\n",
    "indexed(phrase(['Va', '!']), vocabulary=fr2id), fr_indexed(phrase(['Comment', 'ca', 'va', '?']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we will padd words (because we use batch training and each element in a batch should have the same size). This means that e need to right padd all the phrases with a `null` char until all are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:39:57.919018Z",
     "start_time": "2019-09-05T14:39:57.903118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def padd_right(tokens: list, fixed_size=3, padd=\"|\")-> list:\n",
    "    \"\"\"\n",
    "    Adds enought padding characters to the right of the given list to as to make it a fixed size == fixed_size\n",
    "    \"\"\"\n",
    "    l = len(tokens)\n",
    "    to_padd = max(0, fixed_size - l)\n",
    "    return list(tokens) + ([padd] * to_padd)\n",
    "\n",
    "padded = padd_right([1, 2, 3], fixed_size=10, padd=0)\n",
    "\n",
    "fixed_length_fr = partial(padd_right, fixed_size=max_fre_size, padd=STOP_WORD)\n",
    "fixed_length_en = partial(padd_right, fixed_size=max_eng_size, padd=STOP_WORD)\n",
    "\n",
    "assert len(padded) == 10\n",
    "assert padded[-1] == 0\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:39:59.476548Z",
     "start_time": "2019-09-05T14:39:59.469677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n"
     ]
    }
   ],
   "source": [
    "def to_str(tokens: List[object]):\n",
    "    \"\"\"\n",
    "    Pretty prints a list of tokens\n",
    "    \"\"\"\n",
    "    return \" \".join(map(str, tokens))\n",
    "    \n",
    "print(to_str(fixed_length_fr([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:40:16.291267Z",
     "start_time": "2019-09-05T14:40:16.284158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'va ! _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(indexed_tokens: List[int], vocabulary: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Decodes a list of token ids to into a list of words, using the given vocabulary\n",
    "    \"\"\"\n",
    "    return [vocabulary[_id] for _id in indexed_tokens]\n",
    "\n",
    "en_decode = partial(decode, vocabulary=id2en)\n",
    "fr_decode = partial(decode, vocabulary=id2fr)\n",
    "\n",
    "to_str(fr_decode(encode_input[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:40:15.421213Z",
     "start_time": "2019-09-05T14:40:10.012378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9910934c1af5401d87718bd297e60fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5142514759d457cb6dc0a0b48e41a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=160871), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((160871, 62), (160871, 53))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_length_fre_generator = map(lambda tokens: fr_indexed(fixed_length_fr(lower(tokens))), tqdm(raw.French_tokens[:subsample]))\n",
    "fixed_length_eng_generator = map(lambda tokens: en_indexed(fixed_length_en(phrase(tokens))), tqdm(raw.English_tokens[:subsample]))\n",
    "\n",
    "encode_input = np.array(list(fixed_length_fre_generator), dtype=np.int32)\n",
    "decode_input = np.array(list(fixed_length_eng_generator), dtype=np.int32)\n",
    "\n",
    "encode_input.shape, decode_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:40:18.307063Z",
     "start_time": "2019-09-05T14:40:18.269830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160871, 53)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_output = np.full_like(decode_input, fill_value=en2id[STOP_WORD])\n",
    "\n",
    "decode_output[:,:-1] = decode_input[:, 1:]\n",
    "\n",
    "# Test that the decode_output is one timestep ahead\n",
    "assert np.alltrue(decode_output[:, :max_eng_size-1] == decode_input[:, 1:max_eng_size])\n",
    "\n",
    "decode_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:40:20.061131Z",
     "start_time": "2019-09-05T14:40:20.053904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0, 5597,   20,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32),\n",
       " array([5597,   20,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_input[0], decode_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:59:57.519035Z",
     "start_time": "2019-09-05T16:59:57.511443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['>', 'craignez-vous', '...', '_', '_', '_', '_', '_', '_', '_',\n",
       "       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n",
       "       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n",
       "       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n",
       "       '_', '_', '_', '_'], dtype='<U13')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(fr_decode(decode_input[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model needs to learn that:\n",
    "```\n",
    "0 -> 5597, after that\n",
    "5597 -> 20, after that\n",
    "20 -> 1, after that\n",
    "1 -> 1, etc.\n",
    "```\n",
    "all in one go!\n",
    "\n",
    "Even though the model doesn't actually predict `5597`, `20`, `1`, `1` at each step, on next steps of the training we will asume that those were the correct answers and continue (with the same state) with thsese (also called teacher-forcing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:40:21.846692Z",
     "start_time": "2019-09-05T14:40:21.780005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128696, 62), (128696, 53), (128696, 53))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "encode_input_train, encode_input_test, decode_input_train, decode_input_test, decode_output_train, decode_output_test = train_test_split(encode_input, decode_input, decode_output, test_size=0.2, shuffle=True)\n",
    "\n",
    "encode_input_train.shape, decode_input_train.shape, decode_output_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full training data (lables), when one-hot-encoded take arround 11GB of RAM space, which is larger than we might want. We use a generator to split batches and one-hot-encode a batch at a time. This is more time consuming but way more space efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:40:24.777724Z",
     "start_time": "2019-09-05T14:40:23.836613Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((64, 62), (64, 53), (64, 53, 14202))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "english_language_vocabulary_size = len(id2en)\n",
    "batch_size = 64\n",
    "\n",
    "def data_generator(encode_input, decode_input, decode_output, batch_size=32, subsample=10000):\n",
    "    assert encode_input.shape[0] == decode_input.shape[0] == decode_output.shape[0]\n",
    "    length = encode_input.shape[0] if not subsample else subsample\n",
    "    while True:\n",
    "        for i in range(0, length, batch_size):\n",
    "            yield [encode_input[i:i+batch_size, :], decode_input[i:i+batch_size, :]], to_categorical(decode_output[i:i+batch_size, :], num_classes=english_language_vocabulary_size)\n",
    "            \n",
    "gen = data_generator(encode_input_train, decode_input_train, decode_output_train, batch_size=batch_size)\n",
    "[inp, tch], out = next(gen)\n",
    "inp.shape, tch.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:40:26.927077Z",
     "start_time": "2019-09-05T14:40:26.915925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('je suis désolé , je ne parle pas français . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ',\n",
       " \"> i 'm sorry , i do n't speak french . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\",\n",
       " \"i 'm sorry , i do n't speak french . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 11\n",
    "\n",
    "def _fr(data):\n",
    "    return to_str(fr_decode(data))[:100]\n",
    "\n",
    "def _en(data):\n",
    "    return to_str(en_decode(data))[:100]\n",
    "\n",
    "_fr(inp[sample]), _en(tch[sample]), _en(np.argmax(out[sample], axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:40:30.841713Z",
     "start_time": "2019-09-05T14:40:30.836386Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:40:35.148391Z",
     "start_time": "2019-09-05T14:40:34.693340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "FreTokens (InputLayer)          (None, 62)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngTokens (InputLayer)          (None, 53)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FreEmbeds (Embedding)           (None, 62, 200)      5723200     FreTokens[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "EngEmbeds (Embedding)           (None, 53, 200)      2840400     EngTokens[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FreLSTM (LSTM)                  [(None, 201), (None, 323208      FreEmbeds[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "EngLSTM (LSTM)                  [(None, 53, 201), (N 323208      EngEmbeds[0][0]                  \n",
      "                                                                 FreLSTM[0][1]                    \n",
      "                                                                 FreLSTM[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 53, 14202)    2868804     EngLSTM[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,078,820\n",
      "Trainable params: 12,078,820\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Note: turns out that we can pass the shape shape=(None,) and this will make the model a variable length one!\n",
    "fre = Input(shape=(max_fre_size,), name=\"FreTokens\")\n",
    "eng = Input(shape=(max_eng_size,), name=\"EngTokens\")\n",
    "\n",
    "fre_emb = Embedding(input_dim=len(id2fr), output_dim=200, name=\"FreEmbeds\")\n",
    "eng_emb = Embedding(input_dim=len(id2en), output_dim=200, name=\"EngEmbeds\")\n",
    "\n",
    "# TODO: this shouldn't have used return_state=True!!\n",
    "fre_lstm = LSTM(201, name=\"FreLSTM\", return_state=True)\n",
    "\n",
    "# Although we don't need in the trainign session, the internal state of the decoder part\n",
    "# (so we could pass return_state=False) we want to reuse the same layer in the decoder model\n",
    "# used for inference, so we use retrurn_state=True after all \n",
    "eng_lstm = LSTM(201, name=\"EngLSTM\", return_state=True, return_sequences=True)\n",
    "\n",
    "\n",
    "# We recieve three things here because we've set return_state=True.\n",
    "# We also get the internal states of the RNN (hidden, constants)\n",
    "# \n",
    "# hidden = <tf.Tensor 'FreLSTM_4/while/Exit_3:0' shape=(?, 10) dtype=float32>,\n",
    "# constants = <tf.Tensor 'FreLSTM_4/while/Exit_4:0' shape=(?, 10) dtype=float32>\n",
    "single_fre_out, fre_hidden_state, fre_constants = fre_lstm(fre_emb(fre))\n",
    "\n",
    "# At this stage, the single_fre_out is irrelevant to us. What we want to get \n",
    "# is only the internal state of the LSTM\n",
    "_ = single_fre_out\n",
    "\n",
    "# Initialize the English LSTM with the hidden state that we got by passing through \n",
    "# all the words in the French sentence. We basically start from the point that LSTM stopped and we wish to \n",
    "# \"go back in time\" if we can, but reversing in a different language (English).\n",
    "all_eng_out, eng_hidden_state, eng_constants = eng_lstm(eng_emb(eng), initial_state=[fre_hidden_state, fre_constants])\n",
    "\n",
    "# I'm still unsure what's the difference bewteen applying TimeDistributed vs. Dense layer directly.\n",
    "# Both seem to act the same\n",
    "#\n",
    "# We will need to reuse the trained Dense layer so we can requre it to the inference inputs\n",
    "eng_decode = Dense(len(id2en), activation=\"softmax\")\n",
    "output = eng_decode(all_eng_out)\n",
    "\n",
    "model = Model(inputs=[fre, eng], outputs=[output])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T19:59:28.357348Z",
     "start_time": "2019-09-03T19:59:28.351316Z"
    }
   },
   "source": [
    "Note:\n",
    "* that the model is quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:41:01.352470Z",
     "start_time": "2019-09-05T14:40:47.454237Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0905 17:40:47.624840 140650209232704 deprecation.py:323] From /home/tecknoworks/~Envs/deep3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0905 17:40:48.635371 140650209232704 deprecation.py:323] From /home/tecknoworks/~Envs/deep3.6/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  23/2010 [..............................] - ETA: 17:15 - loss: 3.3896"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-5f73b23539b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_output_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubsample_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhistory2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/~Envs/deep3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/~Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/~Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/~Envs/deep3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/~Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/~Envs/deep3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/~Envs/deep3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1439\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subsample_train = None # encode_input_train.shape[0]\n",
    "batch_size = 64\n",
    "subsample_test = None  # encode_input_test.shape[0]\n",
    "epochs = 100\n",
    "\n",
    "steps_per_epoch_train = (subsample_train if subsample_train else encode_input_train.shape[0]) // batch_size\n",
    "steps_per_epoch_test = (subsample_test if subsample_test else encode_input_test.shape[0]) // batch_size\n",
    "\n",
    "train_gen = data_generator(encode_input_train, decode_input_train, decode_output_train, batch_size=batch_size, subsample=subsample_train)\n",
    "test_gen = data_generator(encode_input_test, decode_input_test, decode_output_test, batch_size=batch_size, subsample=subsample_test)\n",
    "\n",
    "history2 = model.fit_generator(train_gen, steps_per_epoch=steps_per_epoch_train, epochs=epochs, validation_data=test_gen, validation_steps=steps_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:54:54.687312Z",
     "start_time": "2019-09-04T13:54:54.478348Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"./_data/fre_eng_lstm_fixed.ep101.hdf5\",overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:56:21.513807Z",
     "start_time": "2019-09-05T14:56:21.466810Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"./_data/fre_eng_lstm_fixed.ep100.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:41:09.830069Z",
     "start_time": "2019-09-05T14:41:09.812709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['je', 'pense', 'que', 'je', 'peux', 'réparer', 'ça', '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_indexed(tokens):\n",
    "    \"\"\"\n",
    "    Function that strips all the decorations from a indexed training sequence (containing ids).\n",
    "    \n",
    "    It removes the following:\n",
    "    * the initial START_WORD\n",
    "    * the ending STOP_WORD\n",
    "    * all the padding words added with fixed_length variant functions\n",
    "\n",
    "    \"\"\"\n",
    "    if type(tokens) != list:\n",
    "        tokens = tokens.tolist()\n",
    "        \n",
    "    # exclude the initial START_WORD, the STOP_WORD and everything after it (padding words)\n",
    "    stop_word_id = en2id[STOP_WORD]\n",
    "    if stop_word_id in tokens:\n",
    "        return tokens[0:tokens.index(en2id[STOP_WORD])]\n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "def strip_plaintext(tokens):\n",
    "    \"\"\"\n",
    "    Function that strips all the decorations from a plaintext training sequence (containing actual words).\n",
    "    \n",
    "    It removes the following:\n",
    "    * the initial START_WORD\n",
    "    * the ending STOP_WORD\n",
    "    * all the padding words added with fixed_length variant functions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # exclude the initial START_WORD, the STOP_WORD and everything after it (padding words)\n",
    "    if STOP_WORD in tokens:\n",
    "        return tokens[0:tokens.index(STOP_WORD)]\n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "def strip(tokens):\n",
    "    \"\"\"\n",
    "    Function that strips all the decorations from a training sequence.\n",
    "    It removes the following:\n",
    "    * the initial START_WORD\n",
    "    * the ending STOP_WORD\n",
    "    * all the padding words added with fixed_length variant functions\n",
    "    \n",
    "    This function can be applied to both indexed or plaintext sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __are_plaintext(tokens):\n",
    "        # Heuristic to test if the tokens are in plaintext or not.\n",
    "        # Assumed plaintext if known plaintext words (almost always guaranteed to be present) \n",
    "        # are included among the tokens.\n",
    "        # \n",
    "        # These signal words are:\n",
    "        # START_WORD, STOP_WORD\n",
    "        return STOP_WORD in tokens or START_WORD in tokens \n",
    "        \n",
    "    if __are_plaintext(tokens):\n",
    "        return strip_plaintext(tokens)\n",
    "    else:\n",
    "        return strip_indexed(tokens)\n",
    "\n",
    "strip(fr_decode(encode_input_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:41:11.484947Z",
     "start_time": "2019-09-05T14:41:11.474921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[992,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1]]),\n",
       " array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"Allez\", \"!\"]\n",
    "\n",
    "def fr_input(tokens):\n",
    "    return np.expand_dims(np.array(fr_indexed(fixed_length_fr(lower(tokens)))), 0)\n",
    "\n",
    "def en_input(tokens: list):\n",
    "    return np.expand_dims(np.array(en_indexed(fixed_length_en(tokens))), 0)\n",
    "\n",
    "fr_input(tokens), en_input([START_WORD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to manually add one word at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:41:15.206967Z",
     "start_time": "2019-09-05T14:41:15.201051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17938, 26738, 26929, 16713,  6732, 14413,    19,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = encode_input_train[12]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:41:16.802743Z",
     "start_time": "2019-09-05T14:41:16.795569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['on', 'trouvera', 'un', 'moyen', 'de', \"l'utiliser\", '.', '_', '_',\n",
       "       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n",
       "       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n",
       "       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n",
       "       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n",
       "       '_'], dtype='<U10')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(fr_decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:41:19.371788Z",
     "start_time": "2019-09-05T14:41:19.365360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on', 'trouvera', 'un', 'moyen', 'de', \"l'utiliser\", '.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_plaintext(fr_decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:41:20.983726Z",
     "start_time": "2019-09-05T14:41:20.977251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17938, 26738, 26929, 16713,  6732, 14413,    19,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = encode_input_train[12]\n",
    "tokens = strip_plaintext(fr_decode(tokens))\n",
    "fr_input(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T17:19:14.486873Z",
     "start_time": "2019-09-05T17:19:14.420269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 53, 14202)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x=[fr_input(tokens), en_input([START_WORD])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T17:20:42.781802Z",
     "start_time": "2019-09-05T17:20:42.214992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "find\n",
      "a\n",
      "way\n",
      "to\n",
      "happen\n",
      "to\n",
      "it\n",
      ".\n",
      "_\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"on trouvera un moyen de l'utiliser .\", '> we find a way to happen to it . _')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = [\"Allez\", \"a\", \"la\", \"voiture\", \"avec\", \"nous\", \".\"]\n",
    "tokens = encode_input_train[12]\n",
    "tokens = strip_plaintext(fr_decode(tokens))\n",
    "\n",
    "pred = [START_WORD]\n",
    "while pred[-1] != STOP_WORD:\n",
    "    p = np.argmax(model.predict(x=[fr_input(tokens), en_input(pred)]), axis=-1)[0][len(pred)-1]\n",
    "    pred.append(id2en[p])\n",
    "    print(id2en[p])\n",
    "\" \".join(tokens), \" \".join(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the generating loop I'm going to quickly simulate it in order to see that everything has been traing correctly and I've not screwed up. \n",
    "\n",
    "*Note: I've been blocked at the bellow couple of cells much, much more than it seems, as it was the main debug point that helped me see that the model was not performing and made me go back and fix the model (several times).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T17:23:57.486572Z",
     "start_time": "2019-09-05T17:23:57.091886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['she', 'is', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['she', 'is', 'a', '_', '_', '_', '_', '_', '_', '_']\n",
      "['she', 'is', 'a', 'beautiful', 'thank', '_', '_', '_', '_', '_']\n",
      "['she', 'is', 'a', 'beautiful', 'university', '_', '_', '_', '_', '_']\n",
      "['she', 'is', 'a', 'beautiful', 'university', 'her', '_', '_', '_', '_']\n",
      "['she', 'is', 'a', 'beautiful', 'university', 'her', '.', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"Elle\", \"est\", \"une\", \"belle\", \"jeune\"]\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\", \"is\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\", \"is\", \"a\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\", \"is\", \"a\", \"beautiful\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\", \"is\", \"a\", \"beautiful\", \"university\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"she\", \"is\", \"a\", \"beautiful\", \"university\", \"her\"])]), axis=-1)[0])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T17:24:09.403205Z",
     "start_time": "2019-09-05T17:24:08.940940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['why', 'why', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['why', 'did', 'did', '_', '_', '_', '_', '_', '_', '_']\n",
      "['why', 'did', 'you', 'tell', '_', '_', '_', '_', '_', '_']\n",
      "['why', 'did', 'you', 'keep', '_', '_', '_', '_', '_', '_']\n",
      "['why', 'did', 'you', 'keep', 'that', '_', '_', '_', '_', '_']\n",
      "['why', 'did', 'you', 'keep', 'that', 'a', '_', '_', '_', '_']\n",
      "['why', 'did', 'you', 'keep', 'that', 'a', 'secret', '_', '_', '_']\n",
      "['why', 'did', 'you', 'keep', 'that', 'a', 'secret', '?', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"pourquoi\", \"l'as-tu\", \"gardé\", \"secret\", \"?\"] # taken from training set\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\", \"you\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\", \"you\", \"keep\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\", \"you\", \"keep\", \"it\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\", \"you\", \"keep\", \"it\", \"a\"])]), axis=-1)[0])[:10])\n",
    "print(en_decode(np.argmax(model.predict(x=[fr_input(tokens), en_input([START_WORD, \"why\", \"did\", \"you\", \"keep\", \"it\", \"a\", \"secret\"])]), axis=-1)[0])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to separate the decoder layers into a new keras Model so we can use it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:46:59.780316Z",
     "start_time": "2019-09-05T14:46:59.446889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FreVariableToken (InputLayer (None, None)              0         \n",
      "_________________________________________________________________\n",
      "FreEmbeds (Embedding)        multiple                  5723200   \n",
      "_________________________________________________________________\n",
      "FreLSTM (LSTM)               [(None, 201), (None, 201) 323208    \n",
      "=================================================================\n",
      "Total params: 6,046,408\n",
      "Trainable params: 6,046,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "EngSingleToken (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngEmbeds (Embedding)           multiple             2840400     EngSingleToken[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "EngHiddLSTM (InputLayer)        (None, 201)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngConsLSTM (InputLayer)        (None, 201)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EngLSTM (LSTM)                  multiple             323208      EngEmbeds[1][0]                  \n",
      "                                                                 EngHiddLSTM[0][0]                \n",
      "                                                                 EngConsLSTM[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 multiple             2868804     EngLSTM[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,032,412\n",
      "Trainable params: 6,032,412\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get encoder states\n",
    "fre_var = Input(shape=(None,), name=\"FreVariableToken\")\n",
    "_, fre_hidden_var, fre_const_var = fre_lstm(fre_emb(fre_var))\n",
    "encoder = Model(inputs=fre_var, outputs=[fre_hidden_var, fre_const_var])\n",
    "encoder.summary()\n",
    "\n",
    "# Make the initial states of the decoder, input symbols\n",
    "eng_one = Input(shape=(1,), name=\"EngSingleToken\")\n",
    "hidden_state = Input(shape=(201,), name=\"EngHiddLSTM\")\n",
    "const_state = Input(shape=(201,), name=\"EngConsLSTM\")\n",
    "\n",
    "# wire the input states to the trained LSTM\n",
    "# we are reusing the eng_emb trained layer to convert token ids into embeddings\n",
    "# (?) If we pass the initial_state to the LSTM do we still need to reuse the already trained one?\n",
    "# (?) i.e. does the LSTM have any other state only persisted in the trained LSTM instance?\n",
    "# (?) EDIT: There are some Wx , Wh matrices what are indeed trained and only stored internally\n",
    "all_eng_out, eng_hidden_state, eng_constants = eng_lstm(eng_emb(eng_one), initial_state=[hidden_state, const_state])\n",
    "\n",
    "# wire the all steps output to the trained eng_decode Dense layer\n",
    "one_hot_words = eng_decode(all_eng_out)\n",
    "\n",
    "decoder = Model(inputs=[eng_one, hidden_state, const_state], outputs=[one_hot_words, eng_hidden_state, eng_constants])\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last piece now is going recursevly until we get to a STOP_WORD token.\n",
    "\n",
    "*Note: The code bellow also contains lots of debug code and output because getting the right tensors to line up wasn't as straigh forward as I'd planned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T17:24:57.321723Z",
     "start_time": "2019-09-05T17:24:57.192226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "[0] French: \tallez a la maison avec nous . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "[0] Indexed: \t[  992   236 14552 15685  2176 17655    19     1     1     1]\n",
      "\n",
      "[0] Hidden:\t[ 0.08  0.03  0.   -0.    0.46 -0.14  0.   -0.26  0.06 -0.  ]\n",
      "[0] Const:\t[ 0.13  0.16  1.09 -0.07 21.51 -0.37  0.   -0.94  2.74 -0.89]\n",
      "[0] Input: \t>\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[0] Top: \t['he', 'go', 'come', 'home', 'we', 'his', 'look', 'get', 'i', 'from']\n",
      "[0] Score: \t[0.57, 0.33, 0.03, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0, 0.0]\n",
      "[0] Pred: \t`>` ~~~: he\n",
      "\n",
      "[1] Hidden:\t[ 0.07  0.34 -0.06  0.73  0.83 -0.05  0.14 -0.64  0.93 -0.32]\n",
      "[1] Const:\t[ 0.1   0.35 -0.06  0.93  9.08 -0.05  0.19 -1.36  1.66 -0.44]\n",
      "[1] Input: \the\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[1] Top: \t[\"'s\", 'is', 'has', 'lives', 'went', 'left', 'goes', 'turned', 'leaves', 'cut']\n",
      "[1] Score: \t[0.76, 0.1, 0.04, 0.03, 0.02, 0.02, 0.01, 0.0, 0.0, 0.0]\n",
      "[1] Pred: \t`> he` ~~~: 's\n",
      "\n",
      "[2] Hidden:\t[ 0.11  0.18 -0.53 -0.02  0.71 -0.09  0.19 -0.71  0.9  -0.25]\n",
      "[2] Const:\t[ 0.11  0.18 -0.59 -0.02  9.08 -0.09  0.19 -1.85  1.48 -0.37]\n",
      "[2] Input: \t's\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[2] Top: \t['home', 'house', 'gone', 'left', 'building', 'room', 'been', 'tv', 'built', 'part']\n",
      "[2] Score: \t[0.57, 0.16, 0.11, 0.06, 0.04, 0.01, 0.01, 0.01, 0.0, 0.0]\n",
      "[2] Pred: \t`> he 's` ~~~: home\n",
      "\n",
      "[3] Hidden:\t[ 0.11 -0.2  -0.2  -0.    0.92 -0.28 -0.58 -0.64  0.45  0.15]\n",
      "[3] Const:\t[ 1.100e-01 -2.000e-01 -8.500e-01 -1.000e-02  1.002e+01 -2.900e-01\n",
      " -6.700e-01 -1.880e+00  4.800e-01  2.200e-01]\n",
      "[3] Input: \thome\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[3] Top: \t['with', '.', 'has', \"'s\", 'and', 'get', 'left', 'is', 'go', 'have']\n",
      "[3] Score: \t[0.59, 0.11, 0.07, 0.05, 0.04, 0.02, 0.02, 0.01, 0.01, 0.01]\n",
      "[3] Pred: \t`> he 's home` ~~~: with\n",
      "\n",
      "[4] Hidden:\t[ 0.4  -0.12 -0.57 -0.    1.    0.45  0.29 -0.57 -0.    0.76]\n",
      "[4] Const:\t[ 4.300e-01 -1.200e-01 -7.200e-01 -0.000e+00  1.074e+01  4.900e-01\n",
      "  3.500e-01 -1.290e+00 -1.000e-02  9.900e-01]\n",
      "[4] Input: \twith\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[4] Top: \t['the', 'them', 'their', 'us', 'all', 'left', 'two', 'work', 'one', ',']\n",
      "[4] Score: \t[0.96, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[4] Pred: \t`> he 's home with` ~~~: the\n",
      "\n",
      "[5] Hidden:\t[ 0.55  0.44 -0.5   0.    1.    0.81 -0.32 -0.63  0.26 -0.4 ]\n",
      "[5] Const:\t[ 0.61  0.89 -1.55  0.22 11.21  1.13 -0.33 -1.47  0.27 -0.42]\n",
      "[5] Input: \tthe\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[5] Top: \t['work', 'left', 'back', 'of', 'while', 'and', 'two', 'after', 'we', ',']\n",
      "[5] Score: \t[0.09, 0.09, 0.07, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.03]\n",
      "[5] Pred: \t`> he 's home with the` ~~~: work\n",
      "\n",
      "[6] Hidden:\t[ 0.38  0.01 -0.94  0.1   1.    0.48  0.69 -0.64 -0.03 -0.74]\n",
      "[6] Const:\t[ 4.000e-01  1.000e-02 -1.720e+00  2.200e-01  1.203e+01  5.200e-01\n",
      "  8.400e-01 -1.600e+00 -3.000e-02 -9.500e-01]\n",
      "[6] Input: \twork\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[6] Top: \t['.', 'two', 'and', 'with', ',', 'of', '?', 'for', 'many', 'their']\n",
      "[6] Score: \t[0.9, 0.03, 0.03, 0.02, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[6] Pred: \t`> he 's home with the work` ~~~: .\n",
      "\n",
      "[7] Hidden:\t[ 0.62  0.76  0.03  0.55  1.    0.43  0.   -0.7   0.7   0.48]\n",
      "[7] Const:\t[ 0.73  1.    0.03  1.03  7.41  0.46  1.32 -1.43  0.86  0.55]\n",
      "[7] Input: \t.\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[7] Top: \t['_', 'let', 'have', 'stopped', 'failed', 'could', 'quit', 'gave', 'spoke', 'seldom']\n",
      "[7] Score: \t[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[7] Pred: \t`> he 's home with the work .` ~~~: _\n",
      "\n",
      "---------------------------\n",
      "[0] French: \tva ! _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "[0] Indexed: \t[27005     2     1     1     1     1     1     1     1     1]\n",
      "\n",
      "[0] Hidden:\t[ 0.35 -0.    0.    0.03  0.69  0.26  0.26  0.17  0.15 -0.  ]\n",
      "[0] Const:\t[ 0.66 -2.35  1.61  0.32 22.39  0.54  0.76  0.2   3.4  -0.  ]\n",
      "[0] Input: \t>\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[0] Top: \t['go', 'come', 'get', 'see', 'leave', 'take', 'eat', 'start', 'be', 'make']\n",
      "[0] Score: \t[0.99, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0] Pred: \t`>` ~~~: go\n",
      "\n",
      "[1] Hidden:\t[ 0.53 -0.93  0.6   0.74  0.75  0.59  0.53 -0.35  0.98  0.16]\n",
      "[1] Const:\t[ 0.62 -1.67  0.7   1.19  8.29  0.68  1.22 -0.54  2.28  0.31]\n",
      "[1] Input: \tgo\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[1] Top: \t['.', '!', 'away', 'ahead', 'down', 'wash', 'to', 'of', 'off', 'there']\n",
      "[1] Score: \t[0.93, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[1] Pred: \t`> go` ~~~: .\n",
      "\n",
      "[2] Hidden:\t[ 0.69 -0.55 -0.16 -0.    0.88  0.28 -0.54 -0.4   0.97  0.12]\n",
      "[2] Const:\t[ 0.85 -0.67 -0.16 -1.    9.29  0.29 -0.64 -0.76  2.04  0.97]\n",
      "[2] Input: \t.\n",
      "(14202,) (201,) (201,)\n",
      "()\n",
      "[2] Top: \t['_', \"''\", '``', 'will', 'let', 'do', 'can', 'shall', 'they', 'tell']\n",
      "[2] Score: \t[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[2] Pred: \t`> go .` ~~~: _\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'> go . _'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_not_finished(generated_english_word_ids):\n",
    "    \"\"\"\n",
    "    Checks if we have a stopping event encountered.\n",
    "    Resons to stop generating words:\n",
    "    * STOP_WORD encountered\n",
    "    * max_lenght words generated\n",
    "    \"\"\"\n",
    "    id_last_word_predicted = generated_english_word_ids[-1]\n",
    "    \n",
    "    is_stop_word_hit = id_last_word_predicted == en2id[STOP_WORD]\n",
    "    is_max_length_hit = len(generated_english_word_ids) == max_eng_size\n",
    "    \n",
    "    return not is_stop_word_hit and not is_max_length_hit \n",
    "\n",
    "\n",
    "def translate(fre_tokens: List[str], debug=True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Translates the given sentence from French to English.\n",
    "    \n",
    "    The input of this function is a list of french word tokens.\n",
    "    \"\"\"\n",
    "    # Scan the full french phrase and return the hidden state of the LSTM\n",
    "    # This will be used to initialize the first step of the LSTM english decoder\n",
    "    french_phrase = fr_input(fre_tokens)[0]\n",
    "    \n",
    "    # Somehow it matters to have the full padded string as input\n",
    "    # Sending only the actuall tokens without the padding considerably degrades the performance\n",
    "#     french_phrase = fr_indexed(lower(fre_tokens + [STOP_WORD] * 45))\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"---------------------------\")\n",
    "        print(f\"[0] French: \\t{to_str(fr_decode(french_phrase))[:80]}\")\n",
    "        print(f\"[0] Indexed: \\t{french_phrase[:10]}\\n\")\n",
    "    \n",
    "    eng_tokens = [START_WORD]\n",
    "    eng_inp = en_indexed(lower(eng_tokens))\n",
    "\n",
    "    [hidden], [const] = encoder.predict([[french_phrase]])\n",
    "#     print(hidden.shape, const.shape)\n",
    "#     print(f\"[0] Hidden:\\t{np.round(hidden, 2)[:10]}\")\n",
    "#     print(f\"[0] Const:\\t{np.round(const, 2)[:10]}\")\n",
    "\n",
    "#     print(hidden[:, 0], const[:, 0])\n",
    "        \n",
    "    id_last_word_predicted = eng_inp[-1]\n",
    "#     id_stop_word = en2id[STOP_WORD]\n",
    "    \n",
    "    # Continously make predictions until we have a stopping event\n",
    "    while is_not_finished(eng_inp):\n",
    "#         fixed_eng_inp = fixed_length_en([eng_inp[-1]], padd=id_stop_word)\n",
    "        fixed_eng_inp = [eng_inp[-1]]\n",
    "    \n",
    "        if debug:\n",
    "            print(f\"[{len(eng_inp)-1}] Hidden:\\t{np.round(hidden, 2)[:10]}\")\n",
    "            print(f\"[{len(eng_inp)-1}] Const:\\t{np.round(const, 2)[:10]}\")\n",
    "            print(f\"[{len(eng_inp)-1}] Input: \\t{to_str(en_decode(fixed_eng_inp))}\")\n",
    "\n",
    "        # pass the decoder LSTM hidden staten and the last word we've just predicted\n",
    "        [[[one_hot_words]], [hidden], [const]] = decoder.predict([[fixed_eng_inp], [hidden], [const]])\n",
    "#         [[[one_hot_words], [o2]], [hidden, h1], [const, c1]] = decoder.predict([[fixed_eng_inp, fixed_eng_inp], [hidden, hidden], [const, const]])\n",
    "        print(one_hot_words.shape, hidden.shape, const.shape)\n",
    "        \n",
    "        # one_hot_words.shape = (53, 14203)\n",
    "        # predicted_words = (53,) \n",
    "        predicted_words = np.argmax(one_hot_words, axis=-1)\n",
    "        print(predicted_words.shape)\n",
    "        \n",
    "        # the last word is the one corresponding to the last seen french word (if we discard the START_WORD) \n",
    "        # id_last_word_predicted = predicted_words[len(eng_inp) - 1]\n",
    "        id_last_word_predicted = predicted_words\n",
    "        \n",
    "        if debug:\n",
    "            # print the top words predicted, for each timestep\n",
    "            top_words = np.argsort(one_hot_words)[::-1][:10]\n",
    "            print(f\"[{len(eng_inp)-1}] Top: \\t{[id2en[_id] for _id in top_words]}\")\n",
    "            print(f\"[{len(eng_inp)-1}] Score: \\t{[np.round(one_hot_words[_id], 2) for _id in top_words]}\")\n",
    "            print(f\"[{len(eng_inp)-1}] Pred: \\t`{to_str(en_decode(eng_inp))}` ~~~: {id2en[id_last_word_predicted]}\\n\")\n",
    "            \n",
    "        eng_inp.append(id_last_word_predicted)\n",
    "        \n",
    "\n",
    "    return en_decode(eng_inp)\n",
    "\n",
    "to_str(translate([\"Allez\", \"a\", \"la\", \"maison\", \"avec\", \"nous\", \".\"]))\n",
    "to_str(translate([\"Va\", \"!\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output above you can ignore the `Hidden` and `Constant` lines. But you cn observe the progression and the possible candidate words along with their probabilities:\n",
    "    \n",
    "    [3] Input: \thome\n",
    "    [3] Top: \t['with', '.', 'has', \"'s\", 'and', 'get', 'left', 'is', 'go', 'have']\n",
    "    [3] Score: \t[0.59, 0.11, 0.07, 0.05, 0.04, 0.02, 0.02, 0.01, 0.01, 0.01]\n",
    "    [3] Pred: \t`> he 's home` ~~~: with\n",
    "\n",
    ".. means that:\n",
    "* on predicting the 4rd word ([3])\n",
    "* the previous choosen token (that triggers the computation) being `home` (and considering the current hidden state)\n",
    "* The next top words are `with, ., has, 's etc..`\n",
    "* Their scores are (respectively) `[0.59, 0.11, 0.07, ..]`\n",
    "* The choosen prediction for the sequence `> he \\'s home` is `with` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer science, beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements.\n",
    "\n",
    "The model aboves does a decent job on translation, but using greedy search (choosing the top performing word) at each step will not guarantee the overall best translation for the phrase. That's because a cerain top scoring word might lead to translation paths that are lower quality afterwards compared if we'd have choosen the second most probable word.\n",
    "\n",
    "In essence, the overall maximum probability is not guaranteed by local maximum probability. We need to take into account all the words in a phrase before judging on a translation. \n",
    "\n",
    "Of course, all possible phrases from the starting condition in order to score them is intractable.\n",
    "Also, even if we were to restrict on the top most 10 probabilies each time on each path would lead to an exponential increase in possible candidate phrases with each new word predicted.\n",
    "\n",
    "A solution is to use a **beam**, a fixed set of best candidates phrases that we want to expand next. \n",
    "\n",
    "Let's say our beam is of size 10:\n",
    "* we start with 10 candidates (all equal with \"START_WORD\")\n",
    "* compute the top 10 predictions and keep them in the beam (along with their probabilities)\n",
    "* for each of the 10 candidates above compute 10 predictions for each\n",
    "* you now have 100 candiadates (with updated probabilities given by each new word)\n",
    "* only retain in the beam the highest 10 scoring phrases \n",
    "    * it's really possible that multiple surviving candidates are generated from a single parent, that's OK\n",
    "* continue until all the phrases in the beam end with \"STOP_WORD\"\n",
    "\n",
    "*Note: In order to promote longer phrases (actually penalize the shorter one, there's some weighting going on), a division by the length of the phrase raised to the power of the hyper-parameter `alpha` (`0.7` is a good value for this but is dataset specific).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:49:32.820547Z",
     "start_time": "2019-09-05T14:49:32.803262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "> _\n",
       "[1.0] 1 1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.7\n",
    "class Candidate():\n",
    "    def __init__(self):\n",
    "        self.words = list()\n",
    "        self.prob = list()\n",
    "        self.p = 0\n",
    "        self.hidden = None\n",
    "        self.const = None\n",
    "        \n",
    "    def __add_word(word_id):\n",
    "        self.words.append(word_id)\n",
    "    \n",
    "    def __update_probability(self, prob: float):\n",
    "        p_log = -np.log(prob)\n",
    "        self.p += p_log\n",
    "        self.prob.append(prob)\n",
    "    \n",
    "    def __update_current_state(self, hidden, const):\n",
    "        self.hidden = hidden\n",
    "        self.const = const\n",
    "\n",
    "    def add_state(self, word_id, prob, hidden, const):\n",
    "        self.words.append(word_id)\n",
    "        self.__update_probability(prob)\n",
    "        self.__update_current_state(hidden, const)\n",
    "        return self\n",
    "    \n",
    "    def score(self):\n",
    "        return np.exp(np.sum(np.log(self.prob)) / (len(strip_indexed(self.words))**alpha))\n",
    "\n",
    "    def __repr__(self):\n",
    "        score = self.score()\n",
    "        return f\"\\n{to_str(en_decode(self.words))}\\n[{np.round(score, 3)}] {' '.join([str(val) for val in np.round(self.prob, 3)])}\"\n",
    "    \n",
    "Candidate().add_state(en2id[START_WORD], 1, [], []).add_state(en2id[STOP_WORD], 1, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:56:35.914260Z",
     "start_time": "2019-09-05T14:56:35.629652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> did anyone see you come in here ?\n",
      "---------------------------\n",
      "[0] French: \tquiconque vous a-t-il vu venir ici ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "[0] Indexed: \t[20896 27648   240 27701 27172 11901   234     1     1     1]\n",
      "\n",
      "(\n",
      "> did anyone see you come in here ? _\n",
      "[0.708] 1.0 0.989 0.68 0.68 0.892 0.964 0.521 0.98 1.0 1.0, \n",
      "> did anyone see you come here ? _\n",
      "[0.675] 1.0 0.989 0.68 0.68 0.892 0.964 0.473 0.999 1.0, \n",
      "> did anyone see tom come here ? _\n",
      "[0.416] 1.0 0.989 0.68 0.68 0.082 0.877 0.71 0.999 1.0, \n",
      "> did anybody front of all here you come ? _\n",
      "[0.395] 1.0 0.989 0.124 0.737 0.996 0.386 0.708 0.58 0.664 1.0 1.0, \n",
      "> did anyone here come to see you ? _\n",
      "[0.392] 1.0 0.989 0.68 0.097 0.601 0.977 0.334 0.999 1.0 1.0, \n",
      "> did anyone here come to come over ? _\n",
      "[0.382] 1.0 0.989 0.68 0.097 0.601 0.977 0.647 0.461 0.999 1.0, \n",
      "> did anybody front of your from here ? _\n",
      "[0.359] 1.0 0.989 0.124 0.737 0.996 0.186 0.573 0.891 1.0 1.0, \n",
      "> did anyone house tom come in ? _\n",
      "[0.347] 1.0 0.989 0.68 0.055 0.585 0.86 0.65 0.883 1.0, \n",
      "> did anybody front of all here ? _\n",
      "[0.343] 1.0 0.989 0.124 0.737 0.996 0.386 0.708 0.412 0.998, \n",
      "> did anyone here see you come over ? _\n",
      "[0.326] 1.0 0.989 0.68 0.097 0.161 0.969 0.996 0.535 1.0 1.0, \n",
      "> did anyone see tom come in ? _\n",
      "[0.321] 1.0 0.989 0.68 0.68 0.082 0.877 0.28 0.83 1.0, \n",
      "> did anybody front of everybody here ? _\n",
      "[0.299] 1.0 0.989 0.124 0.737 0.996 0.064 0.984 0.999 1.0, \n",
      "> did anybody front of you here all ? _\n",
      "[0.29] 1.0 0.989 0.124 0.737 0.996 0.094 0.755 0.553 0.889 1.0, \n",
      "> did anyone see you here ? _\n",
      "[0.289] 1.0 0.989 0.68 0.68 0.892 0.027 0.716 1.0, \n",
      "> did someone see you come here ? _\n",
      "[0.289] 1.0 0.989 0.01 0.811 0.851 0.987 0.744 1.0 1.0, \n",
      "> did anyone house tom come here ? _\n",
      "[0.28] 1.0 0.989 0.68 0.055 0.585 0.86 0.232 0.99 1.0, \n",
      "> did they see tom in coming ? _\n",
      "[0.28] 1.0 0.989 0.067 0.988 0.866 0.428 0.286 0.619 1.0, \n",
      "> did anybody front of all here you came ? _\n",
      "[0.275] 1.0 0.989 0.124 0.737 0.996 0.386 0.708 0.58 0.108 1.0 1.0, \n",
      "> did anyone here come to come with your ? _\n",
      "[0.268] 1.0 0.989 0.68 0.097 0.601 0.977 0.647 0.136 0.572 0.712 1.0, \n",
      "> did anybody front of them here ? _\n",
      "[0.252] 1.0 0.989 0.124 0.737 0.996 0.059 0.837 0.949 0.638)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tecknoworks/~Envs/deep3.6/lib/python3.6/site-packages/ipykernel_launcher.py:65: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def is_not_finished(generated_english_word_ids):\n",
    "    \"\"\"\n",
    "    Checks if we have a stopping event encountered.\n",
    "    Resons to stop generating words:\n",
    "    * STOP_WORD encountered\n",
    "    * max_lenght words generated\n",
    "    \"\"\"\n",
    "    id_last_word_predicted = generated_english_word_ids[-1]\n",
    "    \n",
    "    is_stop_word_hit = id_last_word_predicted == en2id[STOP_WORD]\n",
    "    is_max_length_hit = len(generated_english_word_ids) == max_eng_size\n",
    "    \n",
    "    return not is_stop_word_hit and not is_max_length_hit \n",
    "\n",
    "def beam_search(fre_tokens: List[str], width:int = 3, alpha = 0.7, debug=True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Translates the given sentence from French to English using the beam search approach.\n",
    "    \n",
    "    The input of this function is a list of french word tokens.\n",
    "    \n",
    "    The width paramter tells how many candidates to search at a time (the beam width).\n",
    "    The alpha paramter tells how hard to penalize short sentences (the bigger the alpha the less likely we will get shorter sentences).\n",
    "    \"\"\"\n",
    "    # Scan the full french phrase and return the hidden state of the LSTM\n",
    "    # This will be used to initialize the first step of the LSTM english decoder\n",
    "    # Note: Because we've trained with fixed length training data, we need \n",
    "    # to also supply a fixed length french input. If we pass a variable length one\n",
    "    # the hidden state after running the french sentence, will not be that accurate!\n",
    "    french_phrase = fr_input(fre_tokens)[0]\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"---------------------------\")\n",
    "        print(f\"[0] French: \\t{to_str(fr_decode(french_phrase))[:80]}\")\n",
    "        print(f\"[0] Indexed: \\t{french_phrase[:10]}\\n\")\n",
    "    \n",
    "    # Start with a single START_WORD\n",
    "    eng_inp = en_indexed([START_WORD])\n",
    "\n",
    "    # Scan the french phrase and get the hidden state\n",
    "    [fr_hidden], [fr_const] = encoder.predict([[french_phrase]])\n",
    "    \n",
    "    # start the beam search with a single candidates\n",
    "    # all start with \"START_WORD\"\n",
    "    candidates = [Candidate().add_state(word_id=en2id[START_WORD], prob=1, hidden=fr_hidden, const=fr_const)]\n",
    "    stalled_candidates = []\n",
    "    \n",
    "    stop_word_id = en2id[STOP_WORD]\n",
    "    \n",
    "    while candidates:\n",
    "        # Get previous states\n",
    "        word = [[candidate.words[-1]] for candidate in candidates]\n",
    "        hidden = [candidate.hidden for candidate in candidates]\n",
    "        const = [candidate.const for candidate in candidates]\n",
    "        prior_prob = np.array([[candidate.p] for candidate in candidates]) \n",
    "        \n",
    "        \n",
    "        # Predict next word for each candidate\n",
    "        [one_hot_words, hiddens, consts] = decoder.predict([word, hidden, const])\n",
    "        one_hot_length = one_hot_words.shape[-1]\n",
    "\n",
    "        # Compute the new probabilities  \n",
    "        word_prob = np.squeeze(one_hot_words, axis=1)   # reduce middle 1 dimension axis\n",
    "        word_log_prob = -np.log(word_prob)  # reason on the logs since are more numericaly stable\n",
    "        joint_prob = word_log_prob + prior_prob  # update the priors with the current word predictions\n",
    "    \n",
    "        # Compute the length normalisation trick on the joint probabilities\n",
    "        candidate_lengths = np.array([[len(strip_indexed(candidate.words))] for candidate in candidates])\n",
    "        lenght_normalized = joint_prob / np.power(candidate_lengths, alpha)\n",
    "    \n",
    "        # Get best candidates\n",
    "        descending_sorted_matrix = np.argsort(lenght_normalized, axis=None) # reason on (-) so we can use ascending order for getting the bigest elements\n",
    "        selected_indexes = descending_sorted_matrix[:width] \n",
    "        restored_coordinates = np.unravel_index(selected_indexes, (width, one_hot_length))\n",
    "        candidates_that_generated_best_words, best_k_words = restored_coordinates\n",
    "        best_join_prob = np.max(joint_prob, axis=0)     # actually compute the best scores for each word\n",
    "#         prior_prob = best_join_prob[best_k_words]\n",
    "        best_word_probab = word_prob[candidates_that_generated_best_words, best_k_words]\n",
    "\n",
    "        # Update the candidate list\n",
    "        new_candidates = []\n",
    "        for i in range(width):\n",
    "            word_id = best_k_words[i]\n",
    "            word_probability = best_word_probab[i]\n",
    "            candidate_parent = candidates_that_generated_best_words[i]\n",
    "            hidden = hiddens[candidate_parent]\n",
    "            const = consts[candidate_parent]\n",
    "            parent = candidates[candidate_parent]\n",
    "            \n",
    "            new_candidate = deepcopy(parent).add_state(word_id=word_id, prob=word_probability, hidden=hidden, const=const)\n",
    "\n",
    "            if word_id == stop_word_id:\n",
    "                stalled_candidates.append(new_candidate)\n",
    "                width -=1\n",
    "            else:\n",
    "                new_candidates.append(new_candidate)\n",
    "        \n",
    "        candidates = new_candidates\n",
    "\n",
    "    sorted_candidates = sorted([(candidate.score(), candidate) for candidate in stalled_candidates], reverse=True)\n",
    "    sorted_candidates = list(zip(*sorted_candidates))[1]\n",
    "    return sorted_candidates\n",
    "        \n",
    "# print(beam_search([\"Allez\", \"a\", \"la\", \"maison\", \"avec\", \"nous\", \".\"]))\n",
    "# print(beam_search([\"Va\", \"!\"]))\n",
    "\n",
    "sample = np.random.randint(0, decode_input_train.shape[0])\n",
    "print(to_str(en_decode(strip_indexed(decode_input_train[sample]))))\n",
    "print(beam_search(fr_decode(strip_indexed(encode_input_train[sample])), alpha=0.7, width=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can se the candidates generated by a beam-search with 20 candidates, that tried to translate the french sentence: \n",
    "    \n",
    "    quiconque vous a-t-il vu venir ici ?\n",
    "    \n",
    "and managed to translate it into \n",
    "    \n",
    "    did anyone see you come in here ?\n",
    "    \n",
    "The overall probability of this phrase is `0.708` (out of a maximum of 1.) and at each step, generated words had the following probabilities:\n",
    "    \n",
    "    0.989 | 0.68   | 0.68 | 0.892 | 0.964 | 0.521 | 0.98 1.0\n",
    "    did   | anyone | see  | you   | come  | in    | here ?\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating a text involves a complex cognitive operation (for a human). To decode the meaning of the source text in its entirety, the translator must interpret and analyse all the features of the text, a process that requires in-depth knowledge of the grammar, semantics, syntax, idioms, etc., of the source language, as well as the culture of its speakers. \n",
    "\n",
    "We've may come a long way with the current implementions of machine translations but we're still far away from a perfect long sentence translation which requires full text, multi-sentence understanding and reasoning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229.347px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
