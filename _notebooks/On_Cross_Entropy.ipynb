{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "On Cross-Entropy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1si_4X-tnkMj",
        "colab_type": "text"
      },
      "source": [
        "Cross entropy can be used to define a loss function in machine learning and is usualy used when training a classification problem. \n",
        "\n",
        "> In information theory, the cross entropy between two probability distributions $p$ and $q$ over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution $q$, rather than the true distribution $p$. ([source](https://en.wikipedia.org/wiki/Cross_entropy))\n",
        "\n",
        "This post tries to implement it in pure python to better understand it's inner workings and then compare it to other popular implementations for cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aKcspbiw_UR",
        "colab_type": "text"
      },
      "source": [
        "# Our implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJYiKDs4w5dX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ddc0ef6-e1d6-4080-a369-b6111045f98d"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch \n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhQUiVZZksK-",
        "colab_type": "text"
      },
      "source": [
        "The crossentropy function is defined as:\n",
        "\n",
        "$$Loss = -\\sum_{i}{target_i * \\log(prediction_i)}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLC4WQXdyEj9",
        "colab_type": "text"
      },
      "source": [
        "This seems simple enough so let's implement this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAViCcyFyAo1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d45b0c0-9890-4b60-e109-35bdadf3140f"
      },
      "source": [
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    # - SUM(target * log(pred))\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "categorical_crossentropy([0, 1], [0.5, 0.5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6931471805599453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzCj4K7syKeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "683942d7-600f-4a3f-8f2e-6970e2a132bb"
      },
      "source": [
        "categorical_crossentropy([0, 1], [0.5, 0.5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6931471805599453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAovSeIrybN6",
        "colab_type": "text"
      },
      "source": [
        "I don't trust my code so I need to certify that my implementation is working correctly by comparing it to known and proven implementations. \n",
        "\n",
        "The first one that comes to mind is the `sklearn` one. It is not (confusingly) called `crossentropy` but goes by its other name: `log_loss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36VYDRCNlP9h",
        "colab_type": "code",
        "outputId": "6ed30fd8-7a8c-4fa2-da95-b0fa6542e197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "log_loss([0, 1], [0.5, 0.5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6931471805599453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1hIngWQyw_t",
        "colab_type": "text"
      },
      "source": [
        "Ok! The results matched on both (and also match my analitical computation). Time for a few more tests to make sure we're not missing something with this happy flow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOX1UFoMlr__",
        "colab_type": "code",
        "outputId": "1086fb06-6879-45c4-dc4e-122fec5e38da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "def certify():\n",
        "    tests = [\n",
        "            [[0, 0, 1], [0.3, 0.7, 0.0]],\n",
        "            [[0, 1, 0, 0], [0.1, 0.2, 0.3, 0.4]],\n",
        "            [[1, 0], [0.4, 0.6]],\n",
        "    ]\n",
        "\n",
        "    for [y_true, y_pred] in tests:\n",
        "        my_xent = categorical_crossentropy(y_true, y_pred)\n",
        "        xent = log_loss(y_true, y_pred)\n",
        "        assert my_xent == xent, f\"{y_true}\\t{y_pred}\\n{my_xent} != {xent}\" \n",
        "\n",
        "certify()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f1d99854990a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mmy_xent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mxent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{y_true}\\t{y_pred}\\n{my_xent} != {xent}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcertify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-f1d99854990a>\u001b[0m in \u001b[0;36mcertify\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmy_xent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mxent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mmy_xent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mxent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{y_true}\\t{y_pred}\\n{my_xent} != {xent}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcertify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: [0, 0, 1]\t[0.3, 0.7, 0.0]\ninf != 12.033141381058451"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36sMurOvzdZZ",
        "colab_type": "text"
      },
      "source": [
        "Hmm.. it crashes on the first example.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfLyUbyZzz1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8a3b8c9c-009d-4cb3-e026-2013d3b98eb7"
      },
      "source": [
        "categorical_crossentropy([0, 0, 1], [0.3, 0.7, 0.0]), log_loss([0, 0, 1], [0.3, 0.7, 0.0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(inf, 12.033141381058451)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv6oAbcLz4ZJ",
        "colab_type": "text"
      },
      "source": [
        "The problem is of course in our implementation. We have a `0.0` value (the third in the `y_pred`) on which we are applying the `log`. You may remember that the log function is undefined on `0.0`. The `sklearn` implementation actually clips the end of the provided `y_pred` so it will never be `0.0` or `1.0`. \n",
        "\n",
        "*Offtopic*: `log(1.0)` is actually 0, it is defined, and I'm unsure why they clip the top as well. I assume is related either to the `vanishing gradient problem` or to the idea that a prediction is never actually 100% certain of a result (?).\n",
        "\n",
        "The clipping is performed, employing a sufficiently small `epsilon` value (`sklearn` defaults to `1e-15`), as:\n",
        "    \n",
        "    y_pred = max(eps, min((1-eps), y_pred))\n",
        "\n",
        "We can use the above or make use of `np.clip` which will implement the exact formula above, but faster (they claim)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km12y7bFpeir",
        "colab_type": "code",
        "outputId": "42243697-d68f-42c7-ca06-29fd03cd991f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def _clip_for_log(y_pred, eps=1e-15): \n",
        "    # y_pred = np.maximum(eps, np.minimum((1-eps), y_pred)) # equivalent\n",
        "    y_pred = np.clip(y_pred, eps, 1-eps)\n",
        "    return y_pred\n",
        "\n",
        "_clip_for_log(1), _clip_for_log(np.array([1, 1, 0, 1, 0, 0.5, 0.4, 0.3]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.999999999999999,\n",
              " array([1.e+00, 1.e+00, 1.e-15, 1.e+00, 1.e-15, 5.e-01, 4.e-01, 3.e-01]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0RywmTn165r",
        "colab_type": "text"
      },
      "source": [
        "The improved `crossentropy` function is now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh_7f-C_GBXI",
        "colab_type": "code",
        "outputId": "56e04ed4-c5ad-4abf-a50f-913c87aa1314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    y_pred = _clip_for_log(y_pred)\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "categorical_crossentropy([0, 0, 1], [0.3, 0.7, 0.0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34.538776394910684"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_YZhVAm2HsQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "f695d5d8-51cd-49b1-a85f-dc063fd72a7f"
      },
      "source": [
        "certify()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e627f102689a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcertify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-b39bd15b1efe>\u001b[0m in \u001b[0;36mcertify\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{y_true}\\t{y_pred}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcertify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: [0, 0, 1]\t[0.3, 0.7, 0.0]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeA0SMa72PnN",
        "colab_type": "text"
      },
      "source": [
        "Trying to run the test again shows that (even if the code doesn't crashes anymore) we are getting different results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqYjjhufoqVI",
        "colab_type": "code",
        "outputId": "cede6b58-b1c0-4d18-b674-b4f63fc9fb70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "categorical_crossentropy([0, 0, 1], [0.3, 0.7, 0.0]), log_loss([0, 0, 1], [0.3, 0.7, 0.0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34.538776394910684, 12.033141381058451)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLEEpCNXtOBk",
        "colab_type": "text"
      },
      "source": [
        "What happens is that, in reality, calling \n",
        "\n",
        "    log_loss([0, 0, 1], [0.3, 0.7, 0.0])\n",
        "\n",
        "is interpreted as \n",
        "\n",
        "```\n",
        "log_loss([\n",
        "    0, \n",
        "    0, \n",
        "    1\n",
        "], \n",
        "[\n",
        "    0.3,\n",
        "    0.7,\n",
        "    0.0\n",
        "])\n",
        "``` \n",
        "\n",
        "where each list is a batch of predictions. So the log_loss is actually used as a `binary_crossentropy` on each pair of (target, prediction) and the results (equal to the number of values in the lists) is averaged togheter. \n",
        "\n",
        "Explicitly, we have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1_banBbsZKL",
        "colab_type": "code",
        "outputId": "d44999d0-2077-44b5-d14a-e54efef6a169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(log_loss([0], [0.3], labels=[0, 1]) +\n",
        " log_loss([0], [0.7], labels=[0, 1]) + \n",
        " log_loss([1], [0.0], labels=[0, 1])) / 3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.033141381058451"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ZGvn1euFhT",
        "colab_type": "text"
      },
      "source": [
        "This means that we need to make the `sklearn` `log_loss` think that we're not having batches but a single prediction to evaluate (so instead of shape `(3,)` we need a `(1, 3)`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2qoJsAMt-ed",
        "colab_type": "code",
        "outputId": "43e32d84-9619-49b5-a18d-8d575f1a9805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "categorical_crossentropy([0, 0, 1], [0.3, 0.7, 0.0]), log_loss([[0, 0, 1]], [[0.3, 0.7, 0.0]])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34.538776394910684, 34.538776394910684)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9p8oz_a5M37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e621205-ad33-4da2-b9ab-2e5ec0ac4fdb"
      },
      "source": [
        "def certify():\n",
        "    tests = [\n",
        "            [[0, 0, 1], [0.3, 0.7, 0.0]],\n",
        "            [[0, 1, 0, 0], [0.1, 0.2, 0.3, 0.4]],\n",
        "            [[1, 0], [0.4, 0.6]],\n",
        "    ]\n",
        "\n",
        "    for [y_true, y_pred] in tests:\n",
        "        my_xent = categorical_crossentropy(y_true, y_pred)\n",
        "        xent = log_loss([y_true], [y_pred])\n",
        "        assert my_xent == xent, f\"{y_true}\\t{y_pred}\\n{my_xent} != {xent}\" \n",
        "    print(\"Success, results are equal!\")\n",
        "    \n",
        "certify()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success, results are equal!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcCeq7TaucHY",
        "colab_type": "text"
      },
      "source": [
        "Does this mean that our implementation does not work on batches?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWQoV5kMukTA",
        "colab_type": "code",
        "outputId": "27ba95be-612e-46f1-9b14-d464f99228fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "categorical_crossentropy([[0, 0, 1], [0, 1, 0]], [[0.3, 0.7, 0.0], [0.5, 0.2, 0.3]]), log_loss([[0, 0, 1], [0, 1, 0]], [[0.3, 0.7, 0.0], [0.5, 0.2, 0.3]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36.14821430734479, 18.074107153672394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0j_N_YEfUoo",
        "colab_type": "text"
      },
      "source": [
        "The results of our computatin and `sklearn`'s `log_loss` with batches is different.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qWpwChtvTug",
        "colab_type": "code",
        "outputId": "ca3876df-996c-4a7b-931e-7cf73cf79b24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "categorical_crossentropy([[0, 1, 0]], [[0.5, 0.2, 0.3]]) + categorical_crossentropy([0, 0, 1], [0.3, 0.7, 0.0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36.14821430734479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH8B2ZfwvkOd",
        "colab_type": "text"
      },
      "source": [
        "It works but not correctly. Our implementation does a `sum` over all errors in a batch but we need to return a mean, so we need to divide it by the number of examples in the batch (the batch_size). As such, the new implementation is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k64K5lkywL73",
        "colab_type": "code",
        "outputId": "daf9e6da-f460-4b60-8b58-11e3397f09b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def ensure_ndarray(value):\n",
        "    if not isinstance(value, np.ndarray):\n",
        "        value = np.asarray(value)\n",
        "    return value\n",
        "\n",
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Implements the crossentropy function:\n",
        "    Loss = - SUM(target * log(pred))\n",
        "    \"\"\"\n",
        "\n",
        "    y_true = ensure_ndarray(y_true)\n",
        "    y_pred = ensure_ndarray(y_pred)\n",
        "\n",
        "    # dimensions must match\n",
        "    assert y_true.shape == y_pred.shape\n",
        "\n",
        "    y_pred = _clip_for_log(y_pred)\n",
        "    batch_size = y_true.shape[0]\n",
        "    \n",
        "    return -np.sum(y_true * np.log(y_pred)) / batch_size\n",
        "\n",
        "categorical_crossentropy([[0, 0, 1], [0, 1, 0]], [[0.3, 0.7, 0.0], [0.5, 0.2, 0.3]]), log_loss([[0, 0, 1], [0, 1, 0]], [[0.3, 0.7, 0.0], [0.5, 0.2, 0.3]])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18.074107153672394, 18.074107153672394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIjrH8dA3Gcn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "9512c94f-ed8c-42be-b1b2-1064e48ea66f"
      },
      "source": [
        "certify()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e627f102689a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcertify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-f328bbe97973>\u001b[0m in \u001b[0;36mcertify\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmy_xent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mxent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mmy_xent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mxent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{y_true}\\t{y_pred}\\n{my_xent} != {xent}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Success, results are equal!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: [0, 0, 1]\t[0.3, 0.7, 0.0]\n11.512925464970229 != 34.538776394910684"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJeFeY0332yw",
        "colab_type": "text"
      },
      "source": [
        "Hmm... We're back to square one. The first example doesn't fit anymore because we are using a single list (and not batches). Our current implementation assumes we have a single prediction to make, but computes the batch size on the first dimension, which is 3 (but that's actually the number of classes in our single one-hot-encoded vector). \n",
        "\n",
        "We need to compute the `batch_size` a little more carefully (considering we have a batch computation if the inputs have at least 2 dimensions, else if only a single dimensions is used, the inputs are a single prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmsM6pl14t2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2135273b-9d5d-4a07-bf51-d15b51d7c7f1"
      },
      "source": [
        "def ensure_ndarray(value):\n",
        "    if not isinstance(value, np.ndarray):\n",
        "        value = np.asarray(value)\n",
        "    return value\n",
        "\n",
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Implements the crossentropy function:\n",
        "    Loss = - SUM(target * log(pred))\n",
        "    \"\"\"\n",
        "\n",
        "    y_true = ensure_ndarray(y_true)\n",
        "    y_pred = ensure_ndarray(y_pred)\n",
        "\n",
        "    # dimensions must match\n",
        "    assert y_true.shape == y_pred.shape\n",
        "\n",
        "    y_pred = _clip_for_log(y_pred)\n",
        "    batch_size = y_true.shape[0] if len(y_true.shape) > 1 else 1\n",
        "    \n",
        "    return -np.sum(y_true * np.log(y_pred)) / batch_size\n",
        "\n",
        "certify()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success, results are equal!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYSX41d03EWA",
        "colab_type": "text"
      },
      "source": [
        "Success!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSofEbKbDjXl",
        "colab_type": "text"
      },
      "source": [
        "# Sklearn's vs Ours discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA6luDZ86YXo",
        "colab_type": "text"
      },
      "source": [
        "Now, there's an interesting discussion about our above heuristic:\n",
        "* form an interface perspective, the implementation acts is two ways, given the shape of the inputs:\n",
        "    * in batch mode (dims >=2, first dim is the batch one) \n",
        "    * in single prediction mode (dims == 1, we only have a single prediction to evaluate)\n",
        "\n",
        "Since we have a hybrid behavior, we may try to standardize a part of it:\n",
        "* consider that we **always** have batches.\n",
        "\n",
        "This makes the behavior on the:\n",
        "```categorical_crossentropy([0, 0, 1], [0.3, 0.7, 0.0])``` be interpreted as a batch of `[(0, 0.3), (0, 0.7), (1, 0.0)]` examples (3 in this case) and the result be a mean of these. \n",
        "\n",
        "This is actually the behavior of the `sklearn` implementation. **It always assumes you send in batches**. It may make sense, since the vast majority of the time you want to use this function is in a stochastic gradient descent (batch based) training loop.\n",
        "\n",
        "Unfortunately this still adds some uncertainties (or heterogenous behaviour) as the pairs above `[(0, 0.3), (0, 0.7), (1, 0.0)]` cannot be plainly computed anymore with the initial formula:\n",
        "$$Loss = -\\sum_{i}{target_i*\\log(prediction_i)}$$ since this formulation is valid for a one-hot-encoded `target` variable where there is exactly one value of `1`. In the pair `(0, 0.3)` there is no 1 value in the target, so using this formula yields the result `0` (and it always is `0` for targets equal to `0`). This basically leads to the `Loss` value only represent the errors of the `positive` (`target == 1`) samples in the batch, because these are the only ones in which (the single) product is not `0`.\n",
        "\n",
        "## Binary Crossentropy\n",
        "\n",
        "The `sklearn` implementation solves this case by assuming that if your input dimension is 1 (you have a list of scalar values) the values will not be computed on the **categorical crossentropy** function but a simplified version of it where the pairs `(0, 0.3)` have following internal representation:\n",
        "* `0 is translated to [1, 0] <=> [1 - 0, 0]` \n",
        "* `0.3 is translated to [0.7, 0.3] <=> [1 - 0.3, 0.3]` \n",
        "\n",
        "This redefinition can be translated as:\n",
        "`the correct output is label 0 but the prediction for label 1 is 0.3. So basically I want to say that I predict 0 because I predict a really low label 1 value.`. \n",
        "\n",
        "Generically, the pairs $(label, pred)$ where $label \\in \\{0,1\\}$ and $pred \\in (0, 1]$ are equivaleted to `target = [1 - label, label]` and `prediction = [1 - pred, pred]`. Now we can compute the regular crossentropy formula:\n",
        "$$ Loss = - \\sum_{i}{target_i} *\\log(prediction_i) $$\n",
        "$$ Loss = - [( 1 - label) * \\log(1 - pred) + label * \\log(pred)]$$ \n",
        "\n",
        "The last formulation is called **binary crossentropy**. \n",
        "\n",
        "So in essence, `sklearn.log_loss` chooses to assume that we allways have batches in the input, and when in doubt (single dimension inputs), doesn't compute the `categorical crossentropy` but the `binary crossentropy`. \n",
        "\n",
        "For my taste and implementation I'm going to assume that we always compute the `categorical crossentropy` and relax the batching assumption as the function is called `categroical..`. I always do `categorical`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpwXwDIGgn4W",
        "colab_type": "text"
      },
      "source": [
        "# Keras / Tensorflow crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGNESuYeguPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.metrics import categorical_crossentropy as keras_cat_xent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCVpPrlMg0UC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "033af431-7e74-4018-cd18-c2e4cbf2c611"
      },
      "source": [
        "keras_cat_xent([0, 1], [0.5, 0.5]).numpy()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6931472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bux7gsHyhgUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5de2d6d-4028-47e7-dfd1-b2fc2d69c4f3"
      },
      "source": [
        "targets = [[0, 0, 1], [0, 1, 0]]\n",
        "predics = [[0.3, 0.7, 0.0], [0.5, 0.2, 0.3]]\n",
        "categorical_crossentropy(targets, predics), keras_cat_xent(targets, predics).numpy()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18.074107153672394, array([16.118095,  1.609438], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C4k9-eECa96",
        "colab_type": "text"
      },
      "source": [
        "Ok, using the tensorflow / keras version leads to the following 3 questions:\n",
        "* why do we get 2 values?\n",
        "* what do these values mean?\n",
        "* how can make our numbers match?\n",
        "\n",
        "The answers to the first and second question are somewhat obvious: we have as many results as samples in the batch, ant they are the results of the per-sample categorical crossentropy function. \n",
        "\n",
        "This is because the `K.categorical_crossentropy` function also has a `axis=-1` parameter which instructs on which dimensions to do the reduction. Since we're asking for a reduction on **only** the last dimension (the dimension of one-hot-encoded values) we are left with the dimension 0 elements (the batch size).\n",
        "\n",
        "We can demonstrate this by showing that calling the `K.categorical_crossentropy` function individually for each sample in a batch with size 1 will lead the the same 2 values as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncB0pMv8k39H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d6e0eb2c-9c28-4499-c16b-0522f0e7a8b4"
      },
      "source": [
        "keras_cat_xent([[0, 0, 1]], [[0.3, 0.7, 0.0]]), keras_cat_xent([[0, 1, 0]], [[0.5, 0.2, 0.3]])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([16.118095], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.609438], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaKPM_GzDslS",
        "colab_type": "text"
      },
      "source": [
        "Now for the last question (\"how can we make our previous numbers match\" / \"why don't they match?\"). \n",
        "\n",
        "Recall that on the first sample, our function returned `34.538`, the same for `log_loss` whereas the keras version returned `16.118`.. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsuAJNLIlcYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "225b69b1-bf32-412b-c077-79cf3f041d77"
      },
      "source": [
        "targets = [[0, 0, 1]]\n",
        "predics = [[0.3, 0.7, 0.0]]\n",
        "categorical_crossentropy(targets, predics), log_loss(targets, predics), keras_cat_xent(targets, predics).numpy().sum()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34.538776394910684, 34.538776394910684, 16.118095)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTVWHNZeBhUp",
        "colab_type": "text"
      },
      "source": [
        "After reading the source code of the keras implementation and couldn't find any difference with our implementation, I decided to recompute by hand their answer, when I noticed something strange."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlajBOvqB6kJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68f435d3-81c0-45d7-cc66-7ae894c3101b"
      },
      "source": [
        "TF_EPSILON = 1e-7\n",
        "SK_EPSILON = 1e-15\n",
        "\n",
        "- ((1 * np.log(TF_EPSILON))), - ((1 * np.log(SK_EPSILON)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16.11809565095832, 34.538776394910684)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZccC4ruFPD1",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow / Keras uses a different epsilon! \n",
        "\n",
        "We were previously using `1e-15` but they choose `1e-7`. It is a bit surprinsing that the resulting errors are that large, while the change between them is rather small. \n",
        "\n",
        "Sure, mathematically it make sense that the log of a `10^8` smaller value should result in a bigger error, but from an API point of view, predicting either `1/10^7` or `1/10^15` while the correct answer is `1`, should give pretty close errors. These two predictions are after all synonimus to `pure wrong`. \n",
        "\n",
        "\n",
        "Now, for the question of \"why we have 2 values instead of a single one per on a batch_size == 2?\" the answer is both suprising and confusing..\n",
        "\n",
        "* [`tensorflow.keras.metrics.categorical_crossentropy`](https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/backend.py#L4443-L4504) is a `function` and only computes the per-sample result (without doing a mean over the results).\n",
        "\n",
        "* [`tensorflow.keras.metrics.CategoricalCrossentropy`](https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/metrics.py#L2807-L2818) is a `class` (and `layer`) that computes the per-batch (with the mean, because it subclasses [`tensorflow.keras.metrics.MeanMetricWrapper`](https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/metrics.py#L547)). This is actually the class instanced when using a `Model` if you specify `loss='categorical_crossentropy'`.  \n",
        "\n",
        "* even more surprising, the official [`keras.io`](www.keras.io) implementation of the [`keras.losses.CategoricalCrossentropy`](https://github.com/keras-team/keras/blob/master/keras/losses.py) is returning the per-batch result but defaults to reducing it by doing the `losses_utils.Reduction.SUM_OVER_BATCH_SIZE`, so not a mean, but a **sum**. \n",
        "\n",
        "* Should I tell you that there's also a [`keras.**metrics**.CategoricalCrossentropy`](https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/metrics.py#L857) **`class`** that mirros the Tensorflow implementation (it does the mean)? Presumably you'd use the first version for the `loss=` part and the second one in the `metrics=` part..\n",
        "\n",
        "* and there is also the [`keras.losses.categorical_crossentropy`](https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/backend/tensorflow_backend.py#L3360) `function` which redirects to the Tensorflow `function` (the first point) which returns the per-sample crossentropy.. \n",
        "\n",
        "\n",
        "In all, using the vanila `keras.io` you can get (depending on what you use):\n",
        "* per-batch with SUM reduction\n",
        "* per-batch with MEAN reduction\n",
        "* per-sample \n",
        "\n",
        "On the TensorFlow `keras` port you can get:\n",
        "* per-batch with MEAN reduction\n",
        "* per-sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEXZkqFg2pzQ",
        "colab_type": "text"
      },
      "source": [
        "What about the batching dilema (the one where `sklearn` and ours diverged?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzaAgiU4BjtD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33f3a675-7a32-44b8-ec90-c9ecf44ca52b"
      },
      "source": [
        "keras_cat_xent([0, 0, 1], [0.3, 0.7, 0.0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.118095"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3TcElf89ym5",
        "colab_type": "text"
      },
      "source": [
        "Well, `keras` chooses to **always** do the crossentropy, like we did. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBmPFPiKNXVt",
        "colab_type": "text"
      },
      "source": [
        "One last note about Tensorflow / Keras. The `categroical_crossentropy` has also a parameter `from_logits=False` that can interpret the values of the predictions as logits, meaning that you can use it for multi-class predictions:\n",
        "\n",
        "    lables = [1, 0, 0, 1]\n",
        "\n",
        "where the network is expected to produce results for mutiple classes at the same time. This is interpreted as if each value of the lable represents a `binary_crossentropy` evaluation.\n",
        "\n",
        "Setting `from_logits=True` redirects you to using the [tensorflow.nn.softmax_cross_entropy_with_logits_v2](https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/ops/nn_ops.py#L3108) function. This function has a few caveats to understand:\n",
        "\n",
        "> **NOTE:**  While the classes are mutually exclusive, their probabilities\n",
        "  need not be.  All that is required is that each row of `labels` is\n",
        "  a valid probability distribution.  If they are not, the computation of the\n",
        "  gradient will be incorrect.\n",
        "\n",
        "This means that even I've stated you could have `lables=[1, 0, 0, 1]` this function actually requires that you send it something like `lables = [0.5, 0, 0, 0.5]` for it to be a valid probability distribution. You can convert `[1, 0, 0, 1]` to `[0.5, 0, 0, 0.5]` by passing it through a `softmax` or through a simples scalling method:\n",
        "\n",
        "    def scale(values):\n",
        "        return values / np.sum(values)\n",
        "\n",
        "> **WARNING:** This op expects unscaled logits, since it performs a `softmax`\n",
        "  on `logits` internally for efficiency.  Do not call this op with the\n",
        "  output of `softmax`, as it will produce incorrect results.\n",
        "\n",
        "This means that while we are **required to scale** the `lables` we are **required NOT to scale** the `logits` (i.e. predictions) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiMbUzcbhU2F",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0ogLcoqQU_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e75ee6f4-34a0-4687-c2b6-f8aad1e58e8f"
      },
      "source": [
        "from torch import nn\n",
        "nn.CrossEntropyLoss()(torch.tensor([[0.5, 0.5]]))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tm0vN3ierIa",
        "colab_type": "text"
      },
      "source": [
        "[NLLLoss](https://pytorch.org/docs/master/nn.html#nllloss) is the negative log likelihood implementation:\n",
        "* uses the format `(y_pred, y_true)` instead of the common `(y_true, y_pred)` found in `sklearn`, `keras`, `tensorflow`\n",
        "* `y_pred` is expected to have log values (i.e. `y_pred == log(orig_y_pred)`\n",
        "* `y_true` should contain **class indexes** (i.e. ordinal values not one-hot-encoded values). This is equivalent to the `sparse_categorical_crossentropy` class of modules in `keras` and TensorFlow\n",
        "* expects certain types: \n",
        "    * `torch.Long` for the `y_true`  \n",
        "    * `torch.Float` for the `y_pred`\n",
        "\n",
        "\n",
        "**Obervation**: Because the function **requires** the `y_pred` values to be in `log` format that means that is up to the called to do the `clipping` with whatever values he wishes to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgAeNSYuklqo",
        "colab_type": "code",
        "outputId": "dc8e2954-0925-436d-a57c-9e51aa412263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def f(values):\n",
        "    return torch.tensor(values).float()\n",
        "\n",
        "def l(values):\n",
        "    return torch.tensor(values).long()\n",
        "\n",
        "from torch import nn\n",
        "nn.NLLLoss()(f([np.log([0.5, 0.5])]), l([1]))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6931)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HndIfQl4hl7Z",
        "colab_type": "text"
      },
      "source": [
        "So let's respect these **documented** assumptions and try to check that we can correctly match the results of the `sklearn.metrics.log_loss` and `torch.nn.NLLLoss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFaob5S7fywZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "orig_targets = [[0, 0, 1], [0, 1, 0]]\n",
        "orig_predics = [[0.3, 0.7, 0.0], [0.5, 0.2, 0.3]]\n",
        "\n",
        "targets = np.argmax(orig_targets, axis=-1)\n",
        "predics = np.log(np.clip(orig_predics, SK_EPSILON, 1-SK_EPSILON)) # same clipping type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaufqgX3D6Gy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82697c1c-6d9c-4cbe-8ed7-6988d9306b1e"
      },
      "source": [
        "nn.NLLLoss()(f(predics), l(targets)).numpy(), log_loss(orig_targets, orig_predics)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(18.074108, dtype=float32), 18.074107153672394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAStGNooimDX",
        "colab_type": "text"
      },
      "source": [
        "Notice that by default, calling `.float()` on a PyTorch tensor yields a `float32` values wich leads to a reduction in precision of the results.\n",
        "\n",
        "Let's try to make the tensor a `float64` value and notice what happens "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnL7ypzUi3Xc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f292ec5-0c0f-4cf4-c718-d67212c9bc77"
      },
      "source": [
        "def f(values):\n",
        "    return torch.tensor(values).type(torch.DoubleTensor)\n",
        "\n",
        "def l(values):\n",
        "    return torch.tensor(values).long()\n",
        "\n",
        "orig_targets = [[0, 0, 1], [0, 1, 0]]\n",
        "orig_predics = [[0.3, 0.7, 0.0], [0.5, 0.2, 0.3]]\n",
        "\n",
        "targets = np.argmax(orig_targets, axis=-1)\n",
        "predics = np.log(np.clip(orig_predics, SK_EPSILON, 1-SK_EPSILON))   # same clipping type\n",
        "nn.NLLLoss()(f(predics), l(targets)).numpy(), log_loss(orig_targets, orig_predics)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(18.07410715), 18.074107153672394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI76t4vheo8G",
        "colab_type": "text"
      },
      "source": [
        "We get more decimal points but the results are still a bit off compared to the `sklearn` implementation. I'm not sure why that is.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW-SwbE4lFw5",
        "colab_type": "text"
      },
      "source": [
        "There is also the [CrossEntropyLoss](https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) layer, which is read it correctly only does a `softmax` on the predictions, before computing the `log_loss`. \n",
        "\n",
        "This means that either we need to invert the softmax before calling it, or we apply the softmax on the `sklearn` one if we wish to compare the results.\n",
        "\n",
        "It's easyer to do the second option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2a6EBr-i9q0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3680cc43-7907-4a17-d1c2-95b3aafae812"
      },
      "source": [
        "nn.CrossEntropyLoss()(f(orig_predics), l(targets)), log_loss(orig_targets, softmax(orig_predics))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.3566, dtype=torch.float64), 1.3565655522346258)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEHGD3p-kwKs",
        "colab_type": "text"
      },
      "source": [
        "It worked!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhcXBjOAl3uS",
        "colab_type": "text"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "* I wasn't expected things to be so nuanced when I started writing this!\n",
        "* `keras` in bit of a mess. There are multiple confusing ways to compute the crossentropy.\n",
        "* small details (the epsilon) matter\n",
        "* if not carefull we may sometime get to see the results of a `binary_crossentropy` rather than a `categorical_crossentropy`\n",
        "* PyTorch makes you to explicitly do stuff (like the applying the `log`, the `clipping` or the `softmax`) in order to make you aware of the subtle details that if made implicit (like `keras` and `sklearn` supperbly do) might make you shoot yourself in the foot (without even noticing it)\n",
        "\n",
        "My **main** takeawys are these:\n",
        "* implement everything yourself (or read the sourcecode). I'm affraid of how many details I've missed until now in other more convoluted (get it?!) layers / concepts.\n",
        "* [details matter](https://www.curs-ml.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOeDt7RVx_qe",
        "colab_type": "text"
      },
      "source": [
        "A realy nice article about the cross-entropy loss can also be found [here](https://gombru.github.io/2018/05/23/cross_entropy_loss/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OONV7wtCl7fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}